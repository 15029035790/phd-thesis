\documentclass{USC-Thesis}

\usepackage{indentfirst, times, natbib, setspace, amsmath, algorithm, amssymb, algorithmic}
\usepackage{wrapfig}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{appendix}
\usepackage{array}
%\usepackage{ctable}%
\usepackage{filecontents}
\usepackage{epstopdf}
\usepackage[size=small,format=plain,labelfont=up,textfont=up]{caption}
%\DeclareCaptionType{copyrightbox}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{mathtools}
\pagenumbering{gobble}
\usepackage{multicol}
\usepackage{lipsum}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{color}

\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
}

\newcommand\fnurl[2]{%
\href{#2}{#1}\footnote{\url{#2}}%
}

%\usepackage{subfig}
%\usepackage{subfigure}

\usepackage{comment}
%\usepackage[bibstyle=authoryear,citestyle=authoryearbrak]{biblatex}

%%% endfloat: push all tables and figures to the end of the paper
%%% disable in final manuscript
%  \usepackage{endfloat}
% \usepackage{fullpage}
% \singlespacing

\usepackage{lmodern}            % Use latin modern fonts
\usepackage[T1]{fontenc}        % Use Type 1 encoding 
\usepackage[nottoc]{tocbibind} % include reference in the TOC

\renewcommand\bf\bfseries  % this is new
\renewcommand{\sc}{\textsc}
\newcommand{\rr}{\raggedright}
\newcommand{\tn}{\tabularnewline}

\newcommand{\INDSTATE}[1][1]{\STATE\hspace{#1\algorithmicindent}}

\newcommand{\PGC}{{PrivGeoCrowd}}
\newcommand{\SCG}{{SCGuard}}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newtheorem{thm}{Theorem}[chapter]
\newtheorem{thmun}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Example}

\numberwithin{equation}{chapter}

\title{Location Privacy in Spatial Crowdsourcing}

%\title{Task Assignment with Rigorous Privacy Protection \break
%in Spatial Crowdsourcing}
\author{Hien To}
\major{Computer Science} 
\month{September} 
\year{2017}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\begin{document}
%%%%% FINAL OUTPUT %%%%%

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\topmatter{Dedication}

%\topmatter{Acknowledgments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents  % Table Of Contents
\listoftables     % List of tables
\listoffigures    % List of figures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\topmatter{Abstract}

Spatial crowdsourcing (SC) is a new platform that engages individuals in collecting and analyzing environmental, social and other spatiotemporal information. With SC, requesters outsource their spatiotemporal tasks (tasks associated with location and time) to a set of workers, who will perform the tasks by physically traveling to the tasks' locations. However, current solutions require the workers, who in many cases are simply volunteering for a cause, to disclose their locations to untrustworthy entities. Revealing an individual's location data to other entities may prevent people from contributing to SC applications, thus rendering location privacy a critical obstacle to the growth of SC applications.

This thesis aims to help SC companies (i.e., Uber, TaskRabbit) to protect location privacy of the users (both workers and requesters) participating in their SC platforms. To this end, we identify privacy threats toward both users during the two main phases of SC, tasking and reporting. \emph{Tasking} (aka task assignment) is the process of identifying which tasks should be assigned to which workers, which is handled by a SC-server. The latter phase is reporting, in which workers travel to the tasks' locations, complete the tasks and upload their reports to the server. Countermeasure studies during the reporting phase have been well studied; hence, we shift our focus on privacy protection during the tasking phase. In order to protect privacy during tasking, the server must assign tasks to workers without having access to the raw locations of the users. Hence, we propose privacy-aware frameworks for task assignment in SC as follows.

We first focus on a tasking scenario where the tasks are public and the protection focus is on the workers' locations. We introduce the first framework relies on a trusted third party (TTP) to sanitize the raw location data of the workers. We propose a mechanism based on differential privacy (a mathematically rigorous definition of privacy) and geocasting that achieves effective SC services while offering privacy guarantees to workers. We address scenarios with both static and dynamic (i.e., moving) datasets of workers. Next, we focus on a more realistic scenario where the location privacy of both workers and requesters' tasks need to be protected. We introduce another framework that does not require the TTP---the location data is perturbed locally. We propose a protocol based on geo-indistinguishability (notion of location privacy based on differential privacy) and reachability that achieves effective SC services while offering privacy guarantees to the users. In both scenarios, we investigate analytical/empirical models and task assignment strategies that balance multiple crucial aspects of SC functionality, such as task completion rate, worker travel distance and system overhead.

% abstract of icde18
%With spatial crowdsourcing (SC), requesters outsource their spatiotemporal tasks (tasks associated with location and time) to a set of workers, who will perform the tasks by physically traveling to the tasks' locations. However, current solutions require the locations of the workers and/or the tasks to be disclosed to untrusted parties (SC server) for effective assignments of tasks to workers. In this paper we propose a framework for assigning tasks to workers in an online manner without compromising the location privacy of workers and tasks. We perturb the locations of both tasks and workers based on geo-indistinguishability and then devise techniques to quantify the probability of reachability between a task and a worker, given their perturbed locations. We investigate both analytical and empirical models for quantifying the worker-task pair reachability and propose task assignment strategies that strike a balance among various metrics such as the number of completed tasks, worker travel distance and system overhead. Extensive experiments on real-world datasets show that our proposed techniques result in minimal disclosure of task locations and no disclosure of worker locations without significantly sacrificing the total number of assigned tasks.

%Both frameworks assume that the owners of the location data (either users or data aggregators that already has a signed agreement with the users such as ATT, Google and Apple) to perturb the raw location data before sharing with the SC companies. Therefore, in the second part of the thesis, we study data perturbation techniques that enable the data owners to share the perturbed data to the server without compromising users' location privacy. The challenge is to allow the server to accurately query the perturbed data given a specified privacy level. We consider two popular queries in the context of SC: the number of visits to a location (captures frequency of the users' visits), and the popularity of a location (captures the diversity of the users' visits). Being able to answer such queries would have an impact beyond SC applications---industries and organizations (e.g., academia, CDC) can use the aggregate or processed location data for the greater good (e.g., public safety, prevent spread of disease) without violating users' privacy.

%Location data are protected under differential privacy---a mathematically rigorous definition of privacy. To satisfy the privacy requirements, the basic idea is to add a quantifiable amount of noise set by the data owners. Then, the perturbed location data can be shared without any individual privacy concern. The challenge is how much noise is sufficient to hide individual's location information but at the same time do not compromise the functionality of SC. Therefore, we design a set of efficient techniques to strike a balance between the privacy-utility trade-offs in the context of SC applications.

\mainmatter

\newtheorem{lemma}{Lemma}

\chapter{Introduction}\label{ch.intro}
\section{Motivation}

Smartphones have recently surpassed the PC as the device of choice for accessing the Web, and mobile phone subscription has grown from 5.9 billion worldwide at the end of 2011~\cite{mobile2014mobile} to 6.9 billion in 2014, which is 95.5\% of the world population. There has also been a significant increase in mobile phone bandwidth: from 2.5G (up to 384Kbps) to 3G (up to 14.7Mbps) and recently 4G (up to 100 Mbps)~\cite{sauter2009mobile}.
The increase in computational and communication performance of mobile devices, coupled with the advances in sensor technology (e.g., video cameras, GPS, motion sensors) lead to an exponential growth in data collection and sharing by smartphones. An individual with a mobile phone can nowadays act as a multi-modal sensor collecting and sharing various types of high-fidelity spatiotemporal data instantaneously (e.g., picture, video, audio, location, time, speed, direction, acceleration).

Exploiting mobility of such a large volume of potential users\footnote{We use ``user'' when referring to both worker and requester throughout the thesis.}, a new mechanism for efficient and scalable data collection has emerged, named Spatial crowdsourcing (SC)~\cite{kazemi2012geocrowd}. With spatial crowdsourcing, the goal is to outsource a set of spatial tasks (i.e., tasks related to a location) to a set of workers who perform the spatial tasks by physically traveling to those locations. Spatial crowdsourcing has applications in numerous domains such as environmental sensing (iRain~\cite{iRain}), smart cities (Waze and TaskRabbit), journalism and crisis response (MediaQ~\cite{kim2014mediaq}).

Effective assignment of tasks to workers is an important phase in spatial crowdsourcing, i.e., tasks are completed in a timely fashion, and workers do not incur significant travel cost~\cite{kazemi2012geocrowd,kazemi2013geotrucrowd,deng2013maximizing,to2016real}. 
Hence, the tasking phase requires workers to reveal their locations and requesters to disclose their tasks' locations to potentially untrustworthy entities (e.g., server). However, disclosing individual locations has serious privacy implications. First, leaked locations are often collected and shared without user consent~\cite{Forbes2015,angwin2011,mcmillan2014}, leading to a breach of sensitive information; these include an individual's health (e.g., presence in a cancer treatment center), alternative lifestyles, political and religious preferences (e.g., presence in a church). Second, knowing user locations, an adversary can stage a broad spectrum of attacks such as physical surveillance and stalking, and identity theft~\cite{scheck2010}, leading to real-world consequences. These include discovering patterns of one-night stands from the trajectory of Uber riders~\cite{Perry2014}, monitoring the locations of the riders in real-time for entertainment~\cite{Forbes2014}, tracking a journalist and finding her personal information~\cite{usatoday2014}, or stalking Waze users by generating fake events like accidents~\cite{Wang2016}.
%The consequences may be extreme, for example, a security flaw in a gay dating app named Grindr reveals precise location of 90\% users~\cite{cook2014security}. This flaw led to serious consequences in countries where homosexuals faced extreme dangers, such as Egypt, Russia, Saudi Arabia. In Egypt, four men were sentenced to up to eight years in prison for homosexual conduct~\cite{noackg2014could}.
Thus, ensuring location privacy is an essential aspect of spatial crowdsourcing, because mobile users
may not agree to engage in spatial crowdsourcing if their privacy is violated.


\section{Thesis Statement}
In this thesis, we identify privacy as the major impediment to the success of any spatial crowdsourcing system. Thus, we propose a server-assigned spatial crowdsourcing framework that enables the participation of the individuals (both workers and requesters) without compromising their location privacy. Particularly, our goal is to \emph{use \textbf{regorious privacy techniques} in the \textbf{task assignment} phase of \textbf{spatial crowdsourcing} to hide the locations of workers and tasks from untrustworthy entities (e.g., SC-server) with \underline{low cost}, \underline{low overhead} and without compromising the \underline{performance}} of the SC system. Location privacy of the individual is protected with differential privacy, which is a mathematically rigorous definition of privacy. The cost is measured in terms of the travel distance between a task's location and an assigned worker while overhead refers to the computation and communication overhead required to match a task request to the assigned workers. The performance is measured by the number of assigned (or completed) tasks.

\section{Privacy-Preserving Task Assignment}
\label{sec:intro:ta}

\paragraph{Protecting Locations of Workers:} We first assume a scenario with public tasks and private workers, and the goal is to perform task assignment while at the same time protecting location privacy of the workers. This is a challenging task, as protecting privacy either introduces uncertainty with respect to worker whereabouts, decreasing assignment quality, or creates significant communication overhead.

One may argue that simply removing workers' identity by using fake identity (i.e., pseudonymity) would achieve privacy. However, we argue that hiding users' identity without hiding their locations does not provide privacy. This is because a user's location information can be tracked through several stationary connection points (e.g., cell towers). The user's \emph{location trace} can be easily associated with a certain residence home or office, which reveal the user's identity. This has been referred to as inference attack~\cite{krumm2007inference}. This study collects GPS data from volunteer users and finds their home locations with median error of 60 meters.
A study in~\cite{gonzalez2008understanding} showed that there is a close correlation between people's identities and their mobility.
Furthermore, an article in Nature~\cite{DeMontjoye2013locationunique} performed a large-scale study showed that 4 locations uniquely identify 95\% individuals.
Thus, hiding workers' locations are much more challenging than hiding his/her identity since the workers' locations are needed for effective task assignment.

Location privacy has been studied in the context of location-based services. Proposed solutions~\cite{gruteser2003anonymous,mca06,kalnis2007preventing,andres2013geo} typically focus on location-based queries, such as finding points of interest nearby a user's location without disclosing the actual coordinates. However, in SC, the worker location is no longer part of the query, but instead the result of a spatial query around the task location---searching workers in the proximity of the task. In addition, while some studies consider queries on private locations~\cite{ylx13,choi2014secure}, it is assumed that the data owner entity and the querying entity trust each other, with protection being offered only against intermediate service provider entities. This scenario does not apply to SC, as there is no explicit trust relationship between requesters and workers.

We propose a framework for protecting privacy of worker locations, whereby the SC-server only has access to data sanitized according to {\em differential privacy (DP)}~\cite{dwork2006differential}. In practice, there may be many SC-servers run by diverse organizations that do not have an established trust relationships with the workers. On the other hand, every worker subscribes to a {\em cellular service provider (CSP)} that provides Internet connectivity. The CSP already has access to the worker locations (e.g., through cell tower triangulation), but as opposed to the SC-server, the CSP signs a contract with its subscribers, stipulating the terms and conditions of location disclosure. The CSP collects user locations and releases them to third party SC-servers in noisy form, according to DP. However, using DP introduces three difficult challenges, as follows:

First, the SC-server must match workers to tasks using noisy data, which requires complex strategies to ensure effective task assignment. To create sanitized data releases at the CSP, we adopt the {\em Private Spatial Decomposition (PSD)} approach \cite{cormode2012differentially}. A PSD is a sanitized spatial index, where each index node contains a noisy count of the workers rooted at that node. 
To ensure that task assignment has a high success rate, we introduce an analytical model that determines with high probability a PSD partition around the task location that includes sufficient workers to complete the task.

Second, the DP protection model requires fake entries to be created in the PSD. Thus, the SC-server cannot directly contact workers (even if pseudonyms are used) as establishing a connection to an entity would allow the SC-server to learn whether an entry is real or not, and breach privacy. To address this challenge, we propose the use of geocasting~\cite{navas1997geocast} as means to deliver task requests to workers. Once a PSD partition is identified by the analytical model outlined above, the task request is geocast to all workers in the partition. Geocast introduces overhead considerations that need to be carefully considered in the framework design.

Third, protecting worker locations across multiple timestamps is notoriously difficult \cite{Fan14TKDE}. As workers move, new snapshots of sanitized worker locations must be disclosed, to maintain task assignment effectiveness. However, access to sequential releases gives an adversary more powerful attack opportunities. To counter such threats, differential privacy requires more noise injection, which in the worst case may reach amounts that are proportional to the length of the released location history (i.e., number of disclosed snapshots). Clearly, such large noise would render the data useless, since SC is likely to be a continuously-offered service in practice. We study custom-designed mechanisms for differentially-private release of worker locations across consecutive timestamps that preserve location accuracy, such that the task assignment remains highly effective.

Our contributions are: 
\vspace{0.05in}

\noindent \textbf{1)} We identify the specific challenges of location privacy in SC, and we propose a framework that achieves differentially-private protection guarantees.

\noindent \textbf{2)} We propose an analytical model that measures the probability of task completion with uncertain worker locations, and devise a strategy that finds appropriate PSD partitions to ensure high success rate of task assignment.

\noindent \textbf{3)} We introduce a geocast mechanism for task request dissemination that is necessary to overcome the restrictions imposed by DP, and we factor the geocast system overhead in the PSD partition search strategy.

\noindent \textbf{4)} We extend our solution to the more challenging scenario of multiple-snapshot releases of worker datasets. We investigate techniques for careful privacy budget allocation across consecutive releases, and we employ post-processing techniques based on Kalman filters to reduce the inaccuracy introduced by noise addition.

\noindent \textbf{5)} We conduct extensive experiments on real-world datasets which show that the proposed framework is able to protect workers' location privacy without significantly affecting the effectiveness and efficiency of the SC system.

\paragraph{Protecting Locations of Both Workers and Tasks:} Several studies (e.g.,~\cite{kazemi2012geocrowd,tran2017real}) focus on effective tasking by maximizing the number of assigned tasks while minimizing workers travel distances, for which they require workers to reveal their locations and requesters to disclose their tasks' locations to the server. We argue that to enable effective tasking, the server does not have to know the exact locations of the workers and the tasks because a task can be matched to a nearby worker as long as their proximity is known. However, once the worker agrees to complete the task, he must travel to the task's location, perform it, and report the result to the server. Obviously, at this phase, referred to as \emph{reporting}, the disclosures of the task's location to the assigned worker and vice versa are usually unavoidable. Thus, privacy during the reporting phase is less critical and beyond the scope of this paper; instead, we focus on privacy protection during the tasking phase.

%\footnote{The worker may need to share his route with the task's requester in real-time so that the requester can monitor his progress.}


Privacy-preserving task assignment in SC has been an active area of research in recent years. Existing studies have two major drawbacks. First, they generally focus only on protecting location privacy of workers~\cite{to2014framework,Gong2015,Shen2016} and assume that task locations are public. However, task locations should be secure during tasking since they can be sensitive.
For example, the task locations can indirectly reveal requesters' location, i.e., requesters often post tasks in the proximity of their locations~\cite{to2017location}.
Second, existing studies often assume a trusted entity to sanitize the location data~\cite{kazemi2011privacy,pournajaf2014spatial,to2014framework}. This is not always the case in all applications of SC as there is no explicit trust relationship between any two parties (e.g., requester and worker).
Hence, we assume a broader privacy setting where all SC parties could be curious but not malicious, and aim to \emph{protect location privacy of \textbf{both workers and tasks} during the tasking phase \textbf{without relying on any trusted entity}}.

To obscure the locations of the workers and the tasks from potentially untrustworthy entities (e.g., servers), we can apply location privacy mechanisms, such as cloaking~\cite{gruteser2003anonymous}, perturbation~\cite{andres2013geo,xiao2015protecting}, private information retrieval~\cite{ghinita2008private}, and secure multi-party computation~\cite{goryczka2015comprehensive}. Among them, we adopt a recent perturbation technique, named geo-indistinguishability~\cite{andres2013geo}, for these reasons: it is a mathematically rigorous definition of location privacy, and it suits our privacy setting in that the mechanism for achieving geo-indistinguishability can be performed in real-time by smartphones of workers and requesters, without the need of any trusted anonymization entity.

The challenge is to accurately estimate the worker-task pair reachability given only perturbed (or noisy) locations of the workers and the tasks. Due to the location uncertainty, a nearby task may not be assigned to a worker because the perturbed location of the task is farther away. This may reduce the number of completed tasks. On the other hand, a task can be assigned to a worker whose location is far away because the perturbed location of the worker is close to the task. This may increase the worker travel distance.

More concretely, we first assume the \emph{online} tasking strategy, which has been shown to be scalable and effective for tasking in SC~\cite{asghari2016price,tong2016online,tong2016vldb}. Within online tasking, the set of workers is known up front, and each task, upon arrival, needs to be immediately matched to an available worker (i.e., one at a time); once assigned, the worker becomes unavailable for assignment. The main objective is to assign as many tasks to the workers as possible during a given time interval. Consider the example of three online tasks in Figure~\ref{fig:tasking_framework}. Each task arrives one-by-one in the order of $t_1\rightarrow t_2\rightarrow t_3$. Every worker corresponds to a specific geographical region (aka \emph{spatial region}) represented by a circle where any enclosed task is considered \emph{reachable} from (and thus can be assigned to) the worker. The optimal assignment in this example is to match $t_1$ to $w_2$, $t_2$ to $w_3$ and $t_3$ to $w_1$. However, if $t_2$ is assigned first to $w_1$, $w_1$ becomes unavailable and therefore $t_3$ remains unmatched, resulting in a local optimum. It is known that the optimal algorithm for this problem (in terms of maximizing the number of assigned tasks\footnote{There could be different optimality criteria such as minimizing worker travel distance.}) is Ranking~\cite{karp1990optimal}, which selects a worker that is reachable to a task based on a random rank. However, Ranking may no longer generate the optimal result when it only has access to the perturbed locations of workers and tasks since the reachability between workers and tasks cannot be exactly determined.

\begin{figure}[ht]
		\centering
		\includegraphics[width=0.5\textwidth]{figs/tasking_framework}
	\caption{The tasking phase of spatial crowdsourcing.}
	\label{fig:tasking_framework}
\end{figure}

To address this challenge of location uncertainty, we partition the online tasking setting into a three-stage privacy-aware framework, dubbed \SCG\ (see Table~\ref{tab:stages}). \SCG\ involves different parties at each stage of the task assignment to ensure effective tasking. This is achieved through revealing users' locations gradually, only if needed, as the tasking proceeds from one stage to the other. In the first stage, the server recommends a small set of nearby \emph{candidate workers} for a given task (without knowing the exact locations of either party); the server then forwards the perturbed locations of these workers to the task's requester. On receipt of these perturbed locations, in the second stage, the requester identifies the most likely reachable worker and sends the task location to this worker. Once receiving the exact location of the task, in the final stage, the selected worker accepts the task if it is enclosed within his spatial region; otherwise, he rejects the task. Hence, it is possible for a candidate worker to learn the exact location of the task even when the worker is not reachable to the task; we quantify this disclosure later. The last two stages may repeat until either the task is assigned or no candidate worker is left.

\begin{table}[ht]
\begin{center}
\footnotesize
\begin{tabular}{ | c | l | m{12.5em} | m{10em} | } 
\hline
\textbf{Stage}					& \textbf{By} & \textbf{Input} 				& \textbf{Output} \\
\hline
\textbf{1} 	& SC-Server	&  Uncertain task location, uncertain workers' locations			&  A set of \emph{candidate workers} for a given task \\ 
\hline
\textbf{2}	& Requester & The uncertain candidate workers recommended by the server & The worker of the \emph{highest rank} \\  
\hline
\textbf{3} 	& Worker	&  Exact task location			& Accept the task or not \\ 
\hline
\end{tabular}
\caption{Three stages of the privacy-aware framework.}
\label{tab:stages}
\end{center}
\end{table}

A key component in each stage of the above framework is to determine whether a task is reachable from a worker. We first devise a baseline solution by assuming the perturbed locations of tasks and workers as their actual locations. We call this baseline the ``oblivious'' technique, as it is oblivious to the fact that the locations are perturbed and not real. Obviously, the utility of this approach is very low. To improve the utility of the oblivious approach, we propose analytical and empirical models to quantify the probability of reachability between a task and a worker in each stage of \SCG. Thereafter, we introduce a probability-based solution that improves the baseline in several metrics, including a higher number of assigned tasks, smaller worker travel cost and lower disclosure of location information.

The contributions of this paper are as follows.
\vspace{0.05in}

\noindent \textbf{1)} We propose \SCG, a privacy-aware framework that enables workers and requesters to participate in SC without compromising their location privacy. To the best of our knowledge, this is the first work designed to protect the privacy of both parties in SC without assuming any trusted entity.

\noindent \textbf{2)} We propose the analytical and empirical models to quantify the worker-task pair reachability in every stage of \SCG, based on which of the probabilistic tasking algorithm is introduced.

\noindent \textbf{3)} We conduct an extensive set of experiments on a real-world dataset, named T-Drive~\cite{yuan2010t}, showing three main results. First, the analytical model performs as well as the empirical model without relying on precomputation on past or synthetic data. Second, our probabilistic tasking algorithm is superior to the baseline in all key metrics, including a higher number of assigned tasks ($\times 3$), smaller worker travel cost ($2/3$) and lower disclosure of task location ($/100$), with only a slight increase in system overhead ($20\%$). Third, \SCG\ is able to protect location privacy of both workers and requesters without significantly compromising the key metrics of the SC system.

\chapter{Background in Spatial Crowdsourcing}
\section{Taxonomy of Spatial Crowdsourcing}
\label{sec:taxonomy}
Spatial crowdsourcing opens up a new mechanism for spatial tasks (i.e., tasks related to a location) to be performed by humans. Consequently, we formally define spatial crowdsourcing as the process of crowdsourcing a set of spatial tasks to a set of human workers where performing an outsourced task is only possible if the workers are physically at the location of the task, termed spatial task. In this section, we present a taxonomy for crowdsourcing (Figure \ref{fig:taxonomy}). First, we classify spatial crowdsourcing based on worker's motivation. Next, we define two modes of task publishing in spatial crowdsourcing. Finally, we classify the workers into two groups based on whether or not they have constraints.

\begin{figure}[!htb]\centering
  \includegraphics[width=0.7\textwidth]{figs/taxonomy}
  \caption{The focus of this paper, spatial crowdsourcing, is shown in grey.}
  \label{fig:taxonomy}
\end{figure}

\paragraph{Worker's Motivation} 
A major challenge in any crowdsourcing system is how to motivate people to participate. Four levels of worker motivation can be found in \cite{quinn2011human}, including pay, altruism, fun and implicit. To simplify, crowdsourcing can be classified based on the motivation of the workers into two classes: \textit{reward-based} and \textit{self-incentivised} (Figure \ref{fig:taxonomy}). With reward-based spatial crowdsourcing, every spatial task has a price (assigned by a requester) and workers will receive a certain reward for every spatial task they perform correctly. Examples of this class include \cite{fieldagent,gigwalk}. With self-incentivised spatial crowdsourcing, workers volunteer to perform the tasks or usually have other incentives rather than receiving a reward such as documenting an event or promoting their cultural, political or religious views. An example of this class is \cite{trafficucb}, in which more than 5000 users voluntarily install traffic software onto their phones and report traffic information. Our work focuses on self-incentivised spatial crowdsourcing.

\paragraph{Task Publishing Modes}
Next, we define two task publishing modes in spatial crowdsourcing, \emph{pull} and \emph{push}. With the pull mode, the SC-server publishes the spatial tasks and online workers can choose any spatial task in their vicinity without the need to coordinate with the server. One advantage of the pull mode is that the workers do not need to reveal their locations to SC-server since they can choose any arbitrary task in their vicinity autonomously. However, one drawback of this mode is that the server does not have any control over the allocation of spatial tasks. This may result in some spatial tasks never be assigned, while others are assigned redundantly. Another drawback of the pull mode is that workers choose tasks based on their own objectives (e.g., choosing the $k$ closest spatial tasks to minimize their travel cost), which may not result in a globally optimal assignment. An example of the pull mode is \cite{alt2010location}, where the workers browse for available spatial tasks, and pick the ones in their neighborhood.

With the push mode, the SC-server does not publish the spatial tasks to the workers. Instead, any online worker sends his location to the SC-server. The SC-server after receiving the locations of all online workers, assigns to every worker his closeby tasks. The advantage of the push mode is that unlike the pull mode, the SC-server has the big picture, and therefore, can assign to every worker his nearby tasks while maximizing the overall task assignment. Examples of this mode of spatial crowdsourcing include \cite{kazemi2012geocrowd} and \cite{kazemi2011privacy}. In \cite{kazemi2011privacy}, a framework for small campaigns is proposed, where workers are assigned to their closeby sensing tasks. However, the drawback is that the workers should report their locations to the server for every assignment, which can pose a privacy threat. Recently in~\cite{to2014framework}, a framework was proposed to sanitize workers' locations according to differential privacy while still using SC-server as a broker to assign tasks to workers. A real-world example of the push mode is Uber\footnote{https://www.uber.com/}, a mobile app that connects passengers with drivers of vehicles for ridesharing. Our focus in this paper is on this mode of spatial crowdsourcing.

\paragraph{Worker's Constraints}
Finally, in the case of the push mode, we divide the workers into two groups based on whether or not they have constraints. With workers without constraints, the server has full flexibility on how tasks should be assigned to the workers. This means that workers only send their locations to the server, and the server assigns every spatial task to its nearby worker \cite{kazemi2011privacy}. With workers with constraints, the server needs to satisfy the constraints while assigning the tasks. An example of spatial constraint is that every worker only accepts spatial tasks in a spatial region (i.e., his working region).

\section{Comparison to Other Fields of Study}
\textbf{Crowdsourcing:} Crowdsourcing has recently been attracting extensive attention in the research community. A recent survey in this area can be found in \cite{kittur2013future}. With a growing recognition of crowdsourcing, many crowdsourcing services including oDesk \cite{odesk}, MTurk \cite{mturk} and CrowdFlower \cite{Crowdflower} have emerged, which allow requesters to issue tasks that workers can perform for a certain reward. Crowdsourcing has been largely adopted in a wide range of applications. Examples of such applications include but are not limited to image search \cite{yan2010crowdsearch}, natural language annotations \cite{snow2008cheap}, video and image annotations \cite{chen2009crowdsourceable}, \cite{sorokin2008utility} and \cite{whitehill2009whose}, search relevance \cite{alonso2008crowdsourcing} and \cite{bozzon2012answering}, social games \cite{von2008designing} and \cite{guy2011guess} and graph search \cite{parameswaran2011human}. Moreover, the database community has utilized crowdsourcing in database design, query processing \cite{franklin2011crowddb}, \cite{marcus2011human}, \cite{parameswaran2012crowdscreen}, \cite{demartini2013crowdq} and \cite{zhao2013crowdseed} and data analytics \cite{liu2012cdas} and \cite{wang2012crowder}. In \cite{franklin2011crowddb}, a relational query processing system is proposed, which uses crowdsourcing to answer queries that cannot otherwise be answered. As part of the crowdsourced database systems, human-powered versions of the fundamental operators, such as sort and join \cite{marcus2011human} and filter \cite{parameswaran2012crowdscreen} were developed. Recently in \cite{liu2012cdas}, a system was developed to improve the accuracy of data analytics jobs by exploiting crowdsourcing techniques.

%Since the workers cannot always be trusted, as a future work, we aim to tackle the issue of trust by having tasks performed redundantly by multiple workers. This would require maintaining a reputation scheme for workers and requesters.

\textbf{Spatial Crowdsourcing:} Despite all the studies on crowdsourcing, spatial crowdsourcing only recently received some attention~\cite{to2014framework,deng2013maximizing,kazemi2013geotrucrowd,dang2013maximum,kazemi2012geocrowd,alt2010location}. In \cite{alt2010location}, a crowdsourcing platform is proposed, which utilizes location as a parameter to distribute tasks among workers. In \cite{kazemi2012geocrowd}, a spatial crowdsourcing platform whose goal is to maximize the number of assigned tasks is proposed. Since the workers cannot always be trusted, another work aims to tackle the issue of trust by having tasks performed redundantly by multiple workers \cite{kazemi2013geotrucrowd}. In \cite{dang2013maximum}, the problem of complex spatial tasks (i.e., each task comprises of a set of spatial sub-tasks) is introduced, in which the assignment of the complex task requires performing all of its sub-tasks. Meanwhile, the problem of scheduling tasks for a worker that maximizes the number of performed tasks is proposed in \cite{deng2013maximizing}. In \cite{Hassan2014a}, an online spatial task assignment problem is suggested to maximize the number of successful assignments. Recently in~\cite{to2014framework}, the authors introduced the problem of protecting worker location privacy in spatial crowdsourcing. This study proposes a framework that achieves differentially-private protection guarantees. The solutions for this problem are quite complex, and require tuning multiple parameters to obtain satisfactory results. Thus, the same authors propose \PGC\ \cite{to2015privgeocrowd}, an interactive visualization and tuning toolbox for privacy-preserving spatial crowdsourcing. \PGC\ helps system designers investigate the effect of parameters such as privacy budget and allocation strategy, task-assignment heuristics, dataset density on the effectiveness of private task matching. At the same time, privacy-preserving task assignment using cloaked locations is proposed in \cite{pournajaf2014spatial}.

Moreover, the problem of crowdsourcing location-based queries over Twitter has been studied, which employs a location-based service (e.g., Foursquare) to find the appropriate people to answer a given query \cite{bulut2011crowdsourcing}. Even though this work focuses on location based queries, it does not assign to users any spatial task, for which the user should go to that location and perform the corresponding task. Instead, it chooses users based on their historical Foursquare check-ins. In \cite{musthag2013labor}, spatiotemporal dynamics in mobile task markets, such as \cite{fieldagent,gigwalk}, were studied.

The well-known concept of participatory sensing could be deemed as one class of spatial crowdsourcing, in which workers form a campaign to perform sensing tasks. Examples of participatory sensing campaigns include \cite{trafficucb,kazemi2011privacy,cornelius2008anonysense,hull2006cartel} and \cite{mohan2008nericell}.
However, the major drawback of all the existing work on participatory sensing is that they focus on a single campaign and try to address the challenges specific to that campaign. Another drawback of most existing studies on participatory sensing (e.g., \cite{kazemi2011privacy}) is that they are designed for small campaigns, with a small number of participants, and are not scalable to large spatial crowdsourcing applications. Finally, while most existing work on participatory sensing systems focus on a particular application, our work can be used for any type of spatial crowdsourcing system.

Another class of spatial crowdsourcing is known as volunteered geographic information (or VGI), whose goal is to create geographic information provided voluntarily by individuals. Examples for this class include Google Map Maker \cite{GMM}, OpenStreetMap \cite{OSM} and WikiMapia \cite{wikimapia}. These projects allow the users to generate their own geographic content, and add it to a pre-built map. For example, a user can add the features of a location, or the events occurred at that location. However, the major difference between VGI and spatial crowdsourcing is that in VGI, users voluntarily participate by randomly contributing data, whereas in spatial crowdsourcing, a set of spatial tasks are queried by the requesters, and workers are required to perform those tasks. Moreover, with most VGI projects (\cite{GMM} and \cite{wikimapia}), users are not required to physically go to a particular location in order to generate data with respect to that location. Finally, as the name suggests, VGI falls into the class of self-incentivised crowdsourcing.

\textbf{Matching Problems:} One can consider the task assignment problem in spatial crowdsourcing as the online bipartite matching problem (\cite{karp1990optimal} \cite{kalyanasundaram93} \cite{Khuller94}, \cite{kalyanasundaram2000optimal} and \cite{mehta2007adwords}). Online bipartite matching problem is the most relevant variation of spatial crowdsourcing as it captures the dynamism of tasks arriving at different times. However, in online bipartite matching one of the item sets is given in advance and items from the other set arrive (usually one at a time), while in spatial crowdsourcing both sets, i.e., workers and tasks, can come and go without our knowledge. Thus, to some extent, online matching can be considered as a special case of our task assignment where the worker set (or task set) is fixed. In addition, with online matching, the cost/weight of any match is known in advance. However, with spatial crowdsourcing, the cost for a worker to perform a task mainly corresponds to the time it takes for him to travel to the location of the task. As the result, the cost of a task is not a fixed value but is dependent on the worker's prior location. Hence, the sequence in which the tasks are performed impacts the cost. That is, with spatial crowdsourcing, the cost of the execution of a set of tasks is the distance of the shortest path that starts from the worker's current location and goes through the locations of all the assigned tasks. On the other hand, with online matching \cite{kalyanasundaram2000optimal} the overall cost for one worker would be the sum of the distances between the worker and each assigned task. Finally, the performance of an online algorithm is often evaluated based on competitive ratio - the ratio between its performance and the offline algorithm's performance. The online algorithm is competitive if its competitive ratio is bounded under any circumstance; this is not the goal of MSA, which focuses on the average performance.

Some recent studies in spatial matching \cite{wong2007efficient} and \cite{yiu2008capacity} do focus on efficiency and use the spatial features of the objects for more efficient assignment. Spatial matching is a one to one (or in some cases one to many) assignment between objects of two sets where the goal is to optimize over some aggregate function (e.g., sum, max) of the distance between matched objects. For example, the objective in \cite{varadarajan1998divide} is to pair up 2N points in the plane into N pairs such that sum of the Euclidean distances between the paired points is minimized. In [Wong et al. 2007], given a set of customers and a set of service providers with limited capacities, the goal is to assign the maximum number of customers to their nearest providers, among all the providers whose capacities have not been exhausted in serving other closer customers. These studies assume a global knowledge about the locations of all objects exists a priori and the challenge comes from the complexity of spatial matching. However, spatial crowdsourcing differs due to the dynamism of tasks and workers (i.e., tasks and workers come and go without our knowledge), thus the challenge is to perform the task assignment at a given instance of time with the goal of global optimization across all times. Moreover, the fact that workers need to travel to task locations causes the landscape of the problem to change constantly. This adds another layer of dynamism to spatial crowdsourcing that renders it a unique problem.

%Note that even though both studies propose an algorithm for updating the assignments, the updates are due to the movement of objects (e.g., mobile users) and not due to appearance/disappearance of objects (i.e., tasks and workers).

Expertise matching is the concept of assigning queries to experts that has gained extensive interest from various research fields for its wide range of applications such as product-reviewer alignment, product-endorser matching, paper-reviewer assignment \cite{mimno2007expertise}, \cite{sun2008hybrid} and \cite{hettich2006mining}. In \cite{tang2012optimization}, a framework for expertise matching with various constraints was introduced, which is capable of rendering the optimal solution. However, none of these studies address the problem of expertise in spatial matching. Our objective is different from these studies as we address the problem of maximum expertise matches, while they try to find the best individual match based on some models/methods.

\textbf{Vehicle Routing Problems:} Modeling the assignment cost as the shortest path visiting the location of multiple tasks brings another class of problems to attention. In this context the assignment problem in spatial crowdsourcing becomes similar to the Traveling Salesman Problem (TSP) \cite{lawler1985traveling} and the Vehicle Routing Problem (VRP) \cite{toth2001vehicle}. The goal of VRP is to minimize the cost of delivering goods located at a central depot to customers who have placed orders for such goods with a fleet of vehicles.  The online versions of both TSP and VRP have been studied to some extent where new locations to visit are revealed incrementally. Since there is only one salesman in the standard version of TSP, here we focus on VRP. Different variations of VRP have been studied, yet there are differences between task assignment in spatial crowdsourcing and these variations. 
%With VRP, a server can pay a penalty and deny visiting a location; however, in spatial crowdsourcing the goal is to maximize the number of assigned tasks so the worker does not have the option of denying a task. Furthermore, 
With VRP, all workers start from the same depot, whereas in spatial crowdsourcing each worker can have a different starting location. Moreover, with VRP we have a fixed number of workers, whereas in spatial crowdsourcing the same type of dynamism for tasks can apply to the workers. That is, workers can be added (removed) to (from) the system at any time.

\chapter{Survey of Related Work}
\section{Spatial Crowdsourcing}
Spatial Crowdsourcing (SC) has recently been attracting attention in both the research communities (e.g., \cite{kazemi2012geocrowd,deng2013maximizing,to2014framework,to2016real,to2016sc}) and industry (e.g., TaskRabbit, Gigwalk~\cite{musthag2013labor}).
Various aspects of spatial crowdsourcing are summarized in Table~\ref{tab:sc_studies}.
In the following, we distinguishe SC from related fields, including crowdsourcing, participatory sensing, volunteered geographic information.

\begin{table}
\begin{center}
\begin{tabular}{ | l | p{11cm} | }
\hline
Problem Focus					& Studies \\
\hline
Task assignment 	& \cite{kazemi2012geocrowd,dang2013maximum,
pournajaf2014dynamic,Zhang2014,He2014a,Fonteles2014,Hassan2014a},
\cite{to2015server,yu2015quality,Xiong2015,
Xiao2015,UlHassan2015,Fonteles2015b},
\cite{Guo2016a,Liu2016,Zhang2016c,tong2016online,tong2016vldb,Hu2016,Cheng2016,asghari12auction,Gao2016,Zhang2016SpatialRecruiter,
ul2016efficient,Bessai2016,
Tan2016,Liu2016b,ul2016efficient,to2016real,
song2017trichromatic,zhao2017destination}   \\
\hline
Task scheduling & \cite{deng2013maximizing,Sadilek2013,Chen2014TRACCS,
deng2015task,Li2015,Hadano2015,Chen2015Towards,Fonteles2015a,Fonteles2015a,
Mrazovic2016,SalesFonteles2016,Wang2016Towards,Sun2016,Deng2016} \\
\hline
Privacy & \cite{to2014framework,pournajaf2014spatial,
Gong2015,zhang2015differentially,Hu2015,
to2016sc,Shen2016,Zhu2016,ni2016secure,
sun2017anonymity,liu2017protecting,liu2017privacy,liu2017efficient} \\
\hline
Trust & \cite{kazemi2013geotrucrowd,cheng2015reliable,
Feng2014,Boutsis2014,Song2014,
Wang2015,Zhao2015,Kang2015,An2015,Fan2015Online,
Zhang2016Reliable,Liu2016a} \\
\hline
Scalability & \cite{alfarrarjeh2015scalable} \\
\hline
Incentives & \cite{Lee2010,Alt2010,Yang2012,Jaimes2012,Heimerl2012,Musthag2012,
musthag2013labor,teodoro2014motivations,Rula2014,
thebault2015avoiding,Shah-Mansouri2015,Jin2015,
Fan2016Truthful,Jaiman2016,Guo2016,to2016empirical,Li2016,
Micholia2016,Miao2016,Kandappu2016a,Kandappu2016,Zhang2016Towards} \\
\hline
Applications & \cite{chen2014gmission,kim2014mediaq,to2015effectively,to2016scawg} \\
\hline
\end{tabular}
\caption{Spatial crowdsourcing studies.}
\label{tab:sc_studies}
\end{center}
\end{table}

\section{Privacy Threats}

There have been known attacks on SC applications, such as location-based attacks during tasking in the push mode~\cite{kazemi2011privacy} and collusion attacks during reporting in the pull mode~\cite{Wang2016} (see Table~\ref{tab:threats}). Despite the fact that most studies have solely focused on one of the two major threats, privacy risks to SC users may occur in the other scenarios: reporting in the push mode and tasking in the pull mode.
In this section we present a threat model which characterizes the \emph{full spectrum of privacy threats to workers and requesters during both tasking and reporting phases with either push or pull mode}. Next, we illustrate the privacy risks on TaskRabbit.

\begin{table}
\begin{center}
\begin{tabular}{ | c | c | c | } 
\hline
					& Tasking 				& Reporting \\
\hline
Push 	& \cite{kazemi2011privacy} 	& \cite{Shin2011}  \\  
\hline
Pull 	&  	[Sec.~\ref{sec:tasktabbit}]						& \cite{Wang2016,Shin2011}, [Sec.~\ref{sec:tasktabbit}] \\ 
\hline
\end{tabular}
\caption{Attacks on SC users.}
\label{tab:threats}
\end{center}
\end{table}

\subsection{Threat Model}
\label{sec:threat_model}
As the privacy threats vary according to the modes of task publishing, we discuss possible threats associated with each mode.

\paragraph{\textbf{Privacy Threats with the Push Mode}}

\begin{figure*}[ht]
	\begin{minipage}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{figs/threat_model_push}
		\subcaption{Push mode}
		\label{fig:threat_model_push}
	\end{minipage}
	\hspace{4pt}
	\begin{minipage}[b]{.48\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{figs/threat_model_pull}
		\subcaption{Pull mode}
		\label{fig:threat_model_pull}
	\end{minipage}
	\caption{Threat models in spatial crowdsourcing.}
	\label{fig:threat_model}
\end{figure*}

With the \emph{push} mode, the server takes as input the perturbed locations of both workers and tasks to perform effective task assignment; hence, there is a serious privacy threat from the server which might become a single point of attack. Figure~\ref{fig:threat_model_push} depicts the threat model for the push mode of spatial crowdsourcing. W and R denote workers and requesters, respectively. The dotted circles surrounding them denote that they are protected from a malicious entity shown in the first column of the first row in a dashed shaded box. After the tasking and reporting phases, the links between W and R represent the established connections during each phase. We refer to these links as the \emph{assignment link} and \emph{reporting link}. The dashed links indicate connections that are oblivious to the corresponding malicious entity.
The first row means that locations of workers and tasks are protected from the server at all the time. The role of the server is to create the \emph{assignment links} between the workers and the requesters so that they can establish a direct communication channel among themselves. Each worker-requester pair cooperatively decides whether to accept the assignment from the server. If yes, they send a \emph{consent} message to the server, confirming that the worker will perform the requester's tasks. This agreement is illustrated by the first \emph{reporting link} in Figure~\ref{fig:threat_model_push}. We argue that to preserve location privacy during both tasking and reporting phases, task locations need to be protected from the server. Otherwise, the completion of a task reveals that some workers must have visited the task's location. In restrictive privacy settings, workers and requesters can also be malicious to each other. Hence, to ensure minimum disclosure among them, only workers who aim to perform the tasks should know the tasks' locations (see the second row in Figure~\ref{fig:threat_model_push}). Likewise, a requester should only know the workers' locations once her tasks are matched to and then performed by those workers (see the third row in Figure~\ref{fig:threat_model_push}). 

We emphasize the minimum disclosure of location information for both workers and tasks. The reason for this is twofold. First, the server knows only the assignment links between workers and tasks. Due to such links, the assigned workers (or tasks) may infer that there exists nearby tasks (or workers). These disclosures are unavoidable in the push mode of SC. Second, the disclosure of workers' locations to their corresponding requester is inevitable at the reporting phase per definition of SC. It is worth mentioning that this threat model is restrictive; hence, weaker variants exist. For example, most existing studies in the push mode assume that workers are trusted~\cite{kazemi2011privacy,pournajaf2014spatial,Hu2015} and task locations are public~\cite{Vu2012,to2014framework,Gong2015,Zhang2015,Shen2016}.


\paragraph{\textbf{Privacy Threats with the Pull Mode}}

With the \emph{pull} mode, despite the fact that workers do not need to send their locations to the server, the locations can still be learned during both tasking and reporting phases. As long as a worker connects to the server to either \emph{request} some tasks or \emph{report} results, he may reveal to the server patterns of where and when the connections were made and what kind of tasks he wants to perform. Consequently, in~\cite{Shin2011}, the authors show that linking multiple requests or reports of the worker may allow an adversary to trace him since the worker's location information can be tracked through several stationary connection points (e.g., cell towers). In addition, the worker's location trace can be inferred by both the server and requesters since he must be in the vicinity of the tasks in order to perform them.
Figure~\ref{fig:threat_model_pull} depicts the proposed threat model for the pull mode. To preserve privacy and identity of the workers from the server, both assignment links and reporting links should be secure during tasking and reporting phases, respectively. This is because if the connections are discovered by the server, which already knows the locations of tasks, the server learns the locations of workers since they must have visited the locations of the performed tasks. Hence, the workers must request tasks without revealing their identity to the server; once the tasks are performed, the workers must also disassociate their connections with the performed tasks while uploading task content to the server.
Similar to the push mode, both workers and requesters themselves can be hostile to one another.
Thus, the privacy threats from workers and requesters (rows 2 and 3 in Figure~\ref{fig:threat_model_pull}) are similar to those in the push mode (rows 2 and 3 in Figure~\ref{fig:threat_model_push}), except the difference in the assignment links of the two second rows. The reason for this is that the requester is oblivious to the requests between the worker and the server during tasking.


%\subsection{Threat Model}

\subsection{Case Study of TaskRabbit}
\label{sec:tasktabbit}
We show that an adversary can perform harmful attacks on a typical SC application without much effort. TaskRabbit is a pull-based\footnote{We present the privacy threats to a pull-based SC system only; however, some of these privacy threats also occur in push-based SC such as iRain.} online and mobile marketplace that matches workers with requesters, allowing requesters to find immediate help with everyday tasks including, but not limited to, cleaning, moving, and delivery. In the following we discuss the aforementioned threats to TaskRabbit  users. Note that the following attacks on TaskRabbit.com were conducted in October 2014; the website has been updated since then.

We first show the breach of task location during tasking. 
We signed up as a worker account and searched for delivery tasks in Los Angeles; 2381 spatial tasks were found. We obtained various information about a particular task by clicking on it, such as description, price, task status and cloaked locations. Although each location is cloaked in a circle with a radius of half a km\footnote{We obtained this information via JavaScript code.} (Figure~\ref{fig:task_location}) to protect task locations from workers, the actual drop-off and pick-up locations were mentioned in the task description, i.e., \emph{``Please pick up a box of mini-muffins from (S) promptly at 8 am on Tues, 9/4, and drive them straight to me at (D).''}
It is also worth noting that task requests often contain sensitive information, such as health status of the requesters. An example of a sensitive task is one with title \emph{``super easy task deliver a bag to the doorstep of a sick friend.''}
Nonetheless, these privacy risks are due to the disclosure of task content, which is beyond the scope of this study.

We then show the leak of worker location during tasking and reporting. To gain a competitive advantage, a worker may wish to not disclose locations of his visits to other workers and requesters.
The task status (Figure~\ref{fig:task_status}) infers that the worker, referred to as Bob, was at the pick-up and drop-off locations of the task during the one-hour period between his assigned time and his completed time. The risk of precisely inferring Bob's locations is even higher for time-sensitive tasks such as delivery and help at home, which requires him to meet requesters in-person at a specific place and time. This inference attack shows that TaskRabbit does not guarantee privacy protection for the pull mode in Section~\ref{sec:threat_model}, which says that Bob's locations are private to the server and only requesters who have their tasks performed by Bob should know his locations.
In addition, one can also see much more information about Bob, including his previously performed tasks (Figure~\ref{fig:top_tasks}) and all reviews from the requesters who hired him. These associations between Bob and his performed tasks indicate that the assignment links and reporting links are known to the server.xeeeeeeeeeeeeeeeeeeeeeeeeeeee

\begin{figure*}[ht]
	\begin{minipage}[b]{.245\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{figs/task_location}
		\subcaption{Task locations}
		\label{fig:task_location}
	\end{minipage}
		\begin{minipage}[b]{.245\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{figs/task_price}
		\subcaption{Task price}
		\label{fig:task_price}
	\end{minipage}
	\begin{minipage}[b]{.245\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{figs/task_status}
		\subcaption{Task status}
		\label{fig:task_status}
	\end{minipage}
	\begin{minipage}[b]{.245\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{figs/top_tasks}
		\subcaption{Performed tasks}
		\label{fig:top_tasks}
	\end{minipage}
	\caption{Screenshots of TaskRabbit web application from worker Bob.}
	\label{fig:task_info}
\end{figure*}

Among Bob's requesters, we randomly picked one named Alice.
We further show that her home location can be learned by tracking her task requests.
We searched for household tasks that Alice requested in the past; three of them are shown in Table~\ref{tab:requested_tasks}. We replaced six digits after the decimal point of ``geo\_center'' by 'x' to protect the privacy of the requester. These tasks were in the proximity of each other and likely situated at her home. Our hypothesis is that the tasks' locations were randomly cloaked such that the cloaking regions covered the actual location of the tasks.
The location must be in the overlapped area using triangulation. We validated our hypothesis by confirming that the location of another task, whose location was known, is within the overlapped region. This attack suggests that the more task requests are posted, the more accurately their locations can be learned. This simple attack is against the threat model, which states that the locations of Alice's tasks should only be revealed to the workers who performed her tasks.

\begin{table}
\begin{center}
    \begin{tabular}{ p{5cm}  |   p{6cm}}
    \hline
	Task description & Corresponding JavaScript \\ \hline
    Quick post-party dishwashing clean up needed & \small{``radius'' : ``0.5'', ``geo\_center'' : \{``lat'' : ``33.xxxxxx'', ``lng'' : ``-118.xxxxxx''\}} \\ \hline
    Take down light Christmas decorations & \small{``radius'' : ``0.5'', ``geo\_center'' : \{``lat'' : ``33.xxxxxx'', ``lng'' : ``-118.xxxxxx''\}} \\ \hline
    Put up 20 yard sale signs in Mid-Wilshire area & \small{``radius'' : ''0.5'', ``geo\_center'' : \{``lat'' : ``33.xxxxxx'', ``lng'' : ``-118.xxxxxx''\}} \\ \hline
    \end{tabular}
	\caption{Three tasks requested by requester Alice.}
    \label{tab:requested_tasks}
    \end{center}
\end{table}

\section{Privacy Countermeasures}

\subsection{Location Privacy}
Location privacy has been studied first within the model of spatial $k$-anonymity \cite{gruteser2003anonymous,kido2005anonymous,mca06,gedik2008protecting,xue2009location,dewri2010query,palanisamy2011mobimix}, where the location of a user is hidden among $k$ other users. Other studies focused on space transformation to preserve location privacy~\cite{khoshgozaran2007blind,yiu2008spacetwist}.
Such techniques assume a centralized architecture with a trusted third party, which is a single point of attack. More recent work removes this assumption and provides cryptographic-strength protection~\cite{ghinita2008private}. While location privacy has largely been studied in the context of location-based services, only a few works have studied privacy in participatory sensing (PS)~\cite{kazemi2011towards,hu2010privacy,huang2009towards,cornelius2008anonysense}. The focus of~\cite{kazemi2011towards} is to privately assign a set of spatial tasks to each worker while other works \cite{hu2010privacy,huang2009towards} focus on preserving privacy in a PS campaign during the data contribution (i.e., how participants upload the collected data to the server without revealing their identities). Closest to our work is the approach from \cite{cornelius2008anonysense}, in which a privacy-preserving framework in pull mode is proposed, and the participants collect data in an opportunistic manner without the need to coordinate with the server. However, as discussed in Section~\ref{sec:taxonomy}, the pull mode yields poor results in practice. Furthermore, the privacy model in \cite{cornelius2008anonysense} does not provide formal privacy guarantees. In our work, we focus on the push mode in the context of differential privacy, the de-facto standard for data publication.

\subsection{Location Privacy in Spatial Crowdsourcing}

In this section we survey some state-of-the-art approaches addressing the privacy issues in spatial crowdsourcing. We first categorize the studies into two groups: \emph{tasking in the push mode} and \emph{reporting in the pull mode}. Subsequently, each subgroup is further classified according to the applied techniques. Within each subgroup we identify one key paper shown in boldface to be presented in depth while follow-up studies are briefly discussed. An overview of these studies is presented in Table~\ref{tab:papers}. The table shows that the studies solely focus on location privacy of workers and assume that the locations and content of tasks are public. Moreover, the server is regarded as a primary threat in all studies, while some consider workers and requesters as secondary adversaries. We also notice that the most recent studies focus on the push mode, which requires privacy protection during tasking. This problem is considerably more challenging when compared to the problem of privacy-preserving reporting in the pull mode.

\begin{table*}
\begin{center}
\caption{Overview of problem focuses (Re: reporting, Ta: tasking); privacy techniques used (Ps: pseudonym, Cl: cloaking, Pt: perturbation, Ex: exchange-based, En: encryption-based); threats (W: worker, T: requester, S: server); trusted third party (TTP); optimization type (ST: single task, MT: multiple tasks). x and (x) represent primary and secondary aspects, respectively.}
\label{tab:papers}
\begin{tabular}{l | c  c | c c  c  c  c   | c c | c  c  c | c  c }
\multirow{2}{*}{Paper} & \multicolumn{2}{ c |}{Phase} & \multicolumn{5}{ c |}{Techniques} & \multicolumn{2}{ c |}{Protection} & \multicolumn{3}{ c |}{Threats} & \multicolumn{2}{ c }{TTP} \tn\cline{2-15}
\multicolumn{1}{c |}{} & \centering Re & \centering Ta & \centering Ps & \centering Cl & \centering Pt & \centering En & \centering Ex  & \centering W & T \centering & \centering W & \centering R & \centering S  & \centering Yes & \centering No \tn
\hline
\cite{Shin2011}  											& x & x		& x & (x) & & (x) &       &  x & N/A	   	& & N/A & x 		  	 & x &  	 	 \tn 
\hline
\cite{Boutsis2013}  								& x & 		& (x) & & &  & x	    & x & N/A   	& (x) & N/A & x 				&  & x 		 		 \tn 
\hline
\cite{Zhang2016} 									& x & 		& &  & &  & x 	  &  x & N/A    	&  & N/A & x &  	         	& x			 \tn 
\hline
\cite{kazemi2011privacy}  							&  &	x		& (x) & x & & &      & x &   		& & (x) & x 		  		& x & 			 \tn 
\hline
\cite{Vu2012}  											& & x 		&  & x & & (x) & 	    & x &     	& (x) & (x) & x 				& x & 		\tn 
\hline
\cite{sun2017anonymity}  											& & x 		&  & x & & & 	    & x &     	&  & & x 				& x & 		 \tn 
\hline
\cite{to2014framework} 							&  & x 			& &  & x & &		     & x &      	& (x) & (x) & x 			 & x & 	 	 \tn 
\hline
\cite{Gong2015} 						&  & x  			& &  & x & &		    & x &   		& (x) & (x) & x 				& x & 	 		 \tn
\hline
\cite{zhang2015differentially} 						&  & x			& & & x & &		    & x &    		& (x) & (x) & x 				& x &  	 		  \tn 
\hline
\cite{to2016sc} 				&  &	x		& &  & x & & 	  &  x &    	& (x) & (x) & x 		    	  	& x  & 		 			 \tn 
\hline
\cite{pournajaf2014spatial} 					&  & x		& & x & & &		  &  x &    	& & (x) & x 			& x &  	 	 \tn 
\hline
\cite{Hu2015} 										&  & x			& & x & & &		    & x &     	& & (x) & x 				& x &   	 	 \tn 
\hline
\cite{Shen2016} 								&  & x			& & & & x &		   & x &    & (x) & (x) & x 							&   & x	 	  \tn 
\hline
\cite{liu2017protecting} 								&  & x			& & & & x &		   & x &    &  &  & x 							&   & x	   \tn 
\hline
\cite{liu2017privacy} 								&  & x			& & & & x &		   & x & x    &  &  & x 							&   & x	 \tn
\hline
\cite{liu2017efficient} 								&  & x			& & & & x &		   & x & x    &  &  & x 							&   & x	 \tn
\end{tabular}
\end{center}
\end{table*}

\paragraph{Protection in the Pull Mode}

Privacy protection in the pull mode has been studied in the context of participatory sensing.
In this section we highlight recent studies that often focus on the reporting phase of the pull mode. They use either \emph{pseudonymity}~\cite{Shin2011} or \emph{exchange-based} techniques~\cite{Boutsis2013,Zhang2016}. The pseudonymity method disassociates the connections between one's uploaded data and his/her identity while the latter exchanges workers' crowdsourced data and location information before uploading them to a server so that the server is uncertain about locations of individual workers.

\paragraph{Protection in the Push Mode}
While preserving privacy during reporting in the pull mode has been largely studied in the context of participatory sensing (a recent survey can be found in~\cite{Christin2016a}), recent SC studies focus on the more challenging phase of tasking. These studies generally assume the push mode. 
We emphasize that focusing on the tasking step in the push mode is the correct approach, given that SC workers have to physically travel to the task location. The completion of a task discloses the fact that some worker must have been at that location, and this is unavoidable in SC. 
Focusing on tasking also makes sense from a disclosure volume standpoint. During the assignment, all workers are candidates for participation; therefore, locations of all workers are exposed, absent a privacy-preserving mechanism. Nevertheless, after task request dissemination, only a few workers will participate in task completion, and {\em only if they give their explicit consent} (see the threat model for the push mode in Section~\ref{sec:threat_model}).

Various techniques have been proposed to protect location privacy of workers during task assignment in SC, including \emph{cloaking} (hide the accurate location in a cloaked region)~\cite{kazemi2012geocrowd,Vu2012,pournajaf2014spatial,Hu2015}, \emph{perturbation} (distort the actual location information by adding artificial noise)~\cite{to2014framework,Gong2015,Zhang2015,to2016sc} and \emph{encryption}~\cite{Shin2011,Shen2016}.

Several recent works addressed the topic of location privacy in spatial crowdsourcing~\cite{pournajaf2014spatial,to2015privgeocrowd,Gong15b,Gong15b,zhang2015differentially,hu2015protecting,Li2016}.
A recent survey in this area can be found in~\cite{pournajaf2015participant}.
Particularly, the approach in~\cite{Gong15a} proposes a framework that can protect the workers' location privacy when allocating tasks to the workers. The solution adopts the privacy model used in our work, i.e., it assumes a trusted CSP and differentially private location sanitization. Similar to our study published in~\cite{to2014framework,to2016sc}, they develop analytical models and task allocation strategies that balance privacy, utility, and system overhead. The CSP can integrate mobile server location and reputation information.

The work in~\cite{zhang2015differentially} studies reward-based spatial crowdsourcing that enables task assignment with optimized reward allocation. The authors also reuse our privacy framework introduced in~\cite{to2014framework}, in which the SC-server and workers are connected by a trusted Cell Service Provider (CSP). The PSD receives exact locations from workers and releases a sanitized view to the SC-server, which is used to perform task assignment. 
The approach in \cite{Gong15b} introduces a framework for protecting privacy of workers using location obfuscation techniques. This study assumes a trusted proxy to aggregate exact context (e.g., location) information from workers and output sanitized contexts to the SC-server. The authors develop an optimization model for task selection that maximizes the total expected utility of the server given constraints on privacy and efficiency. However, only worker statistics are protected under differential privacy, whereas locations are obfuscated using cloaking techniques. Location cloaking is vulnerable to background knowledge attacks, and does not provide sufficient protection strength. In contrast, our work provides the strong semantic protection guarantees of differential privacy. Finally, the recent work in~\cite{Li2016} addresses both location privacy and incentives in mobile sensing systems by proposing two credit-based privacy-aware incentive schemes. One of them assumes an online trusted third party, similar to our CSP-centered approach. 

Other studies~\cite{pournajaf2014spatial,hu2015protecting} use cloaking technique (i.e., not distinguish among other $k$ workers) thus do not provide rigorous privacy protection. Cloaking technique not only reveals workers' identities but also is prone to homogeneity attack~\cite{machanavajjhala2007diversity} when all $k$ workers are at the same location.
The study in~\cite{Li2016} uses cryptography, which is computationally expensive; thus, it is not designed for real-time spatial crowdsourcing applications.

%Spatial crowdsourcing (SC) has gained popularity in both the research community (e.g., \cite{kazemi2012geocrowd,to2014framework,tong2016online,liu2017protecting,tran2017real}) and industry (e.g., TaskRabbit, Gigwalk). A recent study \cite{to2015server} distinguishes SC from related fields, such as generic crowdsourcing, participatory sensing, volunteered geographic information, and online matching. We review the literature from the following two aspects: location privacy threats in SC and privacy-preserving task assignment in SC.

%\textbf{Location Privacy Threats:}
%Protecting location privacy in SC has attracted much interest. There have been known attacks on SC applications, such as location-based attacks during tasking in the push mode~\cite{Kazemi2011a} and collusion attacks during reporting in the pull mode~\cite{Wang2016}. A recent survey~\cite{to2017location} shows that locations of workers and requesters' tasks can be learned during both tasking and reporting phases. Hence, various techniques have been proposed to protect location privacy of workers during tasking, including cloaking (hide the accurate location in a cloaked region)~\cite{pournajaf2014spatial}, perturbation (distort the actual location information by adding artificial noise)~\cite{to2014framework,to2017differentially,wang2017location,jin2016dpsense} and encryption~\cite{liu2017protecting,pham2017privateride}. We compare these studies as follows.

%\textbf{Privacy-Preserving Task Assignment:}
%In~\cite{pournajaf2014spatial}, given the cloaking regions of a set of workers, the objective of the server is to match a set of spatial tasks to the workers such that task assignment is maximized while satisfying the travel budget constraint of each worker. Unlike our work, this study protects workers' locations solely. Also, the privacy guarantee is weaker than differential privacy, as cloaking is sensitive to the adversary's prior knowledge. Simultaneously, a differentially private framework is proposed to protect the privacy of workers' locations~\cite{to2014framework}. In~\cite{to2014framework}, the workers do not trust the server but send their locations to a trusted third party (i.e., cell service provider). The third party sanitizes the location data according to differential privacy and releases the so-called private spatial decomposition (PSD). The server performs task assignment using the statistics of the workers' location data provided by the PSD, while the workers refine the assignment using their exact locations. An extension of~\cite{to2014framework} that protects workers' locations across multiple timestamps is introduced in~\cite{to2017differentially}. This study investigates privacy budget allocation techniques across consecutive releases and employs post-processing techniques to reduce the inaccuracy introduced by the addition of noise. Our work improves~\cite{to2014framework,to2017differentially} in two main aspects. First, the proposed privacy model is broader than that in the prior work---workers and requesters may not trust each other and the server. Second, this work does not require any trusted third party to sanitize the location data; instead, individual users locally perform location sanitization on their smartphones.

%Geo-I has been recently used in task allocation to protect the privacy of workers' locations~\cite{wang2017location}. However, similar to the prior work~\cite{pournajaf2014spatial,to2014framework,to2017differentially}, this study assumes that task locations are known to the server. This can be a privacy threat to the tasks' requesters because the platform can infer requesters' locations from the locations of their tasks~\cite{to2017location}. In contrast, our framework aims to protect locations of both workers and tasks. We also emphasize the online setting rather than the offline variant as in~\cite{wang2017location}, in which the server must wait for a number of tasks to arrive prior to task assignment. In~\cite{jin2016dpsense}, Jin et al. propose a differentially private framework to select participants while participating in the spectrum-sensing tasks. Unlike ours, this study addresses a very different problem, in which the sensing locations are predetermined and publicly known.

%A few recent studies use encryption-based approaches~\cite{liu2017protecting,pham2017privateride}. In~\cite{liu2017protecting}, locations of workers and tasks are protected by homomorphic encryption. The platform performs task assignment based on the worker-task distances computed from the encrypted data. The workers receive the encrypted location of the assigned task and decrypt it to obtain the task location. While this approach guarantees exact assignment, its computation overhead is high. A closely related topic of research is the privacy-preserving ride-sharing service~\cite{pham2017privateride}. This work designs a cryptographic protocol to protect riders' and drivers' identifiable information, including location. However, this study uses a cloaking technique, which is vulnerable to background knowledge attack.



\chapter{Privacy-Preserving Task Assignment}\label{ch.PPTA}

\section{Preliminaries}
\subsection{Task Assignment: The Focus of SC}

Crowdsourcing is the process of outsourcing tasks to a distributed group of individuals. The difference
between crowdsourcing and ordinary outsourcing is that a task or problem is outsourced to an undefined
public rather than a specific body, such as paid employees.
Spatial crowdsourcing (SC)~\cite{kazemi2012geocrowd} is a type of online crowdsourcing enabled by mobile devices, where performing an outsourced task is only possible if the worker is physically at the location of the task. There are two modes of tasking based on how workers are matched to tasks: \emph{push} and \emph{pull}.

With the \emph{pull} mode, the server publicly\footnote{Exact geographical coordinates of the tasks may not be published; instead, their cloaked locations or representative names are provided.} publishes the spatial tasks, and online workers autonomously choose tasks in their vicinity without coordinating with the server. One advantage of the pull mode is that the workers do not need to reveal their locations to the server. However, one drawback of this mode is that the server does not have any control over the allocation of spatial tasks; this may result in some spatial tasks never be assigned, while others are assigned redundantly. Another drawback of the pull mode is that workers choose tasks based on their own objectives (e.g., choosing the $k$ closest spatial tasks to minimize their travel cost), which may not result in a globally optimal assignment. An example of the pull mode is TaskRabbit.

With the \emph{push} mode, requesters post tasks that include locations, while online workers send their locations to the server, which assigns tasks to nearby workers. The advantage of this mode is that unlike the pull mode, the server has the big picture and can assign to every worker his nearby tasks while maximizing the overall task assignment. However, the drawback is that locations of both tasks and workers should be sent to the server for effective assignment, which can pose privacy threats. An example of the push mode is Uber.

With the pull mode, the main focus of privacy protection is during the \emph{reporting} phase, which has been well studied in the context of participatory sensing (PS), e.g.,~\cite{Shin2011,kazemi2011privacy,Vu2012,Boutsis2013,Zhang2016}. With PS, the goal is to exploit the ability of mobile users to collect and share data using their sensor-equipped phones for a given campaign. Most studies on PS focus on small campaigns with a limited number of workers; hence, they do not have issues of task assignment. However, with SC, the focus is on devising a scalable, generic and multipurpose crowdsourcing framework, similar to Amazon Mechanical Turk, but spatial, where multiple campaigns can be handled simultaneously. Therefore, most SC studies, including ours, assume the push mode and emphasize privacy protection during the \emph{tasking} phase.

We emphasize that the benefits of offering privacy protection during the \emph{tasking} phase lies in the volume of disclosure. This is because even though all workers may appear eligible for participation, the tasking process based on distance can strictly limit the breadth of task dissemination. At the reporting phase, the disclosure of a task's location to its assigned worker and vice versa is inevitable per definition of SC. Once the worker visits the task's location, his location is known to the requester. This disclosure makes sense because we ensure the privacy of the individual location but not the entire location trace of the worker.

\subsection{Mathematically Rigorous Definitions of Privacy}
\subsubsection{Differential Privacy}
{\em Differential Privacy (DP)}~\cite{dwork2006differential} has emerged as the de-facto standard in data privacy, thanks to its strong protection guarantees rooted in statistical analysis. DP is a {\em semantic} model which provides protection against realistic adversaries with background information. Releasing data according to differential privacy ensures that an adversarys chance of inferring any information about an individual from the sanitized data will not substantially increase, regardless of the adversary's prior knowledge. DP ensures adversary do not know whether an individual is present or not in the original data. DP is formally defined as follows.


\newtheorem{differential_privacy}[definition]{Definition}\label{differential_privacy}
\begin{differential_privacy}[$\epsilon$-indistinguishability]~\cite{dwork2006calibrating}

Consider that a database produces set of query results $\hat{D}$ on the set of queries $Q$ = $\{q_1, q_2, \ldots, q_{|Q|}\}$, and let $\epsilon>0$ be an arbitrarily small real constant. Then, transcript $U$ produced by a randomized algorithm $A$ satisfies $\epsilon$-indistinguishability if for every pair of sibling datasets $D_1$, $D_2$ that differ in only one record, it holds that
$$\ln \frac{Pr[Q(D_1) = U]}{Pr[Q(D_2) = U]} \le \epsilon$$
\end{differential_privacy}

%%
In other words, an attacker cannot learn whether the transcript was obtained by answering the query set $Q$ on dataset $D_1$ or $D_2$. Parameter $\epsilon$ is called {\em privacy budget}, and specifies the amount of protection required, with smaller values corresponding to stricter privacy protection. To achieve $\epsilon$-indistinguishability, DP injects noise into each query result, and the amount of noise required is proportional to the {\em sensitivity} of the query set $Q$, formally defined as:
%%
\newtheorem{sensitivity}[definition]{Definition}\label{sensitivity}
\begin{sensitivity}[$L_1$-Sensitivity]~\cite{dwork2006calibrating}
Given any arbitrary sibling datasets $D_1$ and $D_2$, the sensitivity of query set $Q$ is the maximum change in the query results of $D_1$ and $D_2$
$$\sigma(Q) = \max_{D_1, D_2}||Q(D_1)-Q(D_2)||_1$$
\end{sensitivity}
%DP allows interaction with a database only by means of aggregate (e.g., count, sum) queries. 
There are multiple ways to achieve DP. One approach to achieving DP is adding random noise to each query result to preserve privacy, such that an adversary that attempts to attack the privacy of some individual $w$ will not be able to distinguish from the set of query results whether a record representing $w$ is present or not in the database~\cite{dwork2006calibrating}.
An essential result from~\cite{dwork2006calibrating} shows that a sufficient condition to achieve differential privacy with parameter $\epsilon$ is to add to each query result randomly distributed Laplace noise with mean 0 and scale $\lambda = \sigma(Q)/\epsilon$.

%In this study, we focus on counting queries whose sensitivity is one. This is because for any two neighboring datasets $D_1$ and $D_2$ created by adding/removing a record to/from them, their cardinalities differ by one. Therefore, $L(D) = Q(D) + Lap(1/\varepsilon)$. Note that $Lap(1/\varepsilon)$ denotes a random variable sampled from the Laplace distribution with scale parameter $1/\varepsilon$. With Laplace mechanism, the more noise added, the more privacy; however, the less precision. Thus, given the same privacy level, a good algorithm should be able to accurately answer the queries.

Typically, the interaction with a dataset consists of a series of analyses $A_i$, each required to satisfy $\epsilon_i$-differential privacy. Then, the privacy level of the resulting analysis is computed as follows:
 %%
\newtheorem{sequential_composition}[theorem]{Theorem}\label{sequential_composition}
\begin{sequential_composition} [Sequential Composition \cite{mcsherry2009differentially}]
Let $A_i$ be a set of analyses such that each provides $\varepsilon_i$-DP. Then, running in sequence all analyses $A_i$ provides ($\sum_{i}\varepsilon_i$)-DP.
\end{sequential_composition}
%%
\newtheorem{parallel_composition}[theorem]{Theorem}\label{parallel_composition}
\begin{parallel_composition} [Parallel Composition \cite{mcsherry2009differentially}]
If $D_i$ are disjoint subsets of the original database, and $A_i$ is a set of analyses each providing $\varepsilon_i$-DP, then applying each analysis $A_i$ on partition $D_i$ provides $\max{(\epsilon_i})$-DP.
\end{parallel_composition}

\subsubsection{Geo-Indistinguishability}\label{sec:geo-i}
{\em Geo-indistinguishability (Geo-I)}~\cite{andres2013geo} is a notion of location privacy based on differential privacy (DP). Similar to DP, Geo-I is a semantic model which provides protection against adversaries with background information.
A mechanism provides $\epsilon$-geo-indistinguishability if any two locations at distance at most $r$ produce observations with ``similar'' distributions bounded by $\epsilon$. We refer to this privacy guarantee as $(\epsilon,r)$-Geo-I. The parameter $\epsilon$ is the level of privacy, which specifies the amount of protection required, with smaller values corresponding to stricter privacy protection. The parameter $r$ is the radius of concern within which privacy is guaranteed. This means that an adversary cannot distinguish locations which are at most $r$ distance away. Geo-I is formally defined as follows.

%Let $\mathcal{Z}$ be a set of possible reported locations. 

\newtheorem{geo-indistinguishability}[definition]{Definition}\label{geo-indistinguishability}
\begin{geo-indistinguishability}[$(\epsilon,r)$-Geo-indistinguishability~\cite{andres2013geo}]
Let $\mathcal{X}$ be a set of exact locations. A mechanism $A$ satisfies $(\epsilon,r)$-geo-indistinguishability iff for all $\mathit{x,x'\in \mathcal{X}}$ such that $\mathit{d(x,x')\le r}$:
$$d_P(A(x), A(x'))\le \epsilon d(x, x')$$
$d(x,x')$ is the Euclidean distance between $x$ and $x'$ while $d_P(A(x), A(x'))$ is the multiplicative distance between the two distributions produced by $x$ and $x'$, correspondingly. Note that we use the \emph{constrained} version of Geo-I (i.e., $\mathit{d(x,x')\le r}$), which forces the corresponding distributions to be at most $\epsilon r$ distance from each other.
% while $d_P(A(x), A(x'))$ is the multiplicative distance between the two distributions produced by $x$ and $x'$, correspondingly. 
\end{geo-indistinguishability}
%In fact, this equation is similar to differential privacy where the $d$ is the Hamming distance between two neighboring databases $x, x'$.

To preserve privacy, \emph{random noise} is injected into each location such that, by observing a perturbed location, an adversary cannot infer the true location among all locations within radius $r$.
%attempts to attack the privacy of some individual $w$ will not be able to distinguish from the set of perturbed locations whether a location representing $w$ is present or not in the database.
Particularly, one way to achieve Geo-I is to generate random point $z$ (from actual point $x\in \mathcal{X}$) according to planar Laplace distribution. This function ensures that the probability of reporting a point in a certain (infinitesimal) area around $z$, when the actual locations are $x$ and $x'$, differs at most by a multiplicative factor $e^{-\epsilon d(x,x')}$. Hence, \emph{pdf} of the noise-adding mechanism is called \emph{planar Laplacian centered at} $x$.
\begin{equation}
d_\epsilon(x)(z)=\frac{\epsilon^2}{2\pi}e^{-\epsilon d(x,z)}
\label{eq:pdf}
\end{equation}
where $\frac{\epsilon^2}{2\pi}$ is a normalization factor.

%Geo-I provides privacy protection to a single individual's location data as opposed to aggregate information, which suits our \SCG\ framework to be presented next.

\subsection{Private Spatial Decompositions (PSD)}
\label{sec:PSD-prelim}
The spatial aggregation of users can be used in spatial crowdsourcing requires for optimization. For example, the number of workers within a certain region is used to maximize task assignment~\cite{to2014framework,to2015server}. With privacy, the problem is to publish a synopsis of two-dimensional datasets using differential privacy. The challenge is to enable accurate answers range count queries given a privacy budget.

%%% PSD, general overview
Existing methods often construct a hierarchy of the partitions, or lay a one or two-level equi-width grid over the data domain. Xiao et al.~\cite{xiao2010differentially} proposed imposing a fixed equal-width grid over the base data. It then partitions the data using kd-tree based on noisy counts in the grid. A heuristic was proposed to determine non-uniform nodes that will be split to minimize the non-uniformity error. In~\cite{xiao2011differential}, Xiao et al. applied Wavelet transformation over the dataset before adding noise to it, namely Privlet.
%Privlet ensures differential privacy and provides accurate answers for range-count queries. 
The study in \cite{cormode2012differentially} introduced the concept of private spatial decomposition (PSD) to release spatial datasets in a DP-compliant manner. PSD-based techniques have been shown to outperform both the cell-based methods in~\cite{xiao2010differentially} and the Privlet method~\cite{xiao2011differential}. A PSD is a spatial index transformed according to DP, where each index node is obtained by releasing a noisy count of the data points enclosed by that node's extent. Various index types such as grids, quadtrees or kd-trees \cite{samet2006foundations} can be used as a basis for PSD. 

%%% Data-independent vs dependent structures. Budget Allocation!
The accuracy of PSD is greatly influenced by the type of PSD structure and its parameters (e.g., height, fan-out). With space-based partitioning PSD, the split position for a node does not depend on the spatial distribution of data points. This category includes flat structures such as grids
%, or hierarchical ones such as BSP-trees (Binary Space Partitioning) 
and quadtrees~\cite{samet2006foundations}. The privacy budget $\epsilon$ needs to be consumed only when counting the data points in each index node. Typically, all nodes at same index level have non-overlapping extents, which yields a constant and low sensitivity of $2$ per level (i.e., changing a single location in the data may affect at most two partitions in a level). The budget $\epsilon$ is best distributed across levels according to the geometric allocation~\cite{cormode2012differentially}, where leaf nodes receive more budget than higher levels. The sequential composition theorem applies across nodes on the same root-to-leaf path, whereas parallel composition applies to disjoint paths in the hierarchy.
%Object-based PSD are simple to construct, but can become unbalanced.

PSD structures such as kd-trees \cite{cormode2012differentially} perform splits of nodes based on the placement of data points. To ensure privacy, split decisions must also be done according to DP, and significant budget may be used in the process. Typically, the exponential mechanism \cite{cormode2012differentially} is used to assign a merit score to each candidate split point according to some cost function (e.g., distance from median in case of kd-trees), and one value is randomly picked based on its noisy score. The budget must be split between protecting node counts and building the index structure.
%Such data-dependent PSD are more balanced in theory, but they are not very robust, in the sense that accuracy can decrease abruptly with only slight changes of the PSD parameters, or for certain input dataset distributions.

%%% Adaptive Grid
The study in \cite{qardaji2012differentially} compares tree-based methods with grid-based approaches and shows that uniform grid tends to perform better than recursive partitioning counterparts (e.g., kd-trees and quadtrees). The paper also proposes an adaptive grid approach, where the granularity of the second-level grid is chosen based on the noisy counts obtained in the first-level (sequential composition is applied). Adaptive grid is a hybrid technique which inherits the simplicity and robustness of space-based PSD, but still uses a small amount of data-dependent information in choosing the granularity for the second level. The same authors \cite{qardaji2013understanding} extend the grid-based approaches to arbitrary dimension. This study also shows that that branching factors and parameter settings can greatly influence the performance of hierarchical methods.

%However, this study does not suggest the optimal values of b and h. So I think adaptive grid (i.e., 2-level grid) in icde'13 is considered state-of-the-art. Only a small part of the paper Section 5.1 talks about 2 dimensional case, an generalized version of their icde'13 paper (i.e., grid) with varying branching factor b and depth h of the tree. 


\section{Protecting Locations of Workers Using Trusted Third Party}

\subsection{Privacy Framework}
\label{sec:fwork}
Section~\ref{sec:system1} presents the system model and the workflow for privacy-preserving SC.
Section~\ref{sec:privacy} outlines the privacy model and assumptions.  
Section~\ref{sec:metrics1} discusses design challenges and associated performance metrics.

\subsubsection{System Model}
\label{sec:system1}
We consider the problem of privacy-preserving SC {\em task assignment} in the push mode. Figure~\ref{fig:framework} shows the proposed system architecture. Workers send their locations (Step $0$) to a trusted {\em cellular service provider (CSP)} which collects updates and releases a PSD according to privacy budget $\epsilon$ mutually agreed upon with the workers. The PSD is accessed by the {\em SC-server} (Step $1$), which also receives tasks from a number of {\em requesters} (Step $2$). For simplicity, we focus on the single-SC-server case, but our system model can support multiple SC-servers.

When the SC-server receives a task $t$, it queries the PSD to determine a {\em geocast region (GR)} that encloses with high probability workers close to $t$. Due to the uncertain nature of the PSD, this is a challenging process which will be detailed later in Section~\ref{sec:geocast}. Next, the SC-server initiates a {\em geocast} communication \cite{navas1997geocast} process (Step $3$) to disseminate $t$ to all workers within $\mathit{GR}$. According to DP, sanitizing a dataset requires creation of fake locations in the PSD. If the SC-server is allowed to directly contact workers, then failure to establish a communication channel would breach privacy, as the SC-server is able to distinguish fake workers from real ones. Using geocast is a unique feature of our framework which is necessary to achieve protection. Geocast can be performed either with the help of the CSP infrastructure, or through a mobile ad-hoc network where the CSP contacts a single worker in the $\mathit{GR}$, and then the message is disseminated on a hop-by-hop basis to the entire $\mathit{GR}$. The latter approach keeps CSP overhead low, and can reduce operation costs for workers.  

Upon receiving request $t$, a worker $w$ decides whether to perform the task or not. If yes (Step $4$), she sends a {\em consent} message to the SC-server confirming $w$'s availability.
If $w$ is not willing to participate in the task, then no consent is sent, and no information about the worker is disclosed.

\begin{figure}[!htb]\centering
  \includegraphics[width=0.6\textwidth]{figures/DP-Arch}
  \caption{Privacy framework for spatial crowdsourcing}
  \label{fig:framework}
\end{figure}

\subsubsection{Privacy Model and Assumptions}
\label{sec:privacy}
Our specific objective is to protect both the {\em location} and the {\em identity} of workers {\em during task assignment}. Once a worker consents to a task, the worker herself may directly disclose information to the task requester (e.g., to enable a communication channel between worker and requester). However, such additional disclosure is outside our scope, as each worker has the right to disclose his or her individual information. Our focus is on what happens prior to consent, when worker location and identity must be protected from {\em both} task requesters and the SC server. 

We emphasize that focusing on the SC assignment step is the correct approach, given that SC workers have to physically travel to the task location. Mere completion of a task discloses the fact that some worker must have been at that location, and this is unavoidable in SC. To protect her location after consent, a worker can still enjoy some form of identity protection (e.g., using pseudonyms and anonymous routing), for which solutions are already available (e.g., TOR). On the other hand, no solution exists to date for the more challenging problem of privacy-preserving task assignment, hence we direct our efforts in this direction.
Focusing on task assignment also makes sense from a disclosure volume standpoint. During assignment, all workers are candidates for participation, so locations of all workers would be exposed, absent a privacy-preserving mechanism. On the other hand, after task request dissemination, only few workers will participate in task completion, and {\em only if} they give their {\em explicit consent}.

Workers cannot trust the SC-server, especially as there may be many such entities with diverse backgrounds, e.g., private companies, non-profits, government organizations, academic institutions, etc. On the other hand, the CSP already has a signed agreement with workers through the service contract, so there is already a trust relationship established, as well as mutually-agreed upon rules for data disclosure. Furthermore, the CSP already knows where subscribers are, e.g., using cell tower triangulation, so worker location reporting does not introduce additional disclosure. In addition, having the CSP expose a PSD release of the user location dataset can benefit applications beyond crowdsourcing. For instance, the PSD can be shared with law enforcement agencies for public safety, or with commercial organizations to increase the revenue of the CSP. Therefore, there is sufficient motivation for the CSP to provide such a location sanitization service.

However, the CSP has no expertise, and perhaps no financial interest, to host an SC service (or possibly multiple SC services with diverse requirements). Running such services requires dealing with a diverse set of issues such as interacting with various task requester categories, managing profiles (e.g., some workers may only volunteer for environmental tasks), etc. The role of the CSP is to aggregate locations from subscribed workers, transform them according to DP, and release the data in sanitized form to one or more SC-servers for assignment. As multiple SC-servers can use the same PSD, it is practical for the CSP to provide PSDs for a small fee, e.g., a percentage of the workers' payment, or a tax incentive in case of a public-interest SC application.

\subsubsection{Design Goals and Performance Metrics}
\label{sec:metrics1}
Protecting worker location complicates significantly task assignment, and may reduce the effectiveness and efficiency of worker-task matching. Due to the nature of DP, it is possible for a region to contain no workers, even if the PSD shows a positive count. Therefore, no workers (or an insufficient number thereof) may be notified of the task request. The task may not be completed. Alternatively, a worker may be notified of the task even though she is at a long distance away from the task location, whereas a nearer worker does not receive the request. 
Finally, in the non-private push mode, only one selected worker, whose location and identity is known, is notified of the task request. 
With location protection, redundant messages need to be sent, increasing overhead.
We focus on the following performance metrics:
\begin{itemize}
\item
{\bf Assignment Success Rate (ASR).} Due to PSD data uncertainty, the SC-server may incorrectly assign workers to tasks (e.g., no worker is reached, or task is too far and workers do not accept it). $\mathit{ASR}$ measures the ratio of tasks accepted by a worker
%\footnote{$\mathit{ASR}$ does not capture worker reliability, tasks may still fail to complete after being accepted. Our focus is on assignment success, reliability is outside our scope.} 
to the total number of task requests. The challenge is to maintain $\mathit{ASR}$ close to $100\%$.
\item
{\bf Worker Travel Distance (WTD).} The SC-server is no longer able to accurately evaluate worker-task distance, hence workers may have to travel long distances to tasks. The challenge is to keep the worker travel distance low, even when exact worker locations are not known.
\item
{\bf System Overhead.} Dealing with imprecise locations increases the complexity of assignment, which poses scalability problems. 
A significant metric to measure overhead is the {\underline a}verage number of {\underline n}otified {\underline w}orkers ({\bf ANW}).
This number affects both the {\em communication overhead} required to geocast task requests, as well as the {\em computation overhead} of the matching algorithm, which depends on how many workers need to be notified of a task request.
\end{itemize}

\subsection{Protection for Static Workers' Locations}
\subsubsection{Sanitizing Workers' Locations}
\label{sec:psd}

The first step in the proposed framework consists of building a PSD (at the CSP side) to be later used for task assignment at the SC-server. Building the PSD is an essential step, because it determines how accurate is the released data, which in turn affects $\mathit{ASR}$, $\mathit{WTD}$ and $\mathit{ANW}$. 
In this section, we extend the state-of-the-art {\em Adaptive Grid (AG)} method proposed in \cite{qardaji2012differentially} to address the specific requirements of the SC framework.
Table~\ref{tab:notation1} summarizes the notations used in our presentation.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{ l | l}
\hline
{\bf Symbol} & {\bf Definition} \tn
\hline
$\varepsilon$, $\varepsilon_i$ & Total privacy budget and level-$i$ budget \tn
\hline
$\alpha$ & AG budget split, $\alpha=0.5$ means $\varepsilon_1=\varepsilon_2$  \tn
\hline
$N$ & Total number of workers \tn
\hline
$N'$ & Noisy worker count of level-1 cells \tn
\hline
$m_i\times m_i$ & Level-$i$ grid granularity \tn
\hline
$\bar{n}$ & Expected noisy worker count of a level-2 cell \tn
\hline
$t$ & A task or its location, used interchangeably \tn
\hline
$c_i$ & A level-2 cell \tn
\hline
$n_{c_i}$ & Noisy worker count of $c_i$ \tn
\hline
$p^a_{c_i}$ & Acceptance rate of workers within $c_i$ \tn
\hline
$c'_i$ & Sub-cell of cell $c_i$ \tn
\hline
\end{tabular}
\caption{Summary of notations}
\label{tab:notation1}
\end{center}
\end{table}

PSDs based on uniform grids treat all regions in the dataset identically, despite large variances in location density. As a result, they over-partition the space in sparse regions, and under-partition in dense regions. AG avoids these drawbacks by using a two-level grid and variable cell granularity. At the first level, AG creates a coarse-grained, fixed-size $m_1\times m_1$ grid over the data domain. AG uses a data-independent heuristic to choose level-$1$ granularity as
$$m_1=max(10,\ceil[\Big]{ \frac{1}{4}\sqrt{\frac{N \times \epsilon}{k_1}} })$$
where $N$ is the total number of locations (this is considered known, but a high-precision estimate can also be found using a small fraction of $\epsilon$). $k_1=10$ is suggested in \cite{qardaji2012differentially}.

Next, AG issues $m_1^2$ count queries, one for each level-$1$ cell, using a fraction of the total privacy budget: $\epsilon_1 = \epsilon \times \alpha$, where $0<\alpha<1$. AG then partitions each level-$1$ cell into $m_2\times m_2$ level-$2$ cells, where $m_2$ is adaptively chosen based on the noisy count $N'$of the level-$1$ cell:
\begin{equation}
%\hspace{50px} 
m_2=\ceil[\Big] {\sqrt{{\frac{N' \times \epsilon_2}{k_2}}}}
\label{eq:m2}
\end{equation}
where $\epsilon_2 = \epsilon - \epsilon_1$ is the remaining budget, and the constant is set empirically in \cite{qardaji2012differentially} to $k_2=5$. The budget parameter $\alpha$ determines how privacy budget is divided between the two levels, and a setting of  $\alpha = 0.5$ is recommended \cite{qardaji2012differentially}.

Figure~\ref{fig:AG} shows a snapshot of an adaptive grid, with four level-$1$ cells $A$,$B$,$C$,$D$. Constructing a differentially private {\em AG} requires two steps. First, the noisy counts $N'$ of $A$,$B$,$C$,$D$ are computed by adding random Laplace noise with scale $\lambda_1=2/\varepsilon_1$. Second, based on the noisy counts, level-$1$ cells are further split into level-$2$ cells. According to Eq.~\eqref{eq:m2}, cell $D$, which has noisy count $200$ is partitioned according to a $3 \times 3$ grid, while the granularity for other cells is $2 \times 2$. Thereafter, {\em AG} adds to each level-2 cell ($c_i$, $i=1\ldots 21$) random Laplace noise with scale $\lambda_2=2/\varepsilon_2$. Finally, their corresponding noisy counts $n_{c_i}$ together with the structure of the {\em AG} are published.

\begin{figure}[!ht]
	\centering
		\includegraphics[width=0.65\columnwidth]{figures/tree}
	\caption{A snapshot of adaptive grid ($\varepsilon=0.5$, $\alpha = 0.5$)}
\label{fig:AG}
\end{figure}

Although AG yields good results for general spatial queries \cite{qardaji2012differentially}, it is not directly applicable to SC, due to its rigidity in choosing parameters. Specifically, the granularity $m_2$ of the level-$2$ grid is too coarse, leading to large geocast areas and high communication overhead. According to Eq.~\eqref{eq:m2}, the expected number of workers (i.e., noisy count) in a level-$2$ cell is: 
$$\bar{n}=N'/{m_2}^2=k_2/\epsilon_2$$

\begin{table}[!htb]
    \begin{subtable}{.5\linewidth}
      \centering
		\begin{tabular}{ l | c | c | r }
		\hline
		$\varepsilon$ & \textbf{$\varepsilon_2$} & \textbf{$m_2$} & \textbf{$\bar{n}$} \\
		\hline
		{1} & 0.5 & 3 & 11 \\
		\hline
		{0.5} & 0.25 & 2 & 25 \\
		\hline
		{0.1} & 0.05 & 1 & 100 \\
		\hline
		\end{tabular}
		\caption{Original AG ($k_2=5$)}
		\label{tab:avg_count_old}
    \end{subtable}%
    \begin{subtable}{.5\linewidth}
      \centering
		\begin{tabular}{ l | c | c | r }
		\hline
		$\varepsilon$ & \textbf{$\varepsilon_2$} & \textbf{$m_2$} & \textbf{$\bar{n}$} \\
		\hline
		{1} & 0.5 & 6 & 2.8 \\
		\hline
		{0.5} & 0.25 & 5 & 5.6 \\
		\hline
		{0.1} & 0.05 & 2 & 28.2 \\
		\hline
		\end{tabular}
		\caption{Modified AG ($k_2=\sqrt{2}$)}
		\label{tab:avg_count_new}
    \end{subtable} 
    \caption{Granularity $m_2$ and average count per cell $\bar{n}$ ($N'=100$)}
    \label{tab:avg_count}
\end{table}

Table \ref{tab:avg_count_old} presents different values of $m_2$ and $\bar{n}$ when varying total budget $\epsilon$ with $\alpha=0.5$. The values of $\bar{n}$ are large, especially for stricter privacy settings (i.e., lower $\epsilon$). For $\epsilon=0.1$, $\bar{n}$ is $100$. In practice, a geocast region is likely to include multiple PSD cells, hence $100$ is a lower bound on the $\mathit{ANW}$, while its typical values can grow much higher, leading to prohibitive communication cost. 

We propose a more suitable heuristic for choosing $k_2$. Recall that the primary requirement of SC task assignment is to achieve high $\mathit{ASR}$. To that extent, we want to ensure that the task request is geocast in a non-empty region, i.e., the real worker count is strictly positive. According to the Laplace mechanism of DP, each PSD count is the sum of noisy and real counts. Given the level-$2$ privacy budget $\epsilon_2$, we can also quantify the distribution of added noise, which has standard deviation $\mu=2\sqrt{2}/\epsilon_2$. Therefore, if the PSD count is larger than a significant fraction of $\mu$ (i.e., $\mu/2$), then with high probability there will be at least one worker in the level-$2$ cell.

Our objective is to increase the granularity $m_2$ in order to decrease overhead, but only to the point where there is at least one worker in a cell. Denote by $count_{PSD}$ the value reported by PSD for a certain level-$2$ cell. Given the $Lap(2/\varepsilon_2)$ distribution, the probability that the noisy count is larger than zero is expressed as
$$p_h=1-\frac{1}{2}exp(-\frac{count_{PSD}}{2/\epsilon_2})$$
Furthermore, to include at least one real worker in the cell with high probability, we want to have the PSD count larger than the noise with high probability. , i.e., $\bar{n}=k_2/\varepsilon_2\ge \sqrt{2}/\varepsilon_2$, so at the limit we set $k_2=\sqrt{2}$. The resulting probability of having non-empty cells is $p_h=1-\frac{1}{2}exp(-1/\sqrt{2})=0.75$.
According to Eq.~\eqref{eq:m2}, the corresponding granularity is $m_2=\ceil[\Big]{\sqrt{N'\varepsilon_2/\sqrt{2}}}$. Table \ref{tab:avg_count_new} shows that this new setting significantly reduces $\bar{n}$, and as a result $\mathit{ANW}$.


\subsubsection{Performing Task Assignment on Sanitized Data}
\label{sec:geocast}

When a request for a task $t$ is posted, the SC-server queries the PSD and determines a geocast region $\mathit{GR}$ where the task is disseminated. The goal is to obtain a high success rate for task assignment, while at the same time reducing the worker travel distance $\mathit{WTD}$ and request dissemination overhead $\mathit{ANW}$.

\paragraph{Task Localness and Acceptance Rate}
\label{sec:acceptance_rate}

Travel distance is critical in SC, as workers need to physically visit the task locations. Workers are more likely to perform tasks closer to their home or workplace \cite{musthag2013labor,kazemi2012geocrowd,alt2010location}. The work in \cite{musthag2013labor} shows that 10\% of all workers, denoted as {\em super-agents}, perform more than 80\% of the tasks. Among super-agents, 90\% have daily travel distance less than $40$ miles, and the average travel distance per day is $27$ miles. This property is referred to as \textit{task localness} \cite{kazemi2012geocrowd}. A related study \cite{hecht2010localness} addresses the localness of contents posted by Flickr and Wikipedia users, and proposes a {\em spatial content production model (SCPM)} that computes the \textit{mean contribution distance (MCD)} of each worker as follows:
\begin{equation}
\label{eq:mcd}
%\hspace{50px} 
MCD(w_i) = \displaystyle \sum\limits_{j=1}^{n}\frac{d(L_{w_i},L_{c_j})}{n}
\end{equation}
where $L(w_i)$ is the location of worker $w_i$, and $L_{c_j}$ are the locations of its $n$ contributions.

Based on Eq.~\eqref{eq:mcd}, we can find the \textit{maximum travel distance ($\mathit{MTD}$)} that a high percentage of workers are willing to travel to perform their assigned tasks.
For example, $\mathit{MTD}$ of super-agents in crowdsourcing markets studied in \cite{musthag2013labor} is 40 miles with 90\% cumulative ratio of contributors. Besides communication overhead, task localness is thus another reason to impose an upper bound on geocast region size. Intuitively, the maximum geocast region is a square area with side size equal to $2 \times \mathit{MTD}$ . Hereafter, we refer to $\mathit{MTD}$ as both the maximum travel distance and the maximum geocast region size.

We denote by \textit{acceptance rate (AR)} the probability $p^a (0\le p^a \le 1)$, that a worker accepts to complete a task for which s/he receives a request. We assume that all workers are identical and independent of each other in deciding to perform tasks. The work in \cite{musthag2013labor} studies reward-based SC labor markets and shows that super agents have an average AR of $90.73\%$ while other agents have an acceptance rate of $69.58\%$. Acceptance rate is much smaller in self-incentivized SC \cite{kazemi2012geocrowd}, where the workers voluntarily perform tasks, without receiving incentives.

A worker is more willing to accept nearby tasks, so we model acceptance rate as a decreasing function of travel distance. We consider two cases: {\em (i)} {\em linear}, where AR decreases linearly with distance starting from an initial {\em MAR (Maximum AR)} value (when the worker is co-located with the task) and {\em (ii)} {\em Zipf}, where acceptance rate follows Zipf distribution with skewness parameter $s$.
The higher $s$ is, the faster $p^a$ drops. $p^a$ is maximized when the worker is co-located with the task and becomes negligible at $\mathit{MTD}$. If the distance is larger than $\mathit{MTD}$, $p^a=0$.

\paragraph{Analytical Utility Model}
\label{sec:utility_model}
We develop an analytical {\em utility} model that allows the SC-server to quantify the probability that a task request disseminated in a certain $\mathit{GR}$ is accepted by a worker. Intuitively, the utility depends on the AR and on the worker count $\bar{w}$ estimated to be enclosed within $\mathit{GR}$. An SC-server will typically establish an {\em expected utility} threshold {\em EU} which is the targeted success rate for a task (this is a system goal, rather than an outcome).
Generally, $\mathit{EU}$ is considerably larger than an individual worker's $p^a$, so the $\mathit{GR}$ must contain multiple workers. 

We define $X$ as a random variable for the event that a worker accepts a received task: $\mathit{P(X=True)=p^a}$ and $\mathit{P(X=False)=1-p^a}$. Assuming $w$ independent workers, $\mathit{X\sim Binomial(w,p^a)}$. We define the {\em utility} of a geocast region covering $w$ workers as:
\begin{equation}
\label{eq:utility}
%\hspace{50px} 
U=1-(1-p^a)^w
\end{equation} 
$U$ measures the probability that at least one worker accepts the task. 
The utility definition can be extended to the case of redundant task assignment, where multiple workers are required to complete a task (please refer to Section~\ref{sec:redundant}).

\paragraph{Geocast Region Construction}
\label{sec:greedy}

Given task $t$, the GR construction algorithm must balance two conflicting requirements: determine a region that {\em (i)} contains sufficient workers such that task $t$ is accepted with high probability, and {\em (ii}) the size of the geocast region is small. The input to the algorithm is task $t$ as well as the worker PSD, consisting of the two-level AG with a noisy worker count for each grid cell.

The algorithm chooses as initial $\mathit{GR}$ the level-$2$ cell that covers the task, and determines its $U$ value. As long as utility is lower than threshold $\mathit{EU}$, it expands the $\mathit{GR}$ by adding neighboring cells. Cells are added one at a time, based on their estimated increase in $\mathit{GR}$ utility. 
Higher priority is given to closer cells. The algorithm stops either when the utility of the obtained $\mathit{GR}$ exceeds threshold $\mathit{EU}$, or when the size of $\mathit{GR}$ is larger than $\mathit{MTD}$, hence utility can no longer be increased. The $\mathit{GR}$ construction algorithm is a greedy heuristic, as it always chooses the candidate cell that produces the highest utility increase at each step.

\begin{algorithm} [ht]
\caption{\sc Greedy Algorithm (GDY)}
\small
\begin{algorithmic}[1]
\STATE Input: task $t$, $\mathit{MTD}$, $0$$<$$\mathit{EU}$$<$$1$%, acceptance rate function $F$\label{line:input}
\STATE Output: geocast region $\mathit{GR}$
\STATE $\mathit{MTD}$ is the square of size $2 \times \mathit{MTD}$ centered at $t$
\STATE Init $\mathit{GR}=\{\}$, utility $U=0$
\STATE Init max-heap $Q=\{$level-$2$ cell that covers $t\}$ \label{line:init_Q}
\STATE Remove $\{c_i,U_{c_i}\}\leftarrow Q$, $U_{c_i}$ is computed from Eq.~\eqref{eq:utility} \label{line:remove_Q}
\STATE If $c_i=Nil$, return $\mathit{GR}$ \COMMENT{GR is larger than $\mathit{MTD}$}
\STATE $\mathit{GR=GR}\cup c_i$ \label{line:update_q}
\STATE If $U_{c_i}\ge 0$, $U=1-(1-U)(1-U_{c_i})$ \label{line:update_u_inc}
\STATE If $U\ge EU$, return $\mathit{GR}$ \label{line:return_u}
\STATE Find $\mathit{neighbors} = \{\{{c_i}'s\ \mathit{neighbors\}}-\mathit{GR}\} \cap \mathit{MTD}$ \label{line:new_neighbors}
\STATE $Q=Q\cup \mathit{neighbors}$ \label{line:update_heap}
\STATE Goto Line \ref{line:remove_Q}
\end{algorithmic}
\label{alg:geocast}
\end{algorithm}

The pseudocode of the greedy algorithm is depicted in Algorithm~\ref{alg:geocast}. In Line \ref{line:init_Q}, $Q$ is a heap of cells $\{c_i\}$, sorted decreasingly according to cell utility $U_{c_i}$. $U_{c_i}$ is computed according to Eq.~\eqref{eq:utility}, namely $U_{c_i}=1-(1-p^a_{c_i})^{n_{c_i}}$, where $n_{c_i}$ is the noisy worker count of $c_i$, and $p^a_{c_i}$ is the acceptance rate of the workers inside $c_i$. Since worker locations within a cell are not known, we assume they all have the same acceptance rate. Moreover, we assume the worker-task distance is equal to the average distance between the task and each four corners of cell $c_i$. 

When a candidate cell is removed from $Q$ (Line \ref{line:remove_Q}), it is added to $\mathit{GR}$ (Line \ref{line:update_q}), and $\mathit{GR}$ utility is updated in Line \ref{line:update_u_inc}. Updated utility $U'$ is the probability that a worker in either the current geocast region or the newly added cell performs the task: 
$$U' = U(1-U_{c_i}) + (1-U)U_{c_i} + UU_{c_i}=1-(1-U)(1-U_{c_i})$$
Line \ref{line:new_neighbors} computes the new neighboring cells that are not in $\mathit{GR}$, and are not situated farther than $\mathit{MTD}$. These cells are added to $Q$ according to their utilities. If a cell resides partially outside $MTD$, it is pruned to its fraction contained within the $MTD$, and its noisy count is updated proportionally to the pruned area.

In summary, the geocast region construction algorithm greedily expands the $\mathit{GR}$ by choosing to include at each step the grid cell that results in the highest estimated increase in utility. Cell utility takes into account the noisy worker count, as well as the distance between the cell and the task location. Next, we consider two refinements to the heuristic: first, in Section~\ref{sec:part_cell} we investigate a finer-grained solution search space by allowing partial cell inclusion; second, in Section~\ref{sec:shape} we consider the effect that the $\mathit{GR}$ shape has on hop-by-hop task request dissemination.

\paragraph{Partial Cell Selection}
\label{sec:part_cell}

Even though the adaptation of AG proposed in Section~\ref{sec:psd} significantly reduces the granularity of level-$2$ cells, the number of workers can still be rather large, and the resulting $\mathit{ANW}$ can lead to high task request dissemination costs. Such workers may be unnecessarily included in the $\mathit{GR}$, even if the required $\mathit{EU}$ could be achieved with far fewer workers. We propose an optimization that allows partial inclusion in the $\mathit{GR}$ of a level-$2$ cell.

Before adding a new cell $c_i$ to the $\mathit{GR}$ (Line \ref{line:return_u} of Algorithm~\ref{alg:geocast}), the optimization checks whether the utility increase provided by $c_i$ will exceed the required utility $\mathit{EU}$. If so, the algorithm computes a sub-region of $c_i$ whose utility is sufficient to reach $\mathit{EU}$. The pseudocode of the heuristic is depicted in Algorithm~\ref{alg:partial_cell}, which includes two steps. First, it computes the percentage of $c_i$'s area (Lines \ref{line:compute_distance}-\ref{line:compute_percentile}) that is likely to enclose sufficient users. Next, it finds a sub-cell with that area (Lines \ref{line:one_cell}-\ref{line:multiple_cell}) which is uniquely determined by its shape and location. The optimization in Algorithm~\ref{alg:partial_cell} can be inserted as a function call before Line \ref{line:return_u} in the main Algorithm~\ref{alg:geocast}.

\begin{algorithm} [ht]
\caption{\sc Partial Cell Selection Heuristic}
\small
\begin{algorithmic}[1]
\STATE Input: task location $t$, last cell $c_i$, current utility $U_{curr}$\label{line:input_partial_cell}
\STATE Output: $sub$-$cell$ $c_i'$ of $c_i$
\STATE $dist=distance(t,c_i)$ \label{line:compute_distance}
\STATE $p^a_{sub}=acc\_rate(dist)$
\STATE $U_{required} = \frac{EU-U_{curr}}{1-U_{curr}} $ \label{line:u_need}
\STATE Worker count needed to achieve $w_{required}=\log_{1-p^a_{sub}}^{1-U_{required}}$ \label{line:w_need}
\STATE Area $percentile = w_{required}/w_{c_i}$ \label{line:compute_percentile}
\STATE If $c_i$ covers $t$, find $sub$-$cell$ given area $percentile$ \label{line:one_cell}
\STATE Otherwise, find $sub$-$cell$ adjacent with current region \label{line:multiple_cell}
\end{algorithmic}
\label{alg:partial_cell}
\end{algorithm}

To compute a sub-cell, two constraints need to be satisfied. First, the sub-cell needs to be completely inside the parent cell. Second, the sub-cell must be adjacent with the current $\mathit{GR}$ to form a continuous region. Therefore, depending on whether or not the current $\mathit{GR}$ contains one or multiple cells, we use two strategies to find the sub-cell. 

\begin{figure}[tbh]
	\begin{minipage}[b]{.45\linewidth}
	\centering
		\includegraphics[width=0.8\textwidth]{figures/partial_cell1}
		\subcaption{Case 1: splitting $c_i$}
		\label{fig:partial_cell1}
	\end{minipage}
	\hspace{8pt}
	\begin{minipage}[b]{.45\linewidth}
	\centering
		\includegraphics[width=0.8\textwidth]{figures/partial_cell2}
		\subcaption{Case 2: splitting cell 7}
		\label{fig:partial_cell2}
	\end{minipage}
	\caption{Examples of partial cell selection.}
\label{fig:examples}
\end{figure}

Figure~\ref{fig:partial_cell1} depicts the first case where the $\mathit{GR}$ includes only one grid cell $c_i$ (i.e., the task $t_0$ is inside $c_i$, the parent cell). Intuitively, to cover the closest workers to the task, the shape of the sub-cell $c_i'$ (dashed line) must be a square. The boundary of cell $c_i'$ can therefore be completely determined given its area. To satisfy the first constraint, the center of $c_i'$ needs to be in the shaded square, whose center is the same as that of $c_i$, and its size is equal to the difference between the side lengths of $c_i$ and $c_i'$. In addition, the position of $c_i'$ is such that the distance between its center and the task is minimized.
%%%
The distance is zero when the task (e.g., $t_0$) is inside the shaded region (the task is co-located with $c_i'$'s center).
%%%
Otherwise, if the task is outside the shaded square, its closest sub-cell's center must be on the border of the shaded square. Subsequently, depending on the relative position of the task to the shaded circle (i.e., eight possibilities $t_1$-$t_8$), we can find the corresponding sub-cell's center. E.g., the closest sub-cell's center of $t_1$ is the left bottom corner of the shaded square.% while that of $t_2$ with $t_2$ form a perpendicular segment to the square.

Figure~\ref{fig:partial_cell2} presents the second case, in which the $\mathit{GR}$ comprises of multiple cells \{4,7,10,13\}. This example is a flat version of the AG in Figure~\ref{fig:AG}. The arrows depict the expansion process of the geocast algorithm. For example, cells 4 and 13 are expanded from cell 10 while cell 7 is expanded from cell 13. To ensure the $\mathit{GR}$ is a continuous region, we require the long edge of the sub-cell (dashed rectangle) to be adjacent to the neighbor cell (i.e., 13) that the splitting cell (i.e., 7) is expanded from. When its long edge is fixed, the sub-cell is uniquely specified given its area. The rationale behind this choice is to ensure the continuity constraint. 

\paragraph{Communication Cost}
\label{sec:shape}
Dissemination of a task request within the $\mathit{GR}$ can be implemented in two ways:
\begin{itemize}
\item
{\em Infrastructure-based Mode.} In this mode, the CSP sends an individual message to each worker within the $\mathit{GR}$. The cost is proportional to $\mathit{ANW}$, which may be large.
\item
{\em Infrastructure-less Mode.} Workers within the $\mathit{GR}$ can relay the task request hop-by-hop, using a mobile ad-hoc network protocol over WiFi. In this case, the CSP only needs to send several messages to workers (one single message may suffice if the worker network is connected).  
\end{itemize}

Geocasting using hop-by-hop communication is an attractive alternative. The SC-server does not know the actual worker placement, so the $\mathit{GR}$ construction strategy cannot rely on detailed routing information, but fortunately, the shape of the $\mathit{GR}$ is often a good predictor of ad-hoc routing performance. Intuitively, it is cheaper to geocast within a shape with less skew, such as a circle or a square, as opposed to skewed regions such as line-shaped areas, which have large network diameter. For instance, in Figure \ref{fig:partial_cell2}, the region of cells \{1,2,3,4\} is more favorable for geocast than \{2,4,5,6\}, despite the fact that the two areas have equal size. 

We assume that the geocast cost is proportional to the minimum bounding circle that covers the $\mathit{GR}$. Thus, the more {\em compact} the $\mathit{GR}$, the lower the cost.
%Several measures of compactness for two-dimensional shapes are discussed in \cite{li2013efficient}. 
One widely accepted measure proposed in \cite{kim1984digital}    is the {\em Digital Compactness Measurement (DCM)}, which measures region compactness as the ratio between the area of the region and the area of its smallest circumscribing circle. An efficient solution to find the smallest enclosing circle is a randomized algorithm \cite{welzl1991smallest} that runs in linear time to the number of data points in the region. The maximum value of $\mathit{DCM}$ is $1$ when the shape is a circle.

We modify Algorithm~\ref{alg:geocast} to choose new cells to add to $\mathit{GR}$ based on compactness, instead of utility. At each iteration, the cell that increases most the compactness of the $\mathit{GR}$ is chosen from the list of candidates. Due to the inclusion of the new cell, the potential compactness increase of all other candidates may need to be re-computed, to account for the change in shape. 
We also consider a hybrid method that factors in both utility and compactness in cell selection. The merit function of the hybrid is a linear combination of the resulting $\mathit{GR}$ utility and compactness. 

To evaluate the effectiveness of using compactness in the $\mathit{GR}$ search strategy, we use as metric an estimation of the hop count required to disseminate the task request to all workers, given the communication range of the wireless network (e.g., 50-100 meters for WiFi). We approximate the hop count as the diameter of the network divided by the communication range:
\begin{equation}
\label{eq:hops_count}
	\mathit{Hop\ count} = \displaystyle \frac{\mathit{farthest\ distance\ between\ two\ workers}}{2\times\mathit{communication\ range}}
\end{equation}
In practice, the worker network needs to be connected for the ad-hoc geocast to succeed. A message from any worker (i.e., seed) should be able to reach any other in the $\mathit{GR}$, using hop-by-hop wireless communication. Otherwise, if the network contains multiple disconnected components, the task cannot be sent to all workers from a single seed. In the latter case, the CSP would need to send the task to multiple seeds within the ad-hoc network. However, this level of detail goes beyond the scope of our work, and we restrict ourselves to using the hop count metric as an estimation of geocast cost, in conjunction with $\mathit{ANW}$.


\paragraph{Redundant Task Assignment}
\label{sec:redundant}
In some spatial crowdsourcing applications, multiple workers may be required to complete a task~\cite{kazemi2013geotrucrowd}. In this section, we extend our technique to support redundant assignment.
Eq.~\eqref{eq:utility} can be extended as follows.
\begin{equation}
\label{eq:utility_r}
U = 1-\sum_{i=1}^{K}U^i = 1-\sum_{i=1}^{K}\binom{w}{i}(p^a)^i(1-p^a)^{w-i}
\end{equation} 
where $K$ is the number of workers required to perform the task and $U^i=\binom{w}{i}(p^a)^i(1-p^a)^{w-i}$ is the probability that exactly $i$ workers perform the task.

The geocast algorithm can be extended to the case where at least $K$ workers in either the current geocast region $\mathit{GR}$ or the newly added cell $c_i$ perform the task. The updated utility can be computed based on the probability of having at most $K-1$ workers perform the task. Particularly, Line~\ref{line:update_u_inc} of Algorithm~\ref{alg:geocast} can be updated as follows:
\begin{equation}
\label{eq:update_u_r}
U' = 1-\sum_{j=0}^{K-1}U_{c_i}^j\sum_{l=0}^{K-j-1}U^l
\end{equation}
where $U_{c_i}^j=\binom{n_{c_i}}{j}(p^{c_i})^j(1-p^{c_i})^{n_{c_i}-j}$ is the probability that exactly $j$ workers in $c_i$ perform the task and $\sum_{l=0}^{K-j-1}U^l$ is the probability that at most $K-j-1$ workers in $\mathit{GR}$ perform the task. Note that $U_{c_i}^j=0$ if $j>n_{c_i}$.

\subsection{Protection for Dynamic Workers' Locations}
\label{sec:psd_dynamic}

Spatial crowdsourcing systems receive continuously requests for task assignment. Hence, it is important to keep track of whereabouts of {\em moving} users and to release a {\em sequence} of worker PSDs that allow effective spatial task assignment over multiple timestamps. We extend our solution for single-snapshot PSD to the case of dynamic worker datasets.

\subsubsection{Problem Statement and Baseline Solution}
We assume a discrete time model, where the SC server receives task requests at $T$ discrete timestamps, $\{t_1, \ldots, t_T\}$. The SC generates a sequence of PSDs $\{PSD_1, \ldots, PSD_T\}$, one for each timestamp $t_k$, and processes each task request received at timestamp $t_k$ according to private decomposition $PSD_k$.

Since many (or all) of the workers appear in multiple PSDs at distinct timestamps, an adversary has more opportunities to breach the location privacy of workers by correlating information from consecutive PSDs. In the context of differential privacy, this increased knowledge available to the adversary is modeled as an increase in $L_1$-sensitivity. Specifically, in the worst case, the sensitivity grows from $2$ for a single snapshot to $2 \times T$ for a set of $T$ released PSDs. On the other hand, the amount of privacy budget $\epsilon$ remains unchanged. Hence, the problem of privacy-preserving spatial task assignment for multiple snapshots is significantly more challenging than its single-snapshot counterpart. 

\noindent
{\bf Problem Statement.} Given a set of discrete timestamps $\mathcal{T} = \{t_1, \ldots, t_T\}$, $T>1$, and a privacy budget $\epsilon$, release a set of private spatial decompositions $\{PSD_1, \ldots, PSD_T\}$ that satisfies $\epsilon$-differential privacy and supports effective spatial task assignment as measured by performance metrics $\mathit{ASR}$, $\mathit{WTD}$ and $\mathit{ANW}$.

A baseline solution, denoted as $\mathit{BasicD}$ ({\em basic dynamic}), is to repeatedly apply our single-snapshot PSD algorithm at every timestamp, disregarding correlation between data at different timestamps.  At each timestamp, only a fraction $\frac{\epsilon}{T}$ of the entire privacy budget is used, which according to the sequential composition theorem~\cite{cormode2012differentially} guarantees that the overall algorithm satisfies $\epsilon$-differential privacy. The {\em BasicD} pseudocode is presented in Algorithm \ref{algo:basicd}.  However, as $T$ grows, the amount of budget received by the PSD at each timestamp decreases, resulting in a high amount of noise that needs to be added to each worker count. Consequently, the released sanitized data become unusable, and the quality of task assignment decreases significantly. Next, we propose a technique that addresses the limitations of {\em BasicD}. 

\begin{comment}
\begin{algorithm}[ht]
\small
\begin{algorithmic}[1]
\STATE \textbf{Input:} $T$, worker location datasets $\{D_1, ..., D_T\}$, budget $\epsilon$ \\
\STATE \textbf{Output:} $\mathit{PSD_1}$ \\
\STATE  Publish $PSD_1$ of $D_1$ with budget $\epsilon$\\
\end{algorithmic}
\caption{BasicS Algorithm} \label{algo:basics}
\end{algorithm}
\end{comment}

\begin{algorithm}[ht]
\small
\begin{algorithmic}[1]
\STATE \textbf{Input:} $T$, worker location datasets $\{D_1, ..., D_T\}$, budget $\epsilon$\\
\STATE \textbf{Output:} $\{\mathit{PSD_1}, \ldots, \mathit{PSD_T} \}$ \\
\STATE For $k=1$ to $T$:
\STATE \hspace*{4mm} Publish $k^{th}$ PSD of $D_k$ with budget $\epsilon/T$\\
%\STATE \hspace*{4mm} $\mathit{PSD_k}$ $\leftarrow$ Adjust consistency of $\mathit{PSD_k}$
\end{algorithmic}
\caption{BasicD Algorithm} \label{algo:basicd}
\end{algorithm}

\subsubsection{Multiple-Snapshot Worker PSD}

We build upon our single-snapshot solution and augment it with an adaptation of FAST~\cite{Fan14TKDE}, a recent approach to continuously release private counts on top of adaptive grids. 
FAST constructs an internal process model that captures the temporal correlation of aggregated location traces. It improves accuracy of released data at each timestamp by performing posterior estimation to reduce perturbation error.  Also, when $T$ is large, FAST samples the time series of aggregate data to reduce overall perturbation cost.

The main idea of the proposed dynamic worker PSD algorithm is to spend a fraction of the privacy budget to build the structure of the adaptive grid in the first time instance. During subsequent time instances, the AG structure is unchanged, and only the counts of the level-2 cells are updated, according to the new configuration of the dataset.
Figure~\ref{fig:fast_workerpsd} illustrates how FAST is integrated within the proposed worker PSD framework, and Algorithm~\ref{algo:kalman} provides the pseudocode.
At timestamp $t_1$, the algorithm releases the noisy counts of level-2 cells using the Laplace mechanism. In each of the following timestamps, the worker PSD structure aggregates the new \emph{true count} and the old \emph{published counts} of every level-2 cell as the input to the FAST component.

\begin{figure}[!htb]\centering
  \includegraphics[width=0.7\textwidth]{figures/fast_workerpsd}
  \caption{Workflow for dynamic worker PSD computation}
  \label{fig:fast_workerpsd}
\end{figure}


\begin{figure}[!htb]\centering
  \includegraphics[width=0.7\textwidth]{figures/workerpsd_fast}
  \caption{Two-level grid for dynamic worker PSD}
  \label{fig:workerpsd_fast}
\end{figure}

Figure \ref{fig:workerpsd_fast} illustrates the two-level grid decomposition representing the worker PSD. At the first timestamp, a fraction $\epsilon_1 = f \times \epsilon$ of the privacy budget is used to determine the geometry of the level-2 grid (Line 3 in Algorithm \ref{algo:kalman}). The resulting decomposition is represented by cells with continuous borders in Figure \ref{fig:workerpsd_fast}. The remaining budget, amounting to $\epsilon_2 = (1-f)\times \epsilon$, is used to release noisy counts for timestamps $t_1$ to $t_T$, according to FAST approach (Lines 4-8). With adaptive grid, level-2 cells do not overlap, so according to \cite{cormode2012differentially}, each set of counts corresponding to the same cell at distinct timestamps receives budget $\epsilon_2$ (Line \ref{line:apply_fast}).

\begin{algorithm}[ht]
\small
\begin{algorithmic}[1]
\STATE \textbf{Input:} $T$, locations $\{D_1,...D_T\}$, budget $\epsilon$\\
\STATE \textbf{Output:} $\mathit{PSD}$ \textit{\{each second-level cell maintains a list of noisy counts\}}
\STATE Build $\mathit{PSD}$ structure of $D_1$ given budget $\epsilon \times f$ \label{line:ag_structure}
\STATE For $c^i$ in second-level cells of $\mathit{PSD}$: \label{line:each_2ndcell}
\STATE \hspace*{4mm} For $k=1$ to $T$:
\STATE \hspace*{8mm} $c_k^i$ = actual count of $c^i$ in $D_k$ \\
\STATE \hspace*{4mm} Apply FAST to \{$c_1^i,c_2^i,...,c_T^i$\} given budget $(1-f) \times \epsilon$ \\ \label{line:apply_fast}
\STATE \hspace*{4mm} $\mathit{PSD}$ $\leftarrow$ update noisy counts \{$n_{c_1^i},n_{c_2^i},...,n_{c_T^i}$\} of $\mathit{PSD}$
\end{algorithmic}
\caption{Dynamic Worker PSD} \label{algo:kalman}
\end{algorithm}

\noindent
\textbf {Choosing an appropriate $f$ value.}
A larger $f$ value allocates more budget for PSD construction in the initial timestamp, which increases the number of level-2 cells, providing finer granularity. However, a larger $f$ also reduces the budget for later timestamps, which may lead to high noise-to-real-count ratio. To balance these two factors, we propose a simple analytical model to select $f$.

Assume a simplified version of Algorithm~\ref{algo:kalman} where the noisy counts of individual PSD cells are obtained by adding Laplace noise directly with privacy budget $\frac{(1-f)\epsilon}{T}$. Denote by $count_{PSD}$ the value reported by PSD for a certain level-2 cell. Given the $Lap(\frac{2T}{(1-f)\epsilon})$ distribution, denote the probability that the expected noisy count is larger than zero by

$$p_h=1-\frac{1}{2}exp(-\frac{count_{PSD}(1-f)\epsilon}{2T})$$

Our goal is to have PSD count larger than the expected noise. From Section~\ref{sec:psd}, the average number of workers in a level-2 cell is $\bar{n}=\sqrt{2}/\varepsilon_2=2\sqrt{2}/(f\epsilon)$. Thus, $$p_h=1-\frac{1}{2}exp(-\frac{\sqrt{2}(1-f)}{Tf})$$

Figure~\ref{fig:choose_f} illustrates the analytical dependence of $p_h$ on $f$. Note that, $p_h$ asymptotically decreases to 0.5 when $f$ increases. This result suggests that to avoid excessive noise-to-real-count ratio, especially when $T$ is large, $f$ should be set to a relatively small value, e.g., $f \in [0.1,0.2]$. In Section 7, we explore the effect of varying $f$ on system accuracy and performance.

\begin{figure}[!htb]\centering
  \includegraphics[width=0.5\textwidth]{exps/choose_f}
  \caption{The effect of $f$ on probability $p_h$}
  \label{fig:choose_f}
\end{figure}

\subsubsection{FAST Optimizations}

To improve accuracy, FAST employs two operations, {\em filtering} and {\em sampling}. Next, we describe the functionality of these steps.

\noindent
\textbf{Filtering.}
The filtering module generates estimates of monitored aggregates to improve the accuracy of counts released at each timestamp.   
%In our context, ``filtering'' refers to determining the posterior distribution, 
Given the previously observed noisy values, filtering attempts to determine an optimal posterior estimate.  Two operations, \textit{prediction} and \textit{correction}, are recursively applied during the entire movement history $T$.   The \textit{prediction} step generates a prediction of the worker count in a cell at each timestamp based on previous estimates and an internal process model, which provides the temporal correlation between adjacent aggregate values.  The \textit{correction} step combines the noisy observation, when available, with the prediction to generate a posterior estimate.  The correction mechanism varies according to the filtering method. To model worker movement, we use the Kalman filter~\cite{KALMAN60}, which was specifically designed for tracking moving targets, and has high computational efficiency.

We denote by $\{x_1^i,x_1^i,...,x_T^i\}$ the time series values of true counts of a particular cell $c^i$. We assume that $x_k^i$ follows a constant \textit{process model} by the following equations, in which $\omega^i_k$ is the white Gaussian noise with variance $Q^i$.
\begin{equation}
%\hspace{50px}  
x_{k+1}^i = x_k^i + \omega^i_k ; \omega^i_k \sim  \mathbb{N}(0, Q^i)
\label{eq:linear_model}
\end{equation}

To achieve differential privacy with budget $(1-f)\epsilon/T$, Laplace noise with scale $\frac{2T}{(1-f)\epsilon}$ is added to each real worker count (the factor of $2$ is due to the fact that sensitivity of count queries on non-overlapping cells is $2$). The perturbed value $z_k^i$ is determined as:
\begin{equation}
%\hspace{50px}  
z_k^i = x_k^i + \nu_k^i ; \nu^i_k \sim Lap(0, \frac{2T}{(1-f)\epsilon}) 
\label{eq:measurement_model}
\end{equation}

Following the work in \cite{Fan14TKDE}, we approximate the Laplace noise $\nu_k^i$ by the following white Gaussian noise so that the Kalman filter-based algorithms can be used for estimation: 
\begin{align}
%\hspace{50px} 
\nu^i_k \sim \mathbb{N}(0, R) \;
\end{align}
In our experiments, we choose the values for $R$ according to the posterior analysis provided in~\cite{Fan14TKDE}.


\noindent
\textbf{Sampling.} Another method to improve accuracy is by reducing sensitivity of the computation. 

If one chooses to suppress the release of a cell count at a certain timestamp, and instead publish a count that is derived from the values of previous timestamps, then sensitivity decreases. Hence, sampling decides whether the count of a specific timestamp is queried directly, or derived from previous values and the movement model considered. 

In our study, we adopt the adaptive sampling algorithm with \textit{proportional}, \textit{integral}, and \textit{derivative} errors (PID) to adjust the sampling interval dynamically for each cell count series. Following \cite{Fan14TKDE}, we define the feedback error $E_k^i$ for cell $i$ at time $k$ between \textit{prediction} and \textit{correction} values, which measures how well the internal process model describes the current data dynamicity.
  Once the $PID$ error is updated,  a new sampling interval $I'$ can be determined by the following equation:
\begin{equation}
%\hspace{50px}  
I' = max\{1, I + \theta(1- e^{\frac{\Delta - \xi}{\xi}})\}
\end{equation} 
where $I$ is the current sampling interval, and $\theta$ and $\xi$ are user-specified parameters. 
  Intuitively, a small PID error results in a decrease in the sampling rate, and vice versa. 

\subsection{Performance Evaluation}
\label{sec:pe}

\subsubsection{Experimental Methodology}
\label{sec:experimental_setup}
We use two real-world datasets: Gowalla and Yelp. Gowalla contains the check-in history of users in a location-based social network. For our experiments, we use the check-in data in the area of San Francisco, California. We assume that Gowalla users are the workers of the SC system, and their locations are those of the most recent check-in points. We also model each check-in point as a task that was previously accepted for execution by a worker. Based on this model, we determine the mean contribution distances ($\mathit{MCDs}$) according to Eq.~\eqref{eq:mcd}, and we compute maximum travel distance ($\mathit{MTD}$) as the $90\%$ $MCD$ percentile value, leading to a value of $3.6km$. 
%Figure \ref{fig:mcd} illustrates the commutative ratio of contributors when varying the $\mathit{MCDs}$. It shows that $MTD$s (i.e., $90\%$ $MCD$ percentile values) of Gowalla, Yelp and Foursquare datasets are $3.6km$, $13.5km$ and $16.5km$, respectively. 
The Yelp data corresponds to the greater area of Phoenix, Arizona, and includes locations of $15,583$ restaurants, $70,817$ users and $335,022$ user reviews. We use restaurant locations as tasks, and a user review is equivalent to accepting a SC task.
%Finally, Foursquare contains the check-in history of $45,138$ users to $89,968$ venues in the area of Pittsburgh, Pennsylvania. We use venue locations as tasks, and a user check-in is equivalent to accepting a SC task.
Table~\ref{tab:real_datasets} presents the details of the datasets.

%We use the same methodology for all datasets to generate data for every instance. 
For moving users experiments, we divide each dataset into $T$ time instances based on the time of activities (i.e., review time for Yelp and check-in time for Gowalla). 
%In our previous work \cite{to2014framework}, user locations are mid points of MBR of locations of their \emph{all} visited point of interests (POI). However; to generate multiple time snapshots, some workers' locations are updated 
Using this methodology, 10-30\% of the worker locations are updated at each timestamp.% If a user has no activity at a time instance, his location remains the same as in the previous instance.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{ l | c |  c | c | c | r}
\hline
\textbf{Name} & \textbf{Tasks} & \textbf{Workers} & \textbf{MTD} & \textbf{Workers/$km^2$} \tn
\hline
Gowalla (Go.) & 151,075 & 6,160 & 3.6 & 35   \tn
\hline
Yelp (Ye.) & 15,583 & 70,817 & 13.5 & 4  \tn
%\hline
%Foursquare (Fo.) & 89,968 & 45,138 & 16.6 & 90  \tn
\hline
\end{tabular}
\caption{Dataset characteristics}
\label{tab:real_datasets}
\end{center}
\end{table}

\begin{comment}
\begin{figure*}[ht]
	\begin{minipage}[b]{0.33\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/gowalla_sf_workers}
		\subcaption{Gowalla, San Francisco}
		\label{fig:gowalla_sf}	
	\end{minipage}
	\begin{minipage}[b]{0.33\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/yelp_workers}
		\subcaption{Yelp, Greater Phoenix}
		\label{fig:yelp}	
	\end{minipage}
	\begin{minipage}[b]{0.33\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/foursquare_workers}
		\subcaption{Foursquare, Pittsburgh}
		\label{fig:foursquare}	
	\end{minipage}
\caption{Worker distributions in real-world datasets (one time snapshot)}
\label{fig:data_distribution}
\end{figure*}
\end{comment}

%\begin{figure}[!htb]\centering
%  \includegraphics[width=0.5\textwidth]{exps/mcd}
%  \caption{The empirical cumulative distribution of mean contribution distances (MCDs) for the datasets}
%  \label{fig:mcd}
%\end{figure}

To evaluate the overhead of privacy, we compare our proposed solution with a non-private algorithm that has access to exact worker locations. Given a task and the {\em actual} worker locations, the algorithm keeps adding nearby workers one by one (1$\mathit{NN}$, 2$\mathit{NN}$, etc.) until the obtained utility exceeds threshold $\mathit{EU}$, or until the size of the $GR$ is larger than $\mathit{MTD}$. The geocast query is the minimum bounding circle of the nearest workers.

We consider privacy budget $\epsilon \in$ \{0.1, .., {\bf 0.5}, ..,1\}, ranging from strict to loose privacy requirements. We set the expected utility $\mathit{EU} \in$ \{0.3, 0.5, 0.7, {\bf 0.9}\} and the maximum acceptance rate $\mathit{MAR}\in$ \{{\bf 0.1}, 0.3, 0.5, 0.7\}. We vary the number of time instances $T\in$ \{50, 60, ..., {\bf 100}\} and the budget percentile for grid structure $f\in$ \{{\bf 0.1}, 0.2, .., 1\}. We vary the number of workers required to perform the task $K\in$ \{{\bf 1}, 2, 3, 4, 5\}. Default values are shown in bold. For Zipf acceptance rate, skew parameter $s$ is set to $1$. Wireless communication range is $50$ meters.

We randomly generated $1,000$ tasks and measured the performance of the proposed solution with respect to the metrics in Section~\ref{sec:metrics1}: $\mathit{ASR}$, $\mathit{ANW}$, and $\mathit{WTD}$. 
To compute $\mathit{ASR}$, we simulate a binomial model as discussed in Section~\ref{sec:acceptance_rate}. Each worker flips a biased coin and decides whether to accept or not a received task request, based on personalized threshold $p^a$ (recall that $p^a$ takes into account distance to task).
A task is accepted if at least one worker agrees to perform it.
We consider two $\mathit{ASR}$ variations: $\mathit{WTD_{NN}}$ and $\mathit{WTD_{FC}}$. In the former case, the metric value is determined as the distance from the task to the nearest worker that accepts the task, whereas in the latter, the distance to the first worker that accepts the task is considered (i.e., first-come).
We also measure the average hop count $\mathit{HOP}$ required for geocast, according to Eq.~\eqref{eq:hops_count}.
Finally, we also show the results obtained for the average number of cells in a $\mathit{GR}$ ($\mathit{CELL}$) and the compactness of the $\mathit{GR}$. Although these metrics are not directly perceived by the end users, they help us to better understand the underpinnings of the proposed solution.
All measured results are averaged over ten random seeds.


\subsubsection{Experimental Results}

\paragraph{Evaluation of Single-snapshot PSD}

\textbf{Evaluation of $\mathit{GR}$ Construction Heuristics:}
We evaluate the performance of the greedy algorithm for $\mathit{GR}$ construction from Section~\ref{sec:greedy} and its variations. $\mathit{GDY}$ refers to the algorithm using the original AG PSD from \cite{qardaji2012differentially}, whereas $G$-$\mathit{GR}$ uses our customized AG solution. The optimization allowing partial cell selection is denoted by $G$-$\mathit{PA}$, and the combination of both $G$-$\mathit{GR}$ and $G$-$\mathit{PA}$ by $G$-$\mathit{GP}$. Figure~\ref{fig:improved_geocast} illustrates the results.

$G$-$\mathit{GP}$ generally performs best in terms of minimizing $\mathit{ANW}$, $\mathit{WTD}$ and $\mathit{HOP}$ in all combination of datasets (Gowalla, Yelp) and acceptance rate functions (Linear, Zipf). Moreover, by comparing $G$-$\mathit{GP}$ and $G$-$\mathit{PA}$ with $\mathit{GDY}$ and $G$-$\mathit{GR}$, we observe that customized AG granularity contributes mostly to the improvements. Partial cell selection proves useful mostly when the privacy budget is small (i.e., resulting grid is coarse).
Compared to $\mathit{GDY}$, $G$-$\mathit{GP}$ reduces $\mathit{ANW}$ by up to a factor of $5$,
% (the first column of Figure \ref{fig:improved_geocast}). This $\mathit{ANW}$ 
and the improvement is more significant when privacy budget is low.
Specifically, increasing $\epsilon$ provides a more accurate estimation for the worker counts in the PSD, and also the granularity of the level-$2$ AG grows. %higher (i.e., the larger the budget, the higher the granularity of the grid). 
As a result, $\mathit{ANW}$ can be more tightly controlled. Moreover, $G$-$\mathit{GP}$ also yields reduced $\mathit{WTD}$ and $\mathit{HOP}$ by up to a factor of 8 and 7, respectively.

On the other hand, $G$-$\mathit{PA}$ obtains lower $\mathit{ASR}$ than the expected utility of $90\%$, particularly for small $\epsilon$. This can be explained based on the fact that applying partial cell selection tends to reduce aggressively the number of workers included in the $\mathit{GR}$, which may result in under-provisioning (i.e., an insufficient number of workers receive task requests). 
All other methods achieve close to the target $\mathit{EU}$ of $90\%$, but most often this is a result of over-provisioning, which in turn increases $\mathit{ANW}$.

\begin{figure*}[!ht]
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_1_anw}
		\subcaption{$\mathit{ANW}$, Go.-Linear}
		\label{fig:sf_li_1_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_1_atd1}
		\subcaption{$\mathit{WTD_{NN}}$,Go.-Linear}
		\label{fig:sf_li_1_atd1}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_1_atd2}
		\subcaption{$\mathit{WTD_{FC}}$, Go.-Linear}
		\label{fig:sf_li_1_atd2}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_1_hop}
		\subcaption{$\mathit{HOP}$, Go.-Linear}
		\label{fig:sf_li_1_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/sf_li_1_appt}
		\subcaption{$\mathit{ASR}$, Go.-Linear}
		\label{fig:sf_li_1_appt}
	\end{minipage}

	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_1_anw}
		\subcaption{$\mathit{ANW}$, Go.-Zipf}
		\label{fig:sf_zi_1_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_1_atd1}
		\subcaption{$\mathit{WTD_{NN}}$, Go.-Zipf}
		\label{fig:sf_zi_1_atd1}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_1_atd2}
		\subcaption{$\mathit{WTD_{FC}}$, Go.-Zipf}
		\label{fig:sf_zi_1_atd2}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_1_hop}
		\subcaption{$\mathit{HOP}$, Go.-Zipf}
		\label{fig:sf_zi_1_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_1_appt}
		\subcaption{$\mathit{ASR}$, Gow.-Zipf}
		\label{fig:sf_zi_1_appt}
	\end{minipage}

	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_1_anw}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_1_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_1_atd1}
		\subcaption{$\mathit{WTD_{NN}}$, Ye.-Linear}
		\label{fig:ye_li_1_atd1}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_1_atd2}
		\subcaption{$\mathit{WTD_{FC}}$, Ye.-Linear}
		\label{fig:ye_li_1_atd2}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_1_hop}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_1_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_1_appt}
		\subcaption{$\mathit{ASR}$, Ye.-Linear}
		\label{fig:ye_li_1_appt}
	\end{minipage}

	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_1_anw}
		\subcaption{$\mathit{ANW}$, Ye.-Zipf}
		\label{fig:ye_zi_1_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_1_atd1}
		\subcaption{$\mathit{WTD_{NN}}$, Ye.-Zipf}
		\label{fig:ye_zi_1_atd1}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_1_atd2}
		\subcaption{$\mathit{WTD_{FC}}$, Ye.-Zipf}
		\label{fig:ye_zi_1_atd2}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_1_hop}
		\subcaption{$\mathit{HOP}$, Ye.-Zipf}
		\label{fig:ye_zi_1_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_1_appt}
		\subcaption{$\mathit{ASR}$, Ye.-Zipf}
		\label{fig:ye_zi_1_appt}
	\end{minipage}
	\caption{Comparison of $\mathit{GR}$ construction heuristics by varying $\varepsilon$.}
\label{fig:improved_geocast}
\end{figure*}

Figure~\ref{fig:c2} captures in more detail the effect of $G$-$GP$ and grid granularity on
$\mathit{ASR}$, as well as the under/over-provisioning tendencies. With coarser-grained grids (i.e., large $k_2$) over-provision occurs, whereas finer-grained grids suffer from excessive noise-to-real-count ratio, resulting in under-provision. Note that our choice of $k_2=\sqrt{2}\sim 1.41$ obtains a good trade-off: it achieves near $90\%$ utility and also reduces $\mathit{ANW}$, $\mathit{WTD}$ and $\mathit{HOP}$.

\begin{figure}[!htb]\centering
	\includegraphics[width=0.7\columnwidth]{exps/c2}
	\caption{Average $\mathit{ASR}$ over $\epsilon \in \{.1,.4,.7,1\}$, varying $k_2$.}
	\label{fig:c2}
\end{figure}

\textbf{Evaluation of Compactness-Based Heuristics:}
We evaluate the effect of the compactness-guided heuristic for $\mathit{GR}$ construction. For brevity, we only include Gowalla results (Yelp dataset shows similar trends). As shown in Figure~\ref{fig:compactness_heuristic}, the compactness-based approach ($G$-$\mathit{GP}$-$\mathit{Compact}$ significantly increases the compactness measure compared to its utility-based counterpart ($G$-$\mathit{GP}$-$\mathit{Pure}$). The hop count is also reduced, by up to $36\%$, particularly when the privacy budget is large. However, the compactness-only approach does not fare that well for lower privacy budgets. On the other hand, the hybrid heuristic that combines utility and compactness in the ranking of candidates ($G$-$\mathit{GP}$-$\mathit{Hybrid}$) manages to perform better than its counterparts for all $\epsilon$ values. We conclude that such a balanced approach is the best solution for $\mathit{GR}$ construction.

\begin{figure*}[!ht]
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/sf_li_2_cmp}
		\subcaption{$\mathit{CMP}$, Go.-Linear}
		\label{fig:sf_li_2_cell}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_2_hop}
		\subcaption{$\mathit{HOP}$, Go.-Linear}
		\label{fig:sf_li_2_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_2_anw}
		\subcaption{$\mathit{ANW}$, Go.-Linear}
		\label{fig:sf_li_2_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_2_atd1}
		\subcaption{$\mathit{WTD_{NN}}$,Go.-Linear}
		\label{fig:sf_li_2_atd1}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_2_atd2}
		\subcaption{$\mathit{WTD_{FC}}$, Go.-Linear}
		\label{fig:sf_li_2_atd2}
	\end{minipage}


	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_2_cmp}
		\subcaption{$\mathit{CMP}$, Go.-Zipf}
		\label{fig:sf_zi_2_cmp}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_2_hop}
		\subcaption{$\mathit{HOP}$, Go.-Zipf}
		\label{fig:sf_zi_2_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_2_anw}
		\subcaption{$\mathit{ANW}$, Go.-Zipf}
		\label{fig:sf_zi_2_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_2_atd1}
		\subcaption{$\mathit{WTD_{NN}}$, Go.-Zipf}
		\label{fig:sf_zi_2_atd1}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_2_atd2}
		\subcaption{$\mathit{WTD_{FC}}$, Go.-Zipf}
		\label{fig:sf_zi_2_atd2}
	\end{minipage}
\begin{comment}	
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_2_cmp}
		\subcaption{$\mathit{CMP}$, Ye.-Linear}
		\label{fig:ye_li_2_cmp}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_2_hop}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_2_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_2_anw}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_2_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_2_atd1}
		\subcaption{$\mathit{WTD1}$, Ye.-Linear}
		\label{fig:ye_li_2_atd1}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_2_atd2}
		\subcaption{$\mathit{WTD2}$, Ye.-Linear}
		\label{fig:ye_li_2_atd2}
	\end{minipage}

	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_2_cmp}
		\subcaption{$\mathit{CMP}$, Ye.-Zipf}
		\label{fig:ye_zi_2_cmp}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_2_hop}
		\subcaption{$\mathit{HOP}$, Ye.-Zipf}
		\label{fig:ye_zi_2_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_2_anw}
		\subcaption{$\mathit{ANW}$, Ye.-Zipf}
		\label{fig:ye_zi_2_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_2_atd1}
		\subcaption{$\mathit{WTD1}$, Ye.-Zipf}
		\label{fig:ye_zi_2_atd1}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_2_atd2}
		\subcaption{$\mathit{WTD2}$, Ye.-Zipf}
		\label{fig:ye_zi_2_atd2}
	\end{minipage}
\end{comment}	
	\caption{Comparison of compactness-based heuristics by varying $\varepsilon$.}
\label{fig:compactness_heuristic}
\end{figure*}

\textbf{Overhead of Achieving Privacy:}
We compare the proposed solution with the non-private algorithm for task assignment, described in Section \ref{sec:experimental_setup}. Figure~\ref{fig:privacy_cost} presents the overhead incurred by privacy when varying $\epsilon$ (for brevity, we only show Gowalla results). As expected, when $\epsilon$ increases, the PSD offers more accurate data, and the overhead (in terms of $\mathit{ANW}$, $\mathit{WTD}$ and $\mathit{HOP}$) decreases.
%, represented by the gap between the two lines in the sub-figures, 
Interestingly though, $\mathit{ASR}$ drops in value. This can be explained through significant over-provisioning that occurs for lower budgets, when the greedy heuristic enlarges the $\mathit{GR}$ in the quest for achieving the desired $\mathit{EU}$. As a result, more workers are notified, and the chances of task acceptance are higher. However, overhead is also much higher.
%The reason is that the higher budget, the less privacy, the smaller gap between privacy and non-privacy techniques. We observed that the privacy cost in $\mathit{ANW}$ is the largest while the cost of $\mathit{WTD}$ is the smallest. 

We also observe that privacy does not significantly increase $\mathit{WTD}$, proving that the greedy $\mathit{GR}$ algorithm does a good job in selecting nearby workers. 
Table~\ref{tab:privacy_cost} summarizes the variation of considered metrics when adding privacy. Note that, the travel distance, which is perhaps the most important factor in SC, is not considerably impacted by privacy. We also observed that the overhead incurred is generally higher for the sparser Yelp data, which is not surprising, 
as it is a well-known fact that differentially private algorithms perform better on dense datasets.
%more more sparse than Gowalla data. It confirmed the common intuition that differentially private algorithms perform better in dense datasets. Also, this observation and the fact that spatial crowdsourcing is more practical in metropolitan areas confirmed the choise of our datasets.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{ l | c | c | c | r}
\hline
\textbf{} & \textbf{$\mathit{ANW}$} & \textbf{$\mathit{HOP}$} & \textbf{$\mathit{WTD_{NN}}$} & \textbf{$\mathit{WTD_{FC}}$} \tn
\hline
Go.-Linear & 161\% & 54\% & 25\% & 18\% \tn
\hline
Go.-Zipf & 103\% & 30\% & 22\% & 23\% \tn
\hline
Ye.-Linear & 202\% & 92\% & 19\%& 20\% \tn
\hline
Ye.-Zipf & 132\% & 41\% & 17\%& 25\% \tn
\hline
\end{tabular}
\caption{The average relative increase in percentage of different measurements when varying $\epsilon \in \{0.1, 0.4,0.7,1\}$}
\label{tab:privacy_cost}
\end{center}
\end{table}

Table~\ref{tab:privacy_cost} also shows the effect of different acceptance rate functions. Zipf incurs lower overhead compared to Linear. The reason is that with Zipf distribution, the acceptance rate of the workers drops faster for the same distance to the task compared with the linear case. The smaller acceptance rate leads to larger $\mathit{ANW}$ in both private and non-private cases; however, $\mathit{ANW}$ increases at a faster rate in the non-private case (Figure~\ref{fig:privacy_cost}).

\begin{figure*}[tbh]
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_3_anw}
		\subcaption{$\mathit{ANW}$, Go.-Linear}
		\label{fig:sf_li_3_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_3_hop}
		\subcaption{$\mathit{HOP}$, Go.-Linear}
		\label{fig:sf_li_3_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_3_atd1}
		\subcaption{$\mathit{WTD_{NN}}$, Go.-Linear}
		\label{fig:sf_li_3_atd1}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_3_atd2}
		\subcaption{$\mathit{WTD_{FC}}$, Go.-Linear}
		\label{fig:sf_li_3_atd2}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/sf_li_3_appt}
		\subcaption{$\mathit{ASR}$, Go.-Linear}
		\label{fig:sf_li_3_appt}
	\end{minipage}

%\begin{comment}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_3_anw}
		\subcaption{$\mathit{ANW}$, Go.-Zipf}
		\label{fig:sf_zi_3_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_3_hop}
		\subcaption{$\mathit{HOP}$, Go.-Zipf}
		\label{fig:sf_zi_3_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_3_atd1}
		\subcaption{$\mathit{WTD_{NN}}$, Go.-Zipf}
		\label{fig:sf_zi_3_cell}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_3_atd2}
		\subcaption{$\mathit{WTD_{FC}}$, Go.-Zipf}
		\label{fig:sf_zi_3_atd2}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/sf_zi_3_appt}
		\subcaption{$\mathit{ASR}$, Go.-Zipf}
		\label{fig:sf_zi_3_appt}
	\end{minipage}
%\end{comment}

\begin{comment}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_3_anw}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_3_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_3_hop}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_3_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_3_atd1}
		\subcaption{$\mathit{WTD1}$, Ye.-Linear}
		\label{fig:ye_li_3_atd1}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_3_atd2}
		\subcaption{$\mathit{WTD2}$, Ye.-Linear}
		\label{fig:ye_li_3_atd2}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_3_appt}
		\subcaption{$\mathit{APPT}$, Ye.-Linear}
		\label{fig:ye_li_3_appt}
	\end{minipage}


	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_3_anw}
		\subcaption{$\mathit{ANW}$, Ye.-Zipf}
		\label{fig:ye_zi_3_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_3_hop}
		\subcaption{$\mathit{HOP}$, Ye.-Zipf}
		\label{fig:ye_zi_3_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_3_atd1}
		\subcaption{$\mathit{WTD1}$, Ye.-Zipf}
		\label{fig:ye_zi_3_cell}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_3_atd2}
		\subcaption{$\mathit{WTD2}$, Ye.-Zipf}
		\label{fig:ye_zi_3_atd2}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_zi_3_appt}
		\subcaption{$\mathit{APPT}$, Ye.-Zipf}
		\label{fig:ye_zi_3_appt}
	\end{minipage}
\end{comment}
	\caption{Overhead of privacy ($G$-$\mathit{GP}$-$\mathit{Hybrid}$) compared to non-private algorithm.}
\label{fig:privacy_cost}
\end{figure*}


\textbf{The Effect of Varying $\mathit{MAR}$}

%\emph{Varying $\mathit{MAR}$}: 
We evaluate the performance of $G$-$\mathit{GP}$-$\mathit{Hybrid}$ on the Yelp dataset by varying the maximum acceptance rate
% and the expected utility $\mathit{EU}$ 
(similar trends were observed for Gowalla). Figure~\ref{fig:varying_mar} shows the results.
% when varying $\mathit{MAR}$. 
A higher acceptance rate yields lower overhead and shorter travel distance (workers are more willing to accept tasks). The $\mathit{GR}$ size is also smaller, leading to a smaller network diameter and $\mathit{HOP}$ value.


\begin{figure*}[tbh]
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_4_anw}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_4_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_4_hop}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_4_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_4_atd1}
		\subcaption{$\mathit{WTD_{NN}}$, Ye.-Linear}
		\label{fig:ye_li_4_atd1}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_4_atd2}
		\subcaption{$\mathit{WTD_{FC}}$, Ye.-Linear}
		\label{fig:ye_li_4_atd2}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_4_cell}
		\subcaption{$\mathit{CELL}$, Ye.-Linear}
		\label{fig:ye_li_4_cell}
	\end{minipage}
	\caption{Performance of geocast algorithm ($G$-$\mathit{GP}$-$\mathit{Hybrid}$) when varying Acceptance Rate (Ye.-Linear).}
\label{fig:varying_mar}
\end{figure*}

\begin{comment}
\begin{figure*}[tbh]
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_5_anw}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_5_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_5_hop}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_5_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\subcaption{$\mathit{WTD_{NN}}$, Ye.-Linear}
		\label{fig:ye_li_5_atd1}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_5_atd2}
		\subcaption{$\mathit{WTD_{FC}}$, Ye.-Linear}
		\label{fig:ye_li_5_atd2}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_5_cell}
		\subcaption{$\mathit{CELL}$, Ye.-Linear}
		\label{fig:ye_li_5_cell}
	\end{minipage}
	\caption{Performance of geocast algorithm ($G$-$\mathit{GP}$-$\mathit{Hybrid}$) when varying $\mathit{EU}$ (Ye.-Linear).}
\label{fig:varying_eu}
\end{figure*}
\end{comment}

Interestingly, Figures~\ref{fig:ye_li_4_atd1} and \ref{fig:ye_li_4_atd2} show that $MAR$ has a significant effect on decreasing $\mathit{WTD}$. This effect is more pronounced than the drop due to increase in privacy budget $\epsilon$, as observed in previous experiments. Figure~\ref{fig:ye_li_4_cell} shows that the number of grid cells in the $\mathit{GR}$ drops as $\mathit{MAR}$ increases, due to increased utility of each cell. For the largest MAR value, a single cell is sufficient as $\mathit{GR}$, so $\mathit{CELL=1}$.


\textbf{Effect of Varying $K$:}
We evaluate the performance of $G$-$\mathit{GP}$-$\mathit{Hybrid}$ on the Yelp dataset by varying the number of workers required to complete a task. Figure~\ref{fig:varying_k} shows the results. As expected, higher $K$ yields higher overhead as more workers are required to perform a task. Also, we observe that as the privacy budget $\epsilon$ gets larger, the impact of $K$ on the performance metrics attenuates.

\begin{figure*}[tbh]
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_6_anw.eps}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_6_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_6_hop}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_6_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_6_atd1}
		\subcaption{$\mathit{WTD_{NN}}$, Ye.-Linear}
		\label{fig:ye_li_6_atd1}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_6_atd2}
		\subcaption{$\mathit{WTD_{FC}}$, Ye.-Linear}
		\label{fig:ye_li_6_atd2}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_6_cell}
		\subcaption{$\mathit{CELL}$, Ye.-Linear}
		\label{fig:ye_li_6_cell}
	\end{minipage}
	\caption{Performance of geocast algorithm ($G$-$\mathit{GP}$-$\mathit{Hybrid}$) when varying number of workers required to complete a task (Ye.-Linear).}
\label{fig:varying_k}
\end{figure*}

\textbf{Comparison with pull-mode Benchmark}

We evaluate the performance of our technique in comparison with a benchmark that resembles the pull mode. With the pull mode, tasks are being broadcast to workers by the CSP, and there is no disclosure of worker locations to the SC server. However, each worker operates with knowledge of its own location only, which may lead to suboptimal assignment. Only workers within $MTD$ distance of a task are notified.

Table~\ref{tab:privacy_wst} summarizes the results. We consider as comparison metrics the average number of notified workers ($\mathit{ANW}$), average number of accepting workers ($AAW$), and travel distance (for the latter, our solution considers two cases, the NN and the FC approach). For brevity, we show the overhead of pull-mode compared to our approach, expressed as a multiplicative factor showing how many times higher (i.e., worse) the overhead of pull mode is.

As expected, due to lack of global location information, the benchmark performs poorly. Specifically, the number of notified workers is higher by a factor ranging from 71 (when our method receives the lowest privacy budget) to 125 (for the largest privacy budget setting of 1.0). Such a high $\mathit{ANW}$ value raises serious concerns about the practicality of pull mode, as a large number of workers must be notified, leading to large consumption of communication bandwidth and battery, both scarce resources for mobile users. The number of accepting workers is larger for the pull mode by a factor ranging from 27 to 47, resulting in an excessive level of task assignment redundancy. The average travel distance is also significantly worse, by a factor ranging from approximately 7 (in the case of FC) to 12 (for our more accurate NN method). These results show that the push-mode approach performs far better than the pull-mode method, even when dealing with sanitized (i.e., perturbed) location information.

\begin{table}[ht]
\begin{center}
\footnotesize
\begin{tabular}{ l | c | c | c | r}
\textbf{$\epsilon$} & \textbf{$\mathit{ANW}$} & \textbf{$\mathit{AAW}$} & \textbf{$\mathit{WTD_{NN}}$} & \textbf{$\mathit{WTD_{FC}}$} \tn
\hline
0.1 & 71.42 & 27.78 & 11.90 & 6.25 \tn
\hline
0.4 & 111.11 & 40 & 12.04 & 7.63 \tn
\hline
0.7 & 125 & 45.45 & 11.49 & 7.87 \tn
\hline
1.0 & 125 & 47.61 & 11.90 & 7.75 \tn
\end{tabular}
\caption{Performance comparison with pull-mode benchmark.}
\label{tab:privacy_wst}
\end{center}
\end{table}


\paragraph{Evaluation of Multiple-snapshot PSD}
\label{sec:results2}

We evaluate the performance of the proposed algorithms for dynamic worker PSD from Section~\ref{sec:psd_dynamic}. In our implementation, we use the $G$-$\mathit{GP}$-$\mathit{Hybrid}$ as a single-snapshot base for the dynamic case, and we consider two multiple-snapshot instances: the first uses the Kalman filtering without sampling (denoted as $\mathit{Kalman}$) and the second uses Kalman filtering in conjunction with the PID adaptive sampling (denoted as $\mathit{KalmanPID}$).
We compare the obtained results against two benchmarks: the non-private case, and the $\mathit{BasicD}$ baseline introduced in Section 6, which divides privacy budget equally among all time instances. For brevity, we present the results only for linear acceptance rate function, as similar results have been observed for the zipf case (the focus of this experiment is on user movement, so the choice of acceptance rate function does not have a significant impact).

Figure~\ref{fig:privacy_cost_d} presents the performance metrics measurements of the considered methods when varying $\epsilon$. For higher $\epsilon$, the PSD offers more accurate data, and the overhead (in terms of $\mathit{ANW}$, $\mathit{WTD}$ and $\mathit{HOP}$) decreases. We observe that both Kalman-based algorithms are superior to $\mathit{BasicD}$ by a significant amount. Furthermore, their performance is not far behind that of the non-private approach. Interestingly, the $\mathit{BasicD}$ baseline obtains a high $\mathit{ASR}$, but this occurs due to excessive over-provisioning at lower budgets. In such cases, the greedy heuristic enlarges the $\mathit{GR}$ to achieve the desired utility, and the overhead is very high.

We also observe that $\mathit{KalmanPID}$ performs better than $\mathit{Kalman}$ in all cases, and particularly when the budget is small (i.e., up to 14\% in $\mathit{ANW}$, $\mathit{WTD}$ and $\mathit{HOP}$). 
The reason for this behavior is the benefit brought by sampling in the case of $\mathit{KalmanPID}$. 
Due to its superior performance, we focus on $\mathit{KalmanPID}$ for the rest of the experiments.


Next, we investigate the performance of the multiple-snapshot algorithm when varying the privacy budget split captured by parameter {\em f}. Recall from Section 6 that a fraction $f$ of the budget is used in the first timestamp to determine the adaptive grid structure. A higher $f$ results in a more accurate initial structure, but in the detriment of accuracy when later determining noisy counts for level-2 cells. Conversely, a small value of $f$ may impact negatively the ability of the structure to capture the initial worker density, but may result in higher accuracy for individual cell counts.

\begin{figure*}[tbh]
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_anw}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_100_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_hop}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_100_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_wtd}
		\subcaption{$\mathit{WTD}$, Ye.-Linear}
		\label{fig:ye_li_100_atd}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_asr}
		\subcaption{$\mathit{ASR}$, Ye.-Linear}
		\label{fig:ye_li_100_asr}
	\end{minipage}
		\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_cell}
		\subcaption{$\mathit{CELL}$, Ye.-Linear}
		\label{fig:ye_li_100_cell}
	\end{minipage}
	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_100_anw}
		\subcaption{$\mathit{ANW}$, Go.-Linear}
		\label{fig:sf_li_100_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_100_hop}
		\subcaption{$\mathit{HOP}$, Go.-Linear}
		\label{fig:sf_li_100_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_100_wtd}
		\subcaption{$\mathit{WTD}$, Go.-Linear}
		\label{fig:sf_li_100_atd}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/sf_li_100_asr}
		\subcaption{$\mathit{ASR}$, Go.-Linear}
		\label{fig:sf_li_100_asr}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/sf_li_100_cell}
		\subcaption{$\mathit{CELL}$, Go.-Linear}
		\label{fig:sf_li_100_cell}
	\end{minipage}
\begin{comment}	
		\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/fo_li_100_anw}
		\subcaption{$\mathit{ANW}$, Fo.-Linear}
		\label{fig:sf_li_100_anw}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/fo_li_100_hop}
		\subcaption{$\mathit{HOP}$, Fo.-Linear}
		\label{fig:sf_li_100_hop}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/fo_li_100_wtd}
		\subcaption{$\mathit{WTD}$, Fo.-Linear}
		\label{fig:sf_li_100_wtd}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/fo_li_100_asr}
		\subcaption{$\mathit{ASR}$, Fo.-Linear}
		\label{fig:sf_li_100_asr}								
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/fo_li_100_cell}
		\subcaption{$\mathit{CELL}$, Fo.-Linear}
		\label{fig:fo_li_100_cell}								
	\end{minipage}
\end{comment}	
	\caption{Performance overhead for multiple-snapshot worker PSD when varying privacy budget $\varepsilon$.}
\label{fig:privacy_cost_d}
\end{figure*}

\begin{figure*}[tbh]
\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_100_anw_vf}
		\subcaption{$\mathit{ANW}$, Go.-Linear}
		\label{fig:sf_li_100_anw_vf}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_100_hop_vf}
		\subcaption{$\mathit{HOP}$, Go.-Linear}
		\label{fig:sf_li_100_hop_vf}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/sf_li_100_wtd_vf}
		\subcaption{$\mathit{WTD}$, Go.-Linear}
		\label{fig:sf_li_100_atd_vf}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/sf_li_100_asr_vf}
		\subcaption{$\mathit{ASR}$, Go.-Linear}
		\label{fig:sf_li_100_asr_vf}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/sf_li_100_cell_vf}
		\subcaption{$\mathit{CELL}$, Go.-Linear}
		\label{fig:sf_li_100_cell_vf}
	\end{minipage}	
	
	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_anw_vf}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_100_anw_vf}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_hop_vf}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_100_hop_vf}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_wtd_vf}
		\subcaption{$\mathit{WTD}$, Ye.-Linear}
		\label{fig:ye_li_100_atd_vf}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_asr_vf}
		\subcaption{$\mathit{ASR}$, Ye.-Linear}
		\label{fig:ye_li_100_asr_vf}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_cell_vf}
		\subcaption{$\mathit{CELL}$, Ye.-Linear}
		\label{fig:ye_li_100_cell_vf}
	\end{minipage}
	\caption{Varying budget split $f$.}
	% Note that ANW, HOP and WTD in non-private case are smaller than private case due to significant under-provision with BasicD and KalmanPID.}
\label{fig:varying_f}
\end{figure*}


\begin{comment}
As the workers can be either online or offline at a particular time, the number of online workers per time instance may change. In Figures \ref{fig:privacy_cost_a} and \ref{fig:privacy_cost_a2}, we evaluate our approaches under two scenarios, i.e., when the number of \emph{online} workers linearly increases and decreases \footnote{the minimum/maximum sets of online workers are selected by randomly sampling 10/100\% of the population}, respectively. As expected, the costs $\mathit{ANW}$, $\mathit{HOP}$ and $\mathit{WTD}$ increases in comparison to the fixed population case in Figure \ref{fig:privacy_cost_d}. Figure \ref{fig:privacy_cost_a} shows that Kalman-based approaches have significantly smaller costs in comparison to $\mathit{BasicD}$ and $\mathit{BasicS}$ (e.g., $\mathit{KalmanPID}$ reduces $\mathit{ANW}$, $\mathit{HOP}$ and $\mathit{WTD}$ of $\mathit{BasicS}$ by 68\%, 63\% and 16\%, respectively). Furthermore, Figure \ref{fig:ye_li_100_asr_a} shows excessive over-provisioning with $\mathit{BasicS}$ ($\mathit{ASR}=98\%$). This is because geocast algorithm is performed on an \emph{sparse} outdated worker PSD after the first time instance. Similar results are shown in Figure \ref{fig:privacy_cost_a2}, excepts $\mathit{BasicS}$ suffers from under-provisioning due to \emph{dense} outdated worker PSD.

\begin{figure*}[tbh]
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_anw_a}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_100_anw_a}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_hop_a}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_100_hop_a}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_wtd_a}
		\subcaption{$\mathit{WTD}$, Ye.-Linear}
		\label{fig:ye_li_100_atd_a}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_asr_a}
		\subcaption{$\mathit{ASR}$, Ye.-Linear}
		\label{fig:ye_li_100_asr_a}
	\end{minipage}
		\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_cell_a}
		\subcaption{$\mathit{CELL}$, Ye.-Linear}
		\label{fig:ye_li_100_cell_a}
	\end{minipage}
	\caption{Overhead of privacy compared to non-private algorithm when the number of workers linearly increases. (f=0.5)}
\label{fig:privacy_cost_a}
\end{figure*}

\begin{figure*}[tbh]
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_anw_a2}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_100_anw_a2}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_hop_a2}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_100_hop_a2}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_wtd_a2}
		\subcaption{$\mathit{WTD}$, Ye.-Linear}
		\label{fig:ye_li_100_atd_a2}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_asr_a2}
		\subcaption{$\mathit{ASR}$, Ye.-Linear}
		\label{fig:ye_li_100_asr_a2}
	\end{minipage}
		\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_100_cell_a2}
		\subcaption{$\mathit{CELL}$, Ye.-Linear}
		\label{fig:ye_li_100_cell_a2}
	\end{minipage}
	\caption{Overhead of privacy compared to non-private algorithm when the number of workers linearly decreases. BasicD and Kalman are not shown for clarity. (f=0.1)}
\label{fig:privacy_cost_a2}
\end{figure*}

\end{comment}


\begin{figure}[tbh]
	\begin{minipage}[b]{0.49\linewidth}
	\centering
		\includegraphics[width=\textwidth]{figures/gowallasf_005}
		\subcaption{KalmanPID, $f = 10\%$}
		\label{fig:gowallasf_005}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}
	\centering
		\includegraphics[width=\textwidth]{figures/gowallasf_045}
		\subcaption{KalmanPID, $f = 90\%$}
		\label{fig:gowallasf_045}
	\end{minipage}
	\caption{AG Structure visualization for Gowalla dataset.}
\label{fig:gowallasf_gs}
\end{figure}

\begin{comment}
\begin{figure}[tbh]
	\begin{minipage}[b]{0.49\linewidth}
	\centering
		\includegraphics[width=\textwidth]{figures/yelp_005}
		\subcaption{KalmanPID, $f = 10\%$}
		\label{fig:yelp_005}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}
	\centering
		\includegraphics[width=\textwidth]{figures/yelp_0005}
		\subcaption{BasicD ($f = 50\%$)}
		\label{fig:yelp_0005}
	\end{minipage}
	\caption{AG Structure visualization for Yelp dataset.}
\label{fig:yelp_gs}
\end{figure}
\end{comment}

%\subsubsection{The Effect of Varying $f$ and $T$}

\begin{figure*}[tbh]
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_anw_vt}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_anw_vt}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_hop_vt}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_hop_vt}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_wtd_vt}
		\subcaption{$\mathit{WTD}$, Ye.-Linear}
		\label{fig:ye_li_wtd_vt}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_asr_vt}
		\subcaption{$\mathit{ASR}$, Ye.-Linear}
		\label{fig:ye_li_asr_vt}
	\end{minipage}
		\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_cell_vt}
		\subcaption{$\mathit{CELL}$, Ye.-Linear}
		\label{fig:ye_li_cell_vt}
	\end{minipage}
	\caption{Varying number of timestamps $T$.}
\label{fig:varying_t}
\end{figure*}

Figure~\ref{fig:varying_f} shows a decreasing trend for $\mathit{ANW}$, $\mathit{HOP}$ and $\mathit{WTD}$ as $f$ increases. As expected, a higher $f$ yields lower overhead ($\mathit{ANW}$) and shorter travel distance ($\mathit{WTD}$) in $\mathit{KalmanPID}$ due to finer-grained grids. The $\mathit{GR}$ size is also smaller, thus leading to a smaller network diameter and $\mathit{HOP}$ value. However, Figures \ref{fig:sf_li_100_asr_vf} and \ref{fig:ye_li_100_asr_vf} show a significant drop in utility with respect to $\mathit{EU}$, i.e, 54\% and 36\%, respectively. To achieve the expected utility $\mathit{EU}=0.9$, $f$ must be set to lower values, such as 0.1. 


To better understand the outcome of the variable $f$ results, we perform another experiment that helps visualize the effect of $f$ on the grid structure.
Figure \ref{fig:gowallasf_gs} shows the AG structures obtained for $\mathit{KalmanPID}$ with different $f$ values. The larger value of $f = 90\%$ produces finer-grained grids in Figure \ref{fig:gowallasf_045}, as $\epsilon_2=f \times \epsilon$ is higher. Finer-grained grids result in small actual counts, but also in large relative errors (i.e., excessive noise-to-real-count ratio). Thus, the geocast algorithm tends to pick only a few cells, causing under-provisioning, and thus low $\mathit{ASR}$. 

\begin{comment}
\begin{figure*}[tbh]
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_anw_vk}
		\subcaption{$\mathit{ANW}$, Ye.-Linear}
		\label{fig:ye_li_anw_vk}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_hop_vk}
		\subcaption{$\mathit{HOP}$, Ye.-Linear}
		\label{fig:ye_li_hop_vk}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/ye_li_wtd_vk}
		\subcaption{$\mathit{WTD}$, Ye.-Linear}
		\label{fig:ye_li_wtd_vk}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_asr_vk}
		\subcaption{$\mathit{ASR}$, Ye.-Linear}
		\label{fig:ye_li_asr_vk}
	\end{minipage}
		\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/ye_li_cell_vk}
		\subcaption{$\mathit{CELL}$, Ye.-Linear}
		\label{fig:ye_li_cell_vk}
	\end{minipage}
	\caption{Varying number of workers required $K$ to complete a task.}
\label{fig:varying_k2}
\end{figure*}
\end{comment}

%\subsubsection{The Effect of Varying $T$}
%\label{sec:vary_t}


Finally, we focus our attention to the influence of the movement history length $T$ on the studied performance metrics.
Figure \ref{fig:varying_t} shows the results on the Yelp dataset (similar trends were observed for the Gowalla dataset). When increasing $T$, the considered performance metrics ($\mathit{ANW}$, $\mathit{HOP}$ and $\mathit{WTD}$) for $\mathit{KalmanPID}$ are stable (there is only a small increase, up to 10\%) while the performance of $\mathit{BasicD}$ decreases significantly when $T$ increases. 
In addition, $\mathit{KalmanPID}$ achieves high ASR, very close to the desired utility threshold $\mathit{EU}$. On the other hand, to obtain a high $\mathit{ASR}$, $\mathit{BasicD}$ needs to over-provision excessively, which explains its high overhead.


\begin{comment}
\textbf{Redundant task assignment (Single-snapshot PSD).}

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{ l | c | c | c | c | r}
\textbf{$K$} & \textbf{$\mathit{ANW}$} & \textbf{$\mathit{HOP}$} & \textbf{$\mathit{WTD_{NN}}$} & \textbf{$\mathit{WTD_{FC}}$} & \textbf{$\mathit{CELL}$} \tn
\hline
2 & 55.1 & 16.2 & 0.24 & 0.38 & 4.6 \tn
3 & 88.1 & 23.7 & 0.28 & 0.52 & 7.0 \tn
4 & 117.4 & 28.1 & 0.32 & 0.56 & 7.8 \tn
5 & 145.2 & 30.0 & 0.37 & 0.61 & 8.5 \tn
\end{tabular}
\caption{Performance of $G$-$\mathit{GP}$-$\mathit{Hybrid}$ when varying number of workers required to complete a task  $K$ (Ye.-Linear, $\epsilon=0.4$).}
\label{tab:single_PSD_K}
\end{center}
\end{table}

\textbf{Redundant task assignment (Multiple-snapshot PSD).}

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{ l | c | c | c | c | r}
\textbf{$K$} & \textbf{$\mathit{ANW}$} & \textbf{$\mathit{HOP}$} & \textbf{$\mathit{WTD}$} & \textbf{$\mathit{ASR}$} & \textbf{$\mathit{CELL}$} \tn
\hline
2 & 63.1 & 19.8 & 0.46 & 84.5 & 0.38 \tn
3 & 94.7 & 25.3 & 0.47 & 81.4& 0.52 \tn
4 & 137.4 & 31.4 & 0.46 & 80.7 & 0.56 \tn
5 & 195.4 & 37.4 & 0.46 & 80.10& 0.61  \tn
\end{tabular}
\caption{Performance of $Kalman$-$PID$ when varying number of workers required to complete a task $K$ (Ye.-Linear, $\epsilon=0.5$).}
\label{tab:mulltiple_PSD_K}
\end{center}
\end{table}

\end{comment}

\subsection{P\lowercase{riv}G\lowercase{eo}C\lowercase{rowd}: A Tool for Tuning Parameters}
\label{sec:software}

\begin{figure*}[!htb]\centering
  \includegraphics[width=0.9\textwidth]{figures/summary.jpg}
  \caption{\PGC\ main GUI integrates several component module panels}
  \label{fig:main}
\end{figure*}

In Section~\ref{sec:architecture} we outline the software architecture of the tool, followed by a presentation of the main GUI elements in Section~\ref{sec:gui} and the demonstration scenario in Section~\ref{sec:scenario}.

\subsubsection{System Architecture}
\label{sec:architecture}

Figure~\ref{fig:architecture} presents the architecture of \PGC, which is currently integrated with MediaQ \cite{kim2014mediaq}, a geospatial crowdsourcing system developed at USC. The GUI is provided to end-users (i.e., CSPs, SC-server administrators or task requesters) entirely in web-based form, which has the benefit of not requiring any additional software to be installed at the client. The web application is written using Javascript, Python, PHP5, and Google Maps API for map rendering. The GeoCrowd API provides SC services \cite{kim2014mediaq} such as profile management, etc. The worker location datasets are stored in a MySQL database, which can be queried using the CSP API to generate sanitized PSDs.

%The CSP API includes services for SCP to publish private location data and geocast a given {\em GR}, 
%Particularly, GeoCrowd API has a service to update {\em GR} construction parameters. 
% The PSD is an in-memmory adaptive grid using Tornado, a Python web server. The PSD API is an intemediate interface to 
%separate the web service layer and the data store. The interface 
%provide request handlers for the SCP API and the GeoCrowd API to release sanitized data and to update geocast algorithm parameters, respectively.

%{\tt Hien, give details of technologies /software used for web-side, Python, DB, Map Libraries, etc} suite. 

%{\tt Also, add one sentence with the specific role of each component}.

%With PriGeoCrowd, requesters can generate task requests using an integrated Web App. By querying the in-memory PSD, SC-server finds the $\mathit{GR}$ for a particular task request. Thereafter, it sends the $\mathit{GR}$ to a lightweight geocast service on the CSP. SCP queries the actual dataset to find ids of the workers in the GR. We use Google Cloud Messaging as a way to realize geocast protocol, in which a push notifi?cation service is sent to all enclosed workers. In addition, a background service on the Android app notify workers in real-time when tasks are geocast to him/her by the CSP. 

\begin{figure}[!htb]\centering
  \includegraphics[width=1.0\columnwidth]{figures/architecture.jpg}
  \caption{Architecture of \PGC}
  \label{fig:architecture}
\end{figure}

\subsubsection{Graphical User Interface}
\label{sec:gui}

Figure~\ref{fig:main} gives an overview of the main GUI, which comprises of several component modules:

\textbf{CSP Panel}: it allows the CSP to sanitize and publish datasets according to privacy budget $\varepsilon \in [0.1,1]$. The system ships with three datasets: Gowalla-San Francisco (SF), Gowalla-Los Angeles (LA) and Yelp-Phoenix (Yelp), but the CSP has the option of uploading additional datasets to the repository. The CSP can also specify the budget allocation split between {\em AG} levels ($0.5$ means equal split between levels $1$ and $2$). The CSP can customize the granularity of the level-$1$ {\em AG} grid used for constructing the worker PSD. Once the ``Publish Data'' button is clicked, the corresponding PSD of the dataset is generated and made available to other modules through the PSD API.

\textbf{SC Panel - Dataset Selection}: The SC-server administrator can select among available sanitized datasets from the drop-down list. Several statistics of the selected dataset are automatically provided on the right-hand side, and the user has the option to visualize the dataset density heatmap, as well as the dataset boundary.

\textbf{SC Panel - GR Construction Tuning}: The SC-server administrator is able to select one of the supported heuristics for $GR$ construction (i.e., distance-based, compactness-based), as well as the parameters of the $\mathit{GR}$ construction algorithm.
%, discussed in Section \ref{sec:geocast}.
%She can decide whether or not to use sub-cell optimization with the selected heuristic. 
%She can select particular values for the input parameters of the algorithm.
The user can choose a threshold for acceptance success rate (ASR) in the range [0.6..0.9]. The maximum task acceptance rate $\mathit{AR}$ threshold (i.e., when worker and task are co-located) can be varied beenween 0.1 and 1.0. The wireless communication range for the geocast is customizable between $25$ and $100$ meters. 
%When one clicks on ''Update'' button, the chosen parameters affect all selected datasets.

\textbf{Task Requester Panel - Geocast Region Rendering}: There are three ways one can submit a task request: (1) by double clicking on the map, (2) by providing latitude/longitude values in the task text box, or (3) by selecting a particular task in the history tab. The latter task list is extracted from MediaQ \cite{kim2014mediaq}. When one specifies a task, its $\mathit{GR}$ is computed and rendered in real-time. In Figure~\ref{fig:main}, the pop-up dialog of the the last visited cell presents some statistics. Typically, the current utility (i.e., accumulated utility) measures the probability that at least one worker accepts the task if it is geocast. This number represents the task satisfaction of the task requester. The distance-to-task value of a cell can be considered as satisfaction of the workers within this cell. In addition, both the compactness and the area of the {\em GR} indicate the cost of the SC-server. The smaller and the more compact the {\em GR}, the smaller the geocast cost.

\textbf{Mobility Panel}: The administrator can generate new datasets from existing ones by having workers moving toward a random direction (i.e., North/South/East/West) by a pre-defined step. The heat map of the updated dataset represents this movement. When the stop button button is clicked, the current snapshot of the worker locations are uploaded to the server, and a new PSD is constructed.

%Besides, PriGeoCrowd also provides usage instruction on the bottom left of Fig \ref{fig:main}. Some GUI-related parameters are given on the top right of the GUI.

\subsubsection{Demonstration Plan}
\label{sec:scenario}

During the demonstration, we will highlight the role of \PGC\ in evaluating the effectiveness and efficiency of private spatial crowdsourcing in several prominent scenarios. Namely, we study the effect of: {\em (i)} varying {\em AG} granularity during sanitization, {\em (ii)} varying dataset density and {\em (iii)} varying the heuristic used in $GR$ construction.

%PriGeoCrowd enables SCP to share or even sell their location data without compromising their customers' location privacy. Moreover, it is a perfect tool for SC companies to evaluate SC services before actual deployment. Since a crowded area better fits SC applications than a sparse area. The worker density information helps the SC companies to decide whether or not to provide SC services for a particular area. Then, they can choose a heuristic depending on their own goal, e.g., minimizing worker travel cost, minimizing communication cost. A wide range of parameter settings and detailed statistics of $\mathit{GR}$ enable SC companies to analyze and choose a good configuration set for deployment. Below, we provide some examples analysis with the default configuration set shown in Fig. \ref{fig:main}.

\textbf{Customized AG granularity}: Figure~\ref{fig:customized_gran} presents the effect of AG granularity on $\mathit{GR}$ size. The work in \cite{to2014framework} uses an adaptation of the original {\em AG} method from \cite{qardaji2012differentially}, which can considerably improve efficiency for SC. The visualization captures the finer granularity obtained using the customized {\em AG}, and highlights the corresponding $\mathit{GR}$s which are significantly more compact.

\begin{figure}[!ht]
	\centering
	\begin{minipage}[b]{0.35\linewidth}
		\includegraphics[width=\textwidth]{figures/sf_gran_false.jpg}
		\subcaption{Original {\em AG}}
		\label{fig:original_AG}
	\end{minipage}
	\hspace{1cm}
	\centering
	\begin{minipage}[b]{0.35\linewidth}
		\includegraphics[width=\textwidth]{figures/sf_gran_true.jpg}
		\subcaption{Modified {\em AG}}
		\label{fig:modified_AG}
	\end{minipage}
	\caption{The effect of customized granularity on $\mathit{GR}$ (SF data)}
\label{fig:customized_gran}
\end{figure}

   
   
%\textbf{Small vs Large Budget}: Fig. \ref{fig:budget} demonstrates the effect of small budget $\varepsilon=0.1$ on $\mathit{GR}$. The cells are much larger than those in the large budget case ($\varepsilon=1$ in Fig. \ref{fig:modified_AG}). Therefore, the $\mathit{GR}$s enclose only a few cells.
%   
%\begin{figure}[!ht]
%	\begin{minipage}[b]{0.9\linewidth}
%	\centering
%		\includegraphics[width=\textwidth]{figures/sf_small_budget.jpg}
%	\end{minipage}
%	\caption{The effect of small budget ($\varepsilon=0.1$) (SF data)}
%\label{fig:budget}
%\end{figure}
   
   
   
\textbf{Dense vs Sparse Area}: Figure~\ref{fig:dense_sparse} shows the effect of worker density on the size of obtained $\mathit{GR}$s. One can observe that the $\mathit{GR}$ obtained in the left-hand side of the figure, corresponding to a sparse suburb area of Phoenix, AZ (Yelp dataset) is much larger than the $\mathit{GR}$ obtained in a denser downtown area of the same dataset. On the other hand, the denser the population, the higher granularity of the {\em AG}. This confirms that our customized {\em AG}~\cite{to2014framework} adapts greatly to the worker density.

%This simplistic example is just an immediate illustration of the density effect, but \PGC\ can allow an in-depth analysis of more advanced scenarios where the density effect is studied in conjunction with varying other system parameters.
% Also, there are some $\mathit{GR}$s with partial cell demonstrate the usefulness of sub-cell optimization.

\begin{figure}[!htb]
\centering
  \includegraphics[width=0.5\textwidth]{figures/yelp_dense_sparse2.jpg}
  \caption{The effect of worker density on $\mathit{GR}$ size (Yelp data)}
  \label{fig:dense_sparse}
\end{figure}

\textbf{Effect of Alternative GR Construction Heuristics}: Figure~\ref{fig:heuristics} demonstrates the behavior of different heuristics on the size and shape of the $\mathit{GR}$. In Figure~\ref{fig:utility} we illustrate the case where $GR$ construction is guided solely by the expected probability of task acceptance success rate (ASR). In this case, populated grid cells tend to be selected first. Figure~\ref{fig:distance} demonstrates the strategy of adding the nearest cell to the $GR$, which results in $\mathit{GR}$s that are centered at the task request. Finally, Figure~\ref{fig:compactness} shows the third strategy that attempts to form $GR$s with a balanced shape, that result to a low hop count when using ad-hoc geocast communication.

\begin{figure}[!ht]
	\centering
	\begin{minipage}[b]{0.25\linewidth}
		\includegraphics[width=\textwidth]{figures/utility.jpg}
		\subcaption{ASR-centric}
		\label{fig:utility}
	\end{minipage}
	\hspace{0.5cm}
	\centering
	\begin{minipage}[b]{0.25\linewidth}
		\includegraphics[width=\textwidth]{figures/distance.jpg}
		\subcaption{Distance}
		\label{fig:distance}
	\end{minipage}
	\hspace{0.5cm}
	\centering
	\begin{minipage}[b]{0.26\linewidth}
		\includegraphics[width=\textwidth]{figures/compactness.jpg}
		\subcaption{Compactness}
		\label{fig:compactness}
	\end{minipage}
	\caption{The effect of different heuristics on $\mathit{GR}$ geometry}	
\label{fig:heuristics}
\end{figure}

\section{Protecting Locations of Workers and Tasks Without Trusted Third Party}
The remainder of this thesis is organized as follows.
Section~\ref{sec:framework} introduces \SCG, and Section~\ref{sec:ota} details our proposed solution. Experimental results are presented in Section~\ref{sec:evaluation}.

\subsection{The SCGuard Privacy Framework}
\label{sec:framework}
In this section we first present a framework for the tasking phase of SC without compromising the privacy of the locations of the individuals (both workers and requesters).
We then identify potential privacy threats from the adversaries (server, requester and worker) and present countermeasures to prevent such threats from occurring.
Finally, we introduce the performance metrics to evaluate and compare our different privacy-preserving algorithms.

%A worker can be in either an online or offline mode. During the assignment period, we have a set of online workers who are ready to work. 
\subsubsection{System Model and Assumptions}
\label{sec:system2} 
We first define the notions of spatial task and worker. A \emph{spatial task} $t$ is required to be answered at a particular location $l_t$. This means task $t$ can be answered by a human only if he is physically located at the task's location $l_t$.
A \emph{worker}, denoted by $w$, is a carrier of a mobile device who volunteers for one of the spatial tasks.
Each worker has a location $l_w$ and a \emph{spatial region} $R_w$, wherein the worker can accept spatial tasks. $R_w$ is represented by a circular region centered at the worker's location. Hence, $R_w$ also refers to a \emph{reachable distance} of the worker---task $t$ is reachable from worker $w$ iff $\mathit{d(w,t)\le R_w}$. During tasking, we assume that each worker performs a single task, and all workers perform every task correctly so that every task needs to be assigned to one and only one worker. We also assume that tasks will not expire during the assignment period.

We focus on \emph{online assignment} where the set of workers $W$ is given in advance while each task in the task set $T$ arrives online (i.e., one at a time). Without privacy protection, the optimal algorithm in terms of maximizing the number of assigned tasks is Ranking~\cite{karp1990optimal}. The algorithm associates each worker with a random number (or rank) so that each task, upon its arrival, is assigned to an unmatched reachable worker of the highest rank\footnote{We also consider distance-based ranking for travel cost optimization in Section~\ref{sec:non-private}}. This algorithm can entirely run on the server. However, with privacy protection, locations of workers and tasks become uncertain, which complicates the task assignment. We propose a privacy-aware framework, named \SCG, for online tasking that involves three distinct stages as follows (see Figure~\ref{fig:privacy_framework}).

In the first stage, to ensure privacy, each worker $w$ perturbs his location $l_w$ with a specified privacy level ($\epsilon,r$) according to the Geo-I mechanism and sends the noisy location $l_{w'}$ together with his reachable distance $R_w$ to the server. Upon the arrival of task $t$, the requester of $t$ perturbs its location $l_t$ with different privacy level ($\epsilon,r$)-Geo-I and sends the perturbed one $l_{t'}$ to the server. The main role of the server is to identify a set of \emph{candidate workers} for the task and then forward their information (i.e., $l_{w'}$ and $R_w$) to the requester. We refer to the first stage as \underline{u}ncertain-\underline{to}-\underline{u}ncertain (\textbf{U2U}) because the locations of both workers and tasks are \underline{u}ncertain to the \emph{server}---the server knows only perturbed locations of workers and tasks. There is no location disclosure at this stage because locations of both task and worker are hidden from all three adversaries. 

The requester, on receipt of the information of the workers, conducts the second stage of \SCG\ without any communication with the server. We isolate this stage from the server to ensure no disclosure to the server. During this stage, the requester sends her task location to the candidate worker $w$ who is most likely reachable within distance $R_w$. We call this stage \underline{u}ncertain-\underline{to}-\underline{e}xact (\textbf{U2E}) because the requester knows the \underline{e}xact location of her task and needs to make a decision as to whether a particular candidate worker is reachable to the task given the worker's perturbed location. This process repeats until the task is assigned or no candidate worker is left.
We discuss three approaches for selecting a candidate worker in Section~\ref{sec:ota}.

The third and final stage, \underline{e}xact-\underline{to}-\underline{e}xact (\textbf{E2E}), is performed by the selected candidate workers. At this point, the requester releases the actual location of the task to the worker, resulting in some disclosure. The worker can verify if the task is reachable by comparing his distance to the task, i.e., $\mathit{d(w,t)\le R_w}$. If the task is reachable, the worker performs the task, and we consider that the task is assigned; otherwise, he rejects the task. Table~\ref{tab:notation2} summarizes the notations used in this paper.

\begin{table}
\begin{center}
\footnotesize
\scriptsize{
\begin{tabular}{ l | l}
\hline
\textbf{Notation} & \textbf{Description} \tn
\hline
$w, t, W, T$ &  a worker, a task, a worker set and a task set \tn
\hline
$w', t', W, T$ &  a worker and a task after perturbation \tn
\hline
$R_w$ & reachable distance that $w$ is willing to travel \tn
%\hline
%$M_{w_i}$ & matching distance of $w_i$ in noisy domain \tn
\hline
$l_w, l_t$ & actual locations of worker $w$ and task $t$ \tn
\hline
$l_{w'}, l_{t'}$ & noisy (perturbed or observed) locations of $w'$ and $t'$ \tn
\hline
$d(w,t)$ & the distance between $l_w$ and $l_t$ \tn
\hline
$d, d'$ & notations of actual and noisy distances \tn
\hline
$w_{max}$ & the worker of the highest rank \tn
\hline
($\epsilon, r$) & parameters that specify the privacy level of Geo-I \tn
\hline
$N_j$ & a set of candidate workers to $t_j$ \tn
\hline
$\alpha, \beta$ & reachability thresholds during U2U and U2E\tn
\hline
\end{tabular}}
\caption{Summary of notations.}\label{tab:notation2}
\end{center}
\end{table}

\begin{figure}[ht]
		\centering
\includegraphics[width=0.75\textwidth]{figs/privacy_framework}
	\caption{\SCG: privacy-aware framework for spatial crowdsourcing.}
	\label{fig:privacy_framework}
\end{figure}

Our approach for task dissemination in U2E is sequential, meaning that the requester sends her task location to one worker in the candidate set at a time. However, one may argue for a parallel approach, where the server first sends the perturbed task location to all workers in the candidate set. Subsequently, the workers simultaneously and independently evaluate whether the task is reachable or not. If so, they send their locations to the requester, who performs the final stage (E2E). This parallel approach may be more efficient but can potentially result in more disclosures. This is because multiple candidate workers may find the task reachable and together send their locations to the requester. Hence, we do not consider this parallel optimization any further.
%Unlike our sequential approach, in which the requester ensures her task is assigned to one and only one worker, this alternative approach tends to redundantly disclose locations of the candidate workers to the requester.
%{\color{red} ''In the worst case, the requester may learn the locations of all candidate workers.''. Removed this sentence as it is about the parallel approach (not our sequential approach). }
%

Another alternative design choice for the U2E stage is for the server to rank the candidate workers rather than the requester. In this case, the candidate workers receive the perturbed task location from the server and return their likelihoods to perform the task to the server. Thereafter, the server matches the task to the worker who most likely will perform the task. This scenario seems to be more efficient in terms of communication cost than the proposed U2E stage; however, the server may be able to learn extra information about the task by observing the responses of one or multiple candidate workers during U2E (reachable or not). The challenge is that these responses are no longer independent of each other since they are computed from the \emph{same} task. Hence, to ensure the same privacy level, the privacy guarantee needs to be extended for a location set~\cite{andres2013geo}. Ensuring Geo-I for a location set drastically reduces the utility of the privacy mechanism as the amount of noise increases linearly with the size of the location set. Thus, we do not consider this design option further.

\subsubsection{Adversary Model}

% assumptions
The adversary model has two core assumptions. First, we assume that all participating entities (server, worker, requester) are curious but not malicious. This means that each entity may learn more than what is exposed during the three stages of \SCG, but they comply with the protocol. The second assumption is that any two entities do not collude with each other to gain information about the third. For instance, if the server wants to collect the location of a requester's task, he may not collude with the workers to learn this information. In what follows, we analyze potential disclosure of location information during each stage of the protocol.

% controlled disclosure to the server
During U2U the server takes as input the perturbed locations of both workers and tasks to perform effective task assignment; 
%hence, there is a serious privacy threat from the server which might become a single point of attack. Fortunately, since the server only receives the perturbed locations of the workers and the requesters' tasks, 
the amount of disclosure to the server is strictly controlled by a given level of privacy according to the Geo-I mechanism. Furthermore, locations of both workers and tasks are protected from the server in all stages of the protocol. This is because the server does not participate in U2E and E2E. Specifically, the server recommends a set of candidate workers to a requester so that they can establish a direct communication channel among themselves. From the perspective of the server, the requester and the candidate workers autonomously decide on whether to accept the recommendation from the server.

% emphasize on the disclosure between workers and requesters (U2E and E2E)
During U2E and E2E, special emphasis is on limiting the disclosure between the workers and the requesters since they may learn each other's exact locations during the course of the protocol. From the viewpoint of a curious requester, locations of the candidate workers are not revealed to the requester during U2E. However, the requester may learn the proximity of the worker to the task (but not the exact location of the worker) by observing the response of a candidate worker during U2E (reachable or not). From the viewpoint of a curious worker, due to the uncertainty of workers' locations during U2E, a task location can be disclosed to multiple candidate workers in E2E before being assigned. This kind of disclosure among requesters and workers, named \emph{false hit}, is quantified in Section~\ref{sec:metrics2}.
%When compared to an ideal scenario where the task location is revealed to one and only one worker.

% potential threats from the server after assignment
After completing an assigned task, the worker reports the result of the task either to the server for quality control, payment, etc. or directly to the requesters to limit the disclosure to the server even further.
%When compared to the tasking stage, at the reporting phase the server can learn more information from the workers and the tasks, including which tasks are assigned to which workers and which tasks remain unassigned. This information may allow the server to learn the location data of workers and requesters by estimating the real-world geographic location of the task results (e.g., image geolocalization). 
Nevertheless, privacy threats during reporting are beyond the scope of this protocol.

\subsubsection{Performance Metrics}
\label{sec:metrics2}

Protecting locations of both workers and tasks significantly complicates task assignment and may reduce the effectiveness and efficiency of task assignment. Due to the noise introduced by Geo-I, a worker-task match observed as reachable in the noisy domain may be unreachable in the actual domain, or vice versa. Both cases may result in tasks remaining unassigned. Thus, to find a reachable worker for a task (i.e., a valid match), multiple messages may need to be sent between the requester and workers, which increases the amount of location disclosure.

To measure these, we introduce the following \emph{end-to-end} performance metrics:
\begin{itemize}
\item
{\bf Utility.} The performance of \SCG\ is measured by the \emph{number of assigned tasks}.
%\footnote{This metric does not capture worker reliability, tasks may still fail to complete after being accepted. Our focus is on assignment success, reliability is outside our scope.}. 
Due to data uncertainty, the server may incorrectly identify candidate workers for a task. The challenge is to obtain a high number of assigned tasks in the presence of uncertainty.
\item
{\bf Travel Cost.} 
%Unlike the non-private setting where a task can be assigned to the closest worker, 
With imprecise locations, the server is no longer able to accurately estimate the distances between workers and tasks. Hence, workers may have to travel long distances to tasks. The challenge is to keep the \emph{worker travel distance} low, even when exact locations are unknown.
\item
{\bf System Overhead.} Dealing with imprecise locations increases the complexity of assignment algorithms, which
poses scalability problems. A significant metric to measure overhead is the \emph{size of the worker candidate set} for a task. This number represents both the communication overhead (server sends the candidate set to the requester) and computational overhead (requester ranks the workers in the candidate set based on certain criteria).
\end{itemize}

In addition, we also report \emph{per-stage} metrics to better understand the performance as well as potential location disclosure of each stage of \SCG.
\begin{itemize}
\item
{\bf Precision/Recall (U2U).} For each task we measure the ratio of the candidate workers who are reachable (precision) and the ratio of the reachable workers in the candidate set (recall).
\item
{\bf False Hit/False Dismissal (U2E).} A false hit is a privacy leak, occurring when a requester estimates an unreachable worker as reachable, measured by the number of times a task location is revealed to a candidate worker who eventually does not perform the task.
A false dismissal occurs when a requester misses a reachable worker.
\end{itemize}

\subsection{Online Task Assignment}
\label{sec:ota}
\subsubsection{Baseline Solution}
In this section we show why existing solutions to the online tasking problem in the non-private setting may not be effective in the private setting. Hence, we introduce a baseline algorithm for the private setting.

\paragraph{Ranking Algorithm}
\label{sec:non-private}
With the online assignment, workers are known and tasks arrive online (one-by-one). Upon arrival, each task needs to be immediately matched to an unmatched worker; the goal is to maximize the number of assigned tasks. A well-known solution to the online assignment problem is the Ranking algorithm~\cite{karp1990optimal}. Ranking randomly permutes the workers and assigns a random priority (or rank) to them. When a task arrives, it is matched to a worker who is reachable to the task and has the highest rank. The expected size of matching obtained by Ranking is at least $(1-\frac{1}{e})|T|=0.63|T|$, where $|T|$ is the total number of tasks. This result is optimal in the non-private setting~\cite{karp1990optimal}. In other words, the competitiveness of any online bipartite matching algorithm is bounded above by 0.63.

\begin{figure}[ht]
%	\begin{minipage}[b]{.54\linewidth}
%		\centering
%		\includegraphics[width=1\textwidth]{figs/example}
%		\subcaption{Actual domain}
%		\label{fig:example_actual}
%	\end{minipage}
	\begin{minipage}[b]{.30\linewidth}
		\centering 
		\includegraphics[width=1\textwidth]{figs/graph}
		\subcaption{Exact reachability graph}
		\label{fig:graph}
	\end{minipage}
	\begin{minipage}[b]{.38\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{figs/example2}
		\subcaption{Noisy layout}
		\label{fig:example_noisy}
	\end{minipage}
	\begin{minipage}[b]{.30\linewidth}
		\centering 
		\includegraphics[width=1\textwidth]{figs/graph2}
		\subcaption{Noisy reachability graph}
		\label{fig:graph2}
	\end{minipage}
	\caption{Example of online tasking with three known workers. Each task arrives one-by-one in the order of $t_1\rightarrow t_2\rightarrow t_3$. Figure~\ref{fig:tasking_framework} shows the exact layout of the workers and the tasks.}
	\label{fig:exam}
\end{figure}

However, the ranking algorithm may not work well in the privacy setting. The reason is that a reachable worker-task pair can be observed as unreachable after perturbation, and vice versa. Figure~\ref{fig:example_noisy} shows the reachability graph of the workers and tasks given their layout shown in Figure~\ref{fig:tasking_framework}. The reachable pair $(w_1,t_3)$ becomes unreachable in the noisy domain, while unreachable pairs $(w_1,t_1)$ and $(w_3,t_3)$ become reachable. In addition, Figure~\ref{fig:graph2} shows an optimal matching in the noisy domain: $(w'_2,t'_1), (w'_1,t'_2)$ and $(w'_3,t'_3)$. Nonetheless, the assignment is actually not optimal because $(w'_3,t'_3)$ is unreachable in the actual domain.

\subsubsection{Baseline Algorithm}
\label{sec:baseline}

Algorithm~\ref{alg:baseline} presents the oblivious algorithm, which considers observed locations as true ones. First, locations of workers and requesters' tasks are locally perturbed according to a specified privacy level ($\epsilon,r$)-Geo-I~\cite{andres2013geo} (Lines~\ref{line:perturb}--\ref{line:perturb2}). During U2U, the server identifies candidate workers for each task such that the observed location of the task is reachable from the observed locations of the workers (Line~\ref{line:reachable_workers}). The server then forwards the candidate workers to the requester, who performs the U2E phase (Line~\ref{line:forward}). As mentioned in Section~\ref{sec:system2}, the reason U2E is performed by the requester instead of the server is that multiple candidate workers may need to be selected until a reachable worker for the task is found. These back-and-forth communications with the workers need to be secure from the server to ensure privacy protection.
During U2E, the requester---once receiving the candidate workers from the server---ranks those workers with respect to certain criteria, such as a random rank~\cite{karp1990optimal}
% (Lines~\ref{line:rank0} and \ref{line:rank}), 
or the worker-task distance (aka the nearest-neighbor strategy) (Line~\ref{line:rank}). Thereafter, the task is matched to the worker of the highest rank (Line~\ref{line:argmax}), who subsequently receives the actual location of the task (Line~\ref{line:u2e}).
During E2E, the selected candidate worker confirms whether the task is actually reachable (Line~\ref{line:check}). If so, this is a valid assignment. Otherwise, the worker rejects the task so that the requester can send the task to the candidate worker of the second highest rank (Line~\ref{line:goto_next}). This matching process (U2E and E2E) continues until either the task is assigned or no candidate worker is left (Line~\ref{line:next}). This \emph{best-effort} strategy clearly provides more opportunity for the task to be assigned, but at the expense of disclosing its location to more workers. This trade-off is illustrated in the following example.

\begin{algorithm} [ht]
\caption{\sc Oblivious Algorithm (Baseline)}
\small
\begin{algorithmic}[1]
\STATE Input: $W, T, R_{w_i}, \epsilon,r$ (refer to the notations in Table~\ref{tab:notation2})
% Input: workers $W$, tasks $T$, reachable distance $R_{w_i}$, privacy level $\epsilon,r$
\STATE Output: a set of valid worker-task matches
\STATE Perturb locations of workers and tasks using Geo-I~\cite{andres2013geo}: \label{line:perturb}
\STATE \hspace{6pt} $l_{w_i}\rightarrow l_{w'_i}$, $l_{t_j}\rightarrow l_{t'_j}$ \label{line:perturb2}
%\STATE Compute random ranks of the workers: \label{line:rank0}
%\STATE \hspace{6pt} For $w_i\in W$ do: $\mathit{Rank_i=random[0,1]}$ \label{line:rank}
\STATE For $t_j\in T$ do: \{assign it to the highest-rank worker\} \label{line:for}
\STATE \hspace{6pt} \textbf{U2U}: Server identifies candidate workers $N_j$ for $t_j$:
\STATE \hspace{18pt} $\mathit{N_j=\{w_i : d(w'_i, t'_j)\le R_{w_i}\}}$ \label{line:reachable_workers}
\STATE \hspace{12pt} If $N_j=\emptyset$: $t_j$ remains unassigned; go to Line~\ref{line:for} \label{line:next}
\STATE \hspace{12pt} Server forwards candidate workers $N_j$ to $t_j$'s requester \label{line:forward}
\STATE \hspace{6pt} \textbf{U2E}: Requester matches $t_j$ to $\mathit{w_{max}}$, where \label{line:argmax} 
\STATE \hspace{12pt} $\mathit{w_{max}=argmax\{Rank(w_i):w_i\in N_j\}}$
\STATE \hspace{18pt} $\mathit{Rank(w_i)=precomputed\ random[0,1]}$ (or $\frac{1}{d(w_i,t_j)}$) \label{line:rank}
\STATE \hspace{12pt} Requester sends exact task location $l_{t_j}$ to $\mathit{w_{max}}$ \label{line:u2e}
\STATE \hspace{6pt} \textbf{E2E}: Worker $\mathit{w_{max}}$ checks if $\mathit{d(w_{max}, t_j) \le R_{w_{max}}}$: \label{line:check}
\STATE \hspace{12pt} If so, match $\mathit{(t_j,w_{max})}$ is a valid assignment
\STATE \hspace{18pt} Update $W$: $\mathit{W=W-\{w_{max}\}}$; go to Line~\ref{line:for}
\STATE \hspace{12pt} Otherwise, update $N_j$: $\mathit{N_j=N_j-\{w_{max}\}}$, go to Line~\ref{line:argmax} \label{line:goto_next}
\end{algorithmic}
\label{alg:baseline}
\end{algorithm}

In Figure~\ref{fig:graph2}, when task $t_1$ arrives, $w_1$ and $w_2$ are candidate workers. Location $l_{t_1}$ is sent to $w_1$ because it has a higher rank. $w_1$ finds $t_1$ unreachable and declines $t_1$, introducing a false hit. Subsequently, $l_{t_1}$ is sent to the next candidate worker of the highest rank, $w_2$.  $w_2$ finds $t_1$ reachable and performs $t_1$. 
%The fact that $w_2$ knows $l_{t_1}$ is not a false hit. 
Next, $t_2$ arrives, and has two candidates, $w_1$ and $w_3$. $w_1$ performs $t_2$ as they are reachable. Finally, $t_3$ arrives and is matched to candidate $w_3$; nevertheless, $w_3$ finds $t_3$ unreachable and rejects $t_3$. In sum, the two assigned tasks $t_1$ and $t_2$ are performed by $w_2$ and $w_1$, respectively. Here, the two false hits include $w_1$ knowing $l_{t_1}$ and $w_3$ knowing $l_{t_3}$.
% In addition, without the best-effort strategy, $t_1$ would have been matched to an unreachable worker $w_1$ and $t_2$ would have been matched to $w_3$, resulting in only one assigned task $t_2$.

We emphasize that the oblivious algorithm guarantees ($\epsilon,r$)-Geo-I to both workers and tasks from the untrusted server. The reason is that the process of finding the candidate workers for a task (after location perturbation) is considered \emph{post-processing}, which does not affect the privacy guarantees of differentially private mechanisms~\cite{mcsherry2009privacy}.
% {\color{red} I rewrote the two sentences. I think this point is worth to summary the baseline solution.}

\subsubsection{Quantifying Worker-Task Pair Reachability}
\label{sec:reachability}

An issue with the oblivious algorithm is that the reachability between a worker and a task is a binary decision based on the perturbed locations, reachable or not, which does not utilize the planar Laplace distribution of the perturbed locations (see Section~\ref{sec:geo-i}). As a result, the baseline solution may include non-reachable workers and miss reachable workers in the candidate set, e.g., $(w_1,t_3)$ becomes unreachable in Figure~\ref{fig:exam}. Therefore, the worker-task pair reachability should be quantified by the \emph{probability of reachability} between the worker and the task. This allows a requester to accurately compare the reachability to her task from multiple candidate workers. 
%, from which the worker with the highest probability is selected.

The objective is to compute the reachability probability of a worker-task pair given their observed distance, i.e., $\mathit{Pr\big(d(w,t) \le R_w | d(w',t')\big)}$ (refer to the notations in Table~\ref{tab:notation2}). We present two approaches to this problem: one is based on approximation analysis (for efficient computation), while another is based on empirical results (requires precomputation on synthetic or historic data).
%Intuitively, this update not only increases the number of assigned tasks, which is the primary goal of this study but also reduces the travel cost and the number of false hits.

\subsubsection{Analytical Approach}
\label{sec:analytical}
An intuitive approach is to derive the distribution (\emph{pdf}) of the actual distances between the locations of workers and tasks $d(w,t)$ given the perturbed locations $l_{w'},l_{t'}$. Recall that the locations are perturbed using the planar Laplace distribution. Once the \emph{pdf} is derived, the reachability probability can be computed efficiently with numerical libraries, such as Python, R and MATLAB. 

The problem of finding the \emph{pdf} of the distance between two uncertain points is related to a family of line picking problems, such as disk line picking.\footnote{http://mathworld.wolfram.com/DiskLinePicking.html} The disk line picking problem is to choose two points at random in a unit disk and find the distribution of the distances between the two points. Such problems have closed form solutions~\cite{tu2002random}. However, in our setting, the two points are drawn from a planar Laplace distribution with different centers rather than uniformly distributed on the same disk. This makes our problem more challenging, and a closed form solution may not exist. 

Because the planar Laplace distribution is difficult to analyze, we propose a two-phase method to parameterize the \emph{pdf}: 1) approximating the planar Laplace distribution by a bivariate normal distribution ($\mathit{BND}$), and 2) deriving the closed form solution to the \emph{pdf} of $d(w,t)$.

\textbf{Approximated $\mathit{\mathbf{BND}}$:}
According to~\cite{andres2013geo}, the \emph{pdf} of the noise-adding mechanism follows a planar Laplace distribution (see Section~\ref{sec:geo-i}) with center at the true location. We approximate the planar Laplace distribution by a $\mathit{BND}$ with the same mean and variance. These are the first two moments, which represent the most important information of a distribution. Since the planar Laplace distribution is symmetric to its center, the approximated $\mathit{BND}$ should be symmetric to the same center (i.e., circular bivariate normal distribution). Subsequently, the approximated distribution is $\mathit{BND(\mu,\Sigma)}$, where $\mu$ is a 2-dimensional mean vector $(w_x,w_y)$\footnote{The subscripts $x$ and $y$ represent the corresponding axis. } representing the worker location, and $\Sigma$ is a diagonal covariance matrix $\bigl[ \begin{smallmatrix} \sigma^2&\\&\sigma^2\end{smallmatrix}\bigr]$, where $\sigma=\sqrt{2}\frac{r}{\epsilon}$ is the standard diviation of the planar Laplace distribution ($\epsilon$ and $r$ are privacy parameters).

Consequently, the distribution of the distance between the perturbed location and its original location is approximated by a normal distribution $\mathit{N(0,2r^2/\epsilon^2)}$. This means that when the perturbed (observed) location is known, the original location is approximated by $\mathit{BND(\mu,\Sigma)}$, centering at the observed point with mean $\mu=(w'_x,w'_y)$ and variance $\Sigma=\bigl[ \begin{smallmatrix}\frac{2r^2}{\epsilon^2}&\\&\frac{2r^2}{\epsilon^2}\end{smallmatrix}\bigr]$.

% If the privacy parameters ($\epsilon,r$) are unknown, mean and variance can still be computed from data using the well-known \emph{method of moment}.
% $$\mu=E[D]=\frac{1}{N}\sum_{i=1}^{N}D_i$$ $$\sigma^2=E[(D-\mu)^2]=\frac{1}{N}\sum_{i=1}^{N}(D_i-\mu)$$
% where $D$ is the random variable that represents the distribution of $d(w,w')$.

Given observed $w'$ and $t'$, we can approximate the original location of $w$ and $t$ both with $\mathit{BND}$. Next, we derive the \emph{pdf}s of $d(w,t)$ for both U2U and U2E stages.

%Since the planar Laplace distribution is symmetric to its center (i.e., the true location), the approximated $\mathit{BND}$ should be symmetric at the observed point. Subsequently, the approximated distribution is $\mathit{BND(\mu,\Sigma)}$, where $\mu$ is a 2-dimensional mean vector $(\mu_x,\mu_y)$ and $\Sigma$ is a diagonal covariance matrix $\bigl( \begin{smallmatrix} \sigma^2&\\&\sigma^2\end{smallmatrix}\bigr)$.
%
%a complex distribution $\mathit{d(w,w')=C_\epsilon^{-1}(p)=\frac{1}{\epsilon}\big(W_{-1}(\frac{p-1}{e} + 1)\big)}$, where $p$ is a random value drawn uniformly in [0,1) and $W_{-1}$ is the Lambert W function. 

\textbf{PDF of $\mathbf{d(w,t)}$ for U2U}: In the U2U stage, given the uncertain locations $l_{w'}$ and $l_{t'}$, our goal is to estimate the \emph{pdf} of $d(w,t)$---the actual distance between the original locations $l_w$ and $l_t$  (see Figure~\ref{fig:quadratic}). As presented, $l_w$ is approximated by $\mathit{BND(\mu_w,\Sigma_w)}$, centering at the observed worker location $l_{w'}$, and  $l_t$ is approximated by $\mathit{BND(\mu_t,\Sigma_t)}$, centering at the observed task location $l_{t'}$. We have $d=d(w,t)=z_x^2+z_y^2$, where $z$ equals to the difference in vector space $z=l_w-l_t$, which follows $\mathit{BND(\mu_w-\mu_t,\Sigma_w+\Sigma_t)}$. Since $d$ has a quadratic form in the bivariate random variable $z$, the moment-generating function (\emph{mgf}) of $d$ has the following form~\cite{mathai1992quadratic}:
$$M(e^{tD})=e^{t\sum_{j=1}^2\frac{b_j^2\lambda_j}{1-2\lambda_j t}}\prod_{j=1}^2 (1-2\lambda_j t)^{-1/2}$$
where $b_j$ is the linear function of $\mu$ ($\mathit{b_1=\mu_{w_x}-\mu_{t_x}}$, $\mathit{b_2=\mu_{w_y}-\mu_{t_y}}$) and $\lambda_j$ are the eigenvalues of $\Sigma=\Sigma_w+\Sigma_t$.

Given the \emph{mgf} of $d$, mean and variance of $d$ can be calculated as follows.
Mean $\mu$ equals to the first derivative of the \emph{mgf} at $\mathit{t=0}$: $\mathit{\mu=E[D]=M_t'(0)}$. Variance $\sigma^2$ can be computed by evaluating the second derivative of the \emph{mgf} at $\mathit{t=0}$: $\mathit{\sigma^2=E[D^2]-(E[D])^2=M_t''(0)-(M_t'(0))^2}$.
Consequently, we approximate the \emph{pdf} of $d$ by a normal distribution $N(\mu,\sigma^2)$, where
mean and variance can be computed efficiently using the built-in python library Scipy.

For simplicity, we derive mean and variance for a special case where both the worker and the task's requester set the same privacy level $(\epsilon,r)$. In such case, the eigenvalues are equal, $\lambda=4r^2/\epsilon^2$ and the \emph{mgf} can be derived as follows, where $\nu$ is the observed worker-task distance $\nu=d(w',t')=\sqrt{b_1^2+b_2^2}$.

\begin{equation}
M(e^{tD})=e^{\frac{\nu^2\lambda t}{1-2\lambda t}}\frac{1}{1-2\lambda t}
\label{eq:mgf}
\end{equation}

We derive the first and second derivatives: $M_t'(0)=\lambda(2+\nu^2)$ and $M_t''(0)=8\lambda^2+12\lambda^2\nu^2+\nu^4$; thus the actual distance $d(w,t)$ follows approximately a normal distribution with mean and variance: $\mu=\lambda(2+\nu^2)$ and $\sigma^2=4\lambda^2(1+2\nu^2)$.

%The $n^{th}$ derivative at  $\mathit{t=0}$ can be efficiently computed 

%{\color{red} I am looking at a possibility that the reachability probability can be directly computed from the \emph{mgf}. If this is possible, we do not need to compute the moments and approximate the \emph{pdf} to a normal distribution.}

\textbf{PDF of $\mathbf{d(w,t)}$ for U2E:} 
In the U2E stage, given the true location of task $l_t$ and the perturbed location of worker $l_{w'}$, we need to estimate the \emph{pdf} of the actual distance $d(w,t)$.
When task location $l_t$ is fixed and worker location $l_w$ follows $\mathit{BND(\mu,\Sigma)}$ centering at the observed worker location $l_{w'}$, the distance between $w$ and $t$ follows the Rice distribution~\cite{stuber2001principles} with parameters ($\nu,\sigma$). $\nu=d(w',t)$ is the distance from the task's location $l_t$ (i.e., the reference point) to the center of the approximated $\mathit{BND}$, $l_{w'}$ (see Figure~\ref{fig:rice}). $\sigma$ is the scale parameter and equals the square root of the variance of the approximated $\mathit{BND}$, $\sqrt{2}r/\epsilon$. The \emph{pdf} of the Rice distribution is:
$$f(x|\nu,\sigma)=\frac{x}{\sigma^2}exp\big(\frac{-(x^2+\nu^2)}{2\sigma^2}\big)I_0(\frac{x\nu}{\sigma^2})$$
where $I_0(.)$ is the modified Bessel function of the first kind with order zero~\cite{abramowitz1966handbook}.
We used the Scipy library to efficiently compute the \emph{pdf} of the Rice distribution.

\begin{figure}[ht]
	\begin{minipage}[b]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{figs/quadratic}
		\subcaption{For U2U}
		\label{fig:quadratic}
	\end{minipage}
	\vspace{10pt}
	\begin{minipage}[b]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{figs/rice}
		\subcaption{For U2E}
		\label{fig:rice}
	\end{minipage}
	\caption{Estimation of the \emph{pdf} of $d(w,t)$ for each stage.}
    \label{fig:reachability_estimation}
\end{figure}

\subsubsection{Empirical Approach}
\label{sec:empirical}

The analytical approach above provides a fast but approximate way to compute the reachability probability. We present an empirical approach that computes the probability from synthetically generated or past data. We show the simulation for each stage of \SCG\ as follows.

For \textbf{U2U}, we generate random locations for a large number of worker-task pairs in a certain region of interest (i.e., Beijing City). All generated locations are perturbed according to ($\epsilon,r$)-Geo-I using random seeds. For each worker-task pair, both the actual distance $d$ and the corresponding noisy distance $d'$ are calculated. The noisy distances are grouped into disjoint ranges: [0...s), [s...2s), ... , [120s...$\infty$), where $s$=100 meters; each range maps to a set of actual distances. We first compute a distribution of $d$ for each range of $d'$. For example, Figure~\ref{fig:actual_dist} shows the distribution of $d$ for a particular range of $d$ ($1900\le d'< 2000$). This distribution centers at the corresponding range of $d'$. Then, the distribution of $d$ can be precomputed for every range of $d'$ and every privacy level ($\epsilon, r$). Consequently, given $\epsilon, r, d'$ from the distribution of $d$, we can compute the reachability probability between a worker and a task as $\mathit{Pr\big(d(w,t) \le R_w | \epsilon,r,d(w',t')\big)}$.
For \textbf{U2E}, we need to precompute the distribution of $d$ for every range of $\epsilon, r$ and $d'$, but now $d'$ is between a random pair of perturbed and exact locations: $\mathit{Pr\big(d(w,t) \le R_w | \epsilon,r,d(w',\mathbf{t})\big)}$.
For \textbf{E2E}, the reachability probability becomes a step function because $d(w,t)$ is exactly computed at this stage (see Figure~\ref{fig:reachable_prob}).

\begin{figure}[ht]
	\begin{minipage}[b]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{exps/actual_dist2k}
		\captionsetup{format=hang}
		\subcaption{Distribution of $d$ when $1900\le d'< 2000$ (U2U)}
		\label{fig:actual_dist}
	\end{minipage}
	\vspace{10pt}
	\begin{minipage}[b]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{exps/reachable_prob}
        \captionsetup{format=hang}
		\subcaption{Reachability probability by varying $d'$}
		\label{fig:reachable_prob}
	\end{minipage}
	\caption{Distributions of $d$ and $Pr(d\le R_w|d')$.}
	\label{fig:empirical}
\end{figure}

Figure~\ref{fig:reachable_prob} illustrates the \emph{pdf} of the reachability probability when varying the observed distance $d'$. Unlike the E2E stage where the worker-task pair reachability is a step function (binary model), the \emph{pdf} values of U2U and U2E decrease linearly with the increase of $d'$.
%The U2E graph provides a better estimation of the reachability than the U2U graph because a requester can estimate her reachability to candidate workers more accurately when compared to the server. 
We also observe that compared to U2E, U2U underestimates when $d'$ is small but overestimates when $d'$ is large. It is worth noting that our empirical approach for precomputing the worker-task pair reachability uses the synthetic location datasets, and therefore does not breach individual information. The precomputed reachability information is used to enhance the performance of task assignment but is not useful to an adversary in gaining information of any individual's location. It is also possible to use either public datasets or past assignment data of completed tasks in the precomputation.


\subsubsection{Probability-based Solution}
\label{sec:improved}
Given the reachability probability, we present a probability-based algorithm that enhances Algorithm~\ref{alg:baseline} at both U2U and U2E stages. The key improvement is to use the probabilistic model (either the analytical or the empirical approach), rather than the binary model, for quantifying reachability between a worker and a task.

\subsubsection{Improvement to U2U}
\label{sec:u2u_improve}

With Algorithm~\ref{alg:baseline}, a worker is a candidate for a task if his observed distance is upper-bounded by the reachable distance of the worker (Line~\ref{line:reachable_workers}).
%The issue is that no candidate worker can be certainly validated or invalidated due to the infinite range of the planar Laplace distribution.
Hence, Line~\ref{line:reachable_workers} of Algorithm~\ref{alg:baseline} may introduce a false positive and false negative. Figure~\ref{fig:u2u_precision_recall} shows the precision/recall scores by varying privacy guarantee $r$. Ensuring high recall is important because low recall means most reachable workers are not in the candidate set, likely resulting in large utility loss.

% \begin{figure}[ht]
% 		\centering
% 		\includegraphics[width=0.3\textwidth]{exps/u2u_precision_recall}
% 	\caption{Precision/recall of the baseline algorithm.}
% 	\label{fig:u2u_precision_recall}
% \end{figure}
	
\textbf{Increasing the recall score:} Algorithm~\ref{alg:baseline} can be updated to ensure high recall during the U2U stage as follows. The candidate workers are selected such that their probability of reachability to a task is greater than a given threshold $\alpha$, termed \emph{U2U threshold} (see Line~\ref{line:pruning} of Algorithm~\ref{alg:probabilistic}). %Figure~\ref{fig:u2u_vary_d_prime} shows the effect of varying $\alpha$ on the precision and recall scores. 
By decreasing $\alpha$, recall is higher but at the cost of lower precision, resulting in an increase of the ratio of unreachable workers in the candidate set. This may incur penalties in the later stage (U2E), such as higher system overhead. We will evaluate the impact of varying $\alpha$ in Section~\ref{sec:evaluation}.

\begin{algorithm} [ht]
\caption{\sc Probability-based Algorithm}
\small
\begin{algorithmic}[1]
\STATE Input: $W, T, R_{w_i}, \epsilon, r, \alpha, \beta$ (refer to the notations in Table~\ref{tab:notation2})
%Input: workers $W$, tasks $T$, reachable distance $R_{w_i}$, privacy level $\epsilon,r$, U2U threshold $\alpha$, U2E threshold $\beta$
\STATE Output: a set of valid worker-task matches
\STATE Perturb locations of workers and tasks using Geo-I~\cite{andres2013geo}:
\STATE \hspace{6pt} $l_{w_i}\rightarrow l_{w'_i}$, $l_{t_j}\rightarrow l_{t'_j}$
\STATE For $t_j\in T$ do: \{assign it to the highest-rank worker\} \label{line:for2}
\STATE \hspace{6pt} \textbf{U2U}: Server identifies candidate workers $N_j$ for $t_j$:
\STATE \hspace{18pt} $N_j=\mathit{\{w_i : Pr\big(d(w_i, t_j)\le R_{w_i} | d(w'_i, t'_j\big) \ge \alpha}\}$ \label{line:pruning}
\STATE \hspace{12pt} If $N_j=\emptyset$: $t_j$ remains unassigned; go to Line~\ref{line:for2} \label{line:next2}
\STATE \hspace{12pt} Server forwards candidate workers $N_j$ to $t_j$'s requester
\STATE \hspace{6pt} \textbf{U2E}: Requester matches $t_j$ to $\mathit{w_{max}}$, where \label{line:u2e2}
\STATE \hspace{12pt} $\mathit{w_{max}=argmax\{Rank(w_i):w_i\in N_j\}}$ 
\STATE \hspace{18pt} $\mathit{Rank(w_i)=Pr(d(w, t)\le R_w)}$ given $\mathit{d'=d(\textbf{w},t')}$ \label{line:rank2}
\STATE \hspace{18pt} If $\mathit{Rank(w_{max}) < \beta}$: go to Line~\ref{line:for2} \label{line:alpha}
\STATE \hspace{12pt} Requester sends exact task location $l_{t_j}$ to $\mathit{w_{max}}$ \label{line:send}
\STATE \hspace{6pt} \textbf{E2E}: Worker $\mathit{w_{max}}$ checks if $\mathit{d(w_{max}, t_j) \le R_{w_{max}}}$: 
\STATE \hspace{12pt} If so, match $(t_j,w_{max})$ is a valid assignment
\STATE \hspace{18pt} Update $\mathit{W=W-{w_{max}}}$; go to Line~\ref{line:for2}
\STATE \hspace{12pt} Otherwise, update $\mathit{N_j=N_j-\{w_{max}\}}$; go to Line~\ref{line:u2e2}
\end{algorithmic}
\label{alg:probabilistic}
\end{algorithm}

\textbf{Improving runtime performance:}
%{\color{red} TODO: Hien. This improvement is not yet implemented because this implementation may take some time and this is not the main focus of this work. I consider to move this paragraph to the discussion section.}
Line~\ref{line:pruning} of Algorithm~\ref{alg:probabilistic} linearly checks all workers, which may be time-consuming for a large number of workers. Hence, we propose a technique to quickly prune workers who are most likely not reachable to a particular task. The technique has two steps.

First, each worker (or task) corresponds to a disk with radius $r_R$, centering at the perturbed location such that the actual location is within the disk with probability at least $\gamma$ (see Section 5 in~\cite{andres2013geo} on how to compute $r_R$). The disks are depicted by the solid circles in Figure~\ref{fig:pruning_u2u}, denoted as $disk(l_{w'},r_R)$ and $disk(l_{t'},r_R)$. Hence, the outer dashed circle represents a region $disk(l_{w'},r_R+R_w)$ that encloses any point in the worker's spatial region $R_w$ with probability at least $\gamma$. Subsequently, we approximate the worker by the larger minimum bounding box (MBR) and the task by the smaller MBR in Figure~\ref{fig:pruning_u2u}.

In the second step, we build an index to quickly prune far-away workers from a task without a full linear scan by applying existing techniques in the uncertain database field~\cite{tao2007range,bernecker2011novel}. For example, the fuzzy search technique in~\cite{tao2007range} is suitable when both data and query (workers and tasks in our context) are represented by rectangles. Applying this pruning technique on the approximated MBRs of workers and tasks gives us a lower bound of $\gamma$ on the reachability probability during U2U. This is because a worker and a task are not reachable from each other if their MBRs do not overlap.
Note that pruning workers during U2U makes sense since it is the most time-consuming stage of \SCG. %We do not include this runtime improvement in the experiments as it is not the focus of this paper.
% and those invalidated workers are unlikely to be selected in the later stage (U2E).

%Therefore, we want to achieve high recall score, meaning that reachable workers are returned for each task with high probability. This results in more assigned tasks. A similar problem has been studied in the context of location-based queries such as 	searching nearby restaurants. The goal is to define an area of retrieval as small as possible but still contains the area of interest with high probability~\cite{andres2013geo}. Unlike this work, our objective is to compute a (noisy) \emph{matching distance} for each worker $M_w$ to ensure a high recall score. 

\begin{figure}[!ht]
	\begin{minipage}[b]{0.45\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/u2u_precision_recall}
        \captionsetup{format=hang}
		\caption{Accuracy of the baseline algorithm.}
		\label{fig:u2u_precision_recall}
	\end{minipage}
	\vspace{5pt}
	\begin{minipage}[b]{0.55\linewidth}
		\centering
		\includegraphics[width=\textwidth]{figs/pruning_u2u}
        \captionsetup{format=hang}
		\caption{Pruning during U2U.}
		\label{fig:pruning_u2u}
	\end{minipage}
\end{figure}

% \begin{figure}[ht]
% 		\includegraphics[width=0.26\textwidth]{figs/pruning_u2u}
% 	\caption{Pruning during U2U.}
% 	\label{fig:pruning_u2u}
% \end{figure}

\subsubsection{Improvement to U2E}
\label{sec:u2e_improve}

The oblivious algorithm ranks candidate workers by either their distances to a task or a random rank associated with each worker (Line~\ref{line:rank} of Algorithm~\ref{alg:baseline}). However, the nearest (or a random) worker may not be reachable to the task. This is because the reachability probability between worker $w$ and task $t$ not only depends on their distance $d(w,t)$ but also on the worker's spatial region $R_w$. To illustrate, in Figure~\ref{fig:example_noisy} $t'_2$ is closer to $w'_1$ than to $w'_3$; however, $t'_2$ is more likely to be reachable from $w'_3$ than from $w'_1$ because $R_{w_3}$ is much greater than $R_{w_1}$.

\textbf{Ranking candidate workers probabilistically:}
We argue that the candidate workers should be ranked based on their reachability to a task. The reason for this is to reduce the number of workers who are being notified of the task location, which results in a small number of false hits (task location disclosures) while reducing travel cost.

%Our hypothesis is that the selection based on reachability probability somewhat combines the randomness of selecting a random worker () and the determinism of selecting the closest worker (), which work well given the uncertain locations. 
Therefore, we modify the U2E stage of Algorithm~\ref{alg:baseline} to capture the probability of reachability, quantified in Section~\ref{sec:reachability}.
Particularly, the requester evaluates the reachability probability between all candidate workers and her task. Subsequently, each candidate worker is associated with a rank $Rank(w_i)$ that equals the corresponding probability of reachability (see Line~\ref{line:rank2} of Algorithm~\ref{alg:probabilistic}). The task is matched to the candidate worker of the highest rank $w_{max}$ (Line~\ref{line:u2e2}). 
%Algorithm~\ref{alg:probabilistic} can use either the empirical or the analytical results of quantifying the reachability (see Section~\ref{sec:reachability}). 
%Consequently, the requester sends the exact task location to the selected worker (Line~\ref{line:send}).

\textbf{Reducing false hits:} Algorithm~\ref{alg:baseline} may disclose a task's location to a large number of candidate workers before the task gets assigned. In the worst case, the task's location can be disclosed to all candidate workers, yet none of them are reachable to the task. Thus, to reduce the number of false hits (or privacy disclosures) during U2E, we propose a thresholding heuristic as follows. A requester cancels her task when the reachability probability from the highest-rank worker $w_{max}$ is smaller than a given threshold $\beta$ (Line~\ref{line:alpha} of Algorithm~\ref{alg:probabilistic}). Subsequently, the requester does not send her task's location to more candidate workers. The choice of $\beta$ affects the number of assigned tasks, false hits and false dismissals. The smaller $\beta$, the more likely the requester sends the actual location of her task to the candidate workers. This results in the task having a higher chance of being assigned, but at the expense of a higher number of false hits. On the other hand, high $\beta$ may lead to false dismissals. We empirically find a good value of $\beta$ in Section~\ref{sec:evaluation}.

%\input{experiment}
\subsection{Performance Evaluation}
\label{sec:evaluation}

We conducted several experiments on real-world data to evaluate the performance of our proposed framework, \SCG. Below, we present the experimental setup in Section~\ref{sec:methodology}, followed by results in Section~\ref {sec:results}.

\subsubsection{Experimental Setup}
\label{sec:methodology}
We performed experiments on the T-Drive dataset~\cite{yuan2010t}. We used one day of the data on Jan 11, 2012, which contains trajectories of more than 9,019 taxis and hundreds of thousands of passengers. We assumed that T-Drive drivers were SC workers and T-Drive passengers were SC requesters. The workers' locations were those of the most recent drop-off locations while tasks were at the pick-up locations. The arrival order of the tasks was determined based on the sorting of their pick-up times.

In all of our experiments, we randomly sampled 500 tasks and 500 workers from T-Drive. These numbers are relatively small when compared to the size of the dataset because we focus on privacy and utility trade-offs rather than runtime performance.
We chose typical ranges of values for $\epsilon,r,R_w$ as follows. Without loss of generality, we assumed the requesters and the workers have the same privacy level ($\epsilon,r$), where $\epsilon \in$ \{0.1, 0.4, \textbf{0.7}, 1.0\} and $r \in$ \{1000, \textbf{700}, 400, 100\} in meters, ranging from strict to loose privacy requirements.
We set the reachable distance of each worker to a random value in meters, 1000$\le R_w\le$3000.
We varied the U2U threshold $\alpha \in$ \{.05, \textbf{.1}, .15, .2, .25, .3, .35, .4\} and U2E threshold $\beta \in$ \{.1, .15, .2, .25, .3, .35, \textbf{.4}\}.
Default values are shown in boldface. It is our intention to have different default values for $\alpha$ and $\beta$. The reason for this is that, in Algorithm~\ref{alg:probabilistic}, the U2U threshold is applied prior to the U2E threshold; therefore, the values of $\alpha$ must be upper-bounded by the default value of $\beta$, and the values of $\beta$ must be at least equal to the default value of $\alpha$.

In the following, we compare the performance of the proposed algorithms in terms of the performance metrics in Section~\ref{sec:metrics2}. In particular, we reported the total number of assigned tasks (\emph{utility}) and the average travel distance (\emph{travel cost}) across the assigned tasks. We also calculated the average number of candidate workers per task (\emph{system overhead}) as well as avearge of the \emph{precision} and \emph{recall} scores (utility during U2U). We measured the total number of \emph{false hits} (aka privacy leak or disclosure) and the total number of \emph{false dismissals} (system overhead during U2E) over all tasks.
All measured results were averaged over ten random seeds.

\subsubsection{Experimental Results}
\label{sec:results}

We compare the performance of the variations of the three algorithms in Section~\ref{sec:ota}: the oblivious algorithm, the probability-based algorithm and the Ranking algorithm that have access to exact location information (ground truth).
First, the ground truth has two variants, \emph{GroundTruth-RR} that uses the \underline{r}andom \underline{r}ank strategy, and \emph{GroundTruth-NN} that uses the \underline{n}earest-\underline{n}eighbor strategy.
Second, \emph{Oblivious-RR} and \emph{Oblivious-RN} refer to Algorithm~\ref{alg:baseline} that uses the corresponding strategy (random rank or nearest-neighbor) to rank the candidate workers. Third, \emph{Probabilistic-Model} and \emph{Probabilistic-Data} are two variants of Algorithm~\ref{alg:probabilistic} which correspond to the analytical and empirical approaches for quantifying the worker-task pair reachability in Section~\ref{sec:reachability}.

%Interestingly, \emph{Probabilistic-Model} even has lower travel cost when compared to \emph{GroundTruth-RR}.

\subsubsection{Overview of Results}
We present the overview of results for comparing 1) analytical vs. empirical approaches for quantifying the reachability 2) random rank vs nearest-neighbor strategies for ranking candidate workers, and 3) performance of the algorithms.

We first compare the analytical and empirical approaches.
The graphs in the first row of Figure~\ref{fig:variants} show the results by varying privacy guarantee $r$. We observe that \emph{Probabilistic-Model} performs as well as \emph{Probabilistic-Data} in terms of utility, and even slightly better in terms of travel cost and privacy leak. This result shows that the analytical model is as accurate as the empirical counterpart for estimating the worker-task pair reachability. Therefore, we use \emph{Probabilistic-Model} from now on because it does not require precomputation.
%This might be the case that the empirical counterpart may not have enough sampled locations (i.e., 100,000).

Next, we compare the two strategies for ranking candidate workers, a random rank and the nearest-neighbor. The graphs in the second row of Figure~\ref{fig:variants} show the results by varying privacy guarantee $r$.
We observe that \emph{GroundTruth-NN} yields marginally lower utility when compared to \emph{GroundTruth-RR} (321 tasks vs. 314 tasks in Figure~\ref{fig:oblivious_NAT}) but at a much smaller travel cost (1483 meters vs. 697 meters in Figure~\ref{fig:oblivious_WTD}). This is because \emph{GroundTruth-RR} focuses solely on the competitive ratio\footnote{The ratio between its performance and the offline algorithm's performance in terms of utility.} without any spatial consideration, such as distance or reachability of a worker-task pair. For the same reason, when compared to \emph{Oblivious-RR}, \emph{Oblivious-RN} yields slightly lower utility at significantly lower travel cost and lower privacy leak. Hence, we will use \emph{GroundTruth-NN} as the ground truth and \emph{Oblivious-RN} as the baseline.

\begin{figure}[!ht]
	\begin{minipage}[b]{0.32\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/probabilistic_NAT}
		\subcaption{Utility (\#Tasks)}
		\label{fig:probabilistic_NAT}
	\end{minipage}
	\begin{minipage}[b]{0.32\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/probabilistic_WTD}
		\subcaption{Travel cost (m)}
		\label{fig:probabilistic_WTD}
	\end{minipage}
	\begin{minipage}[b]{0.32\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/probabilistic_FH}
		\subcaption{\#False hits}
		\label{fig:probabilistic_FH}
	\end{minipage}
	\begin{minipage}[b]{0.32\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/oblivious_NAT}
		\subcaption{Utility (\#Tasks)}
		\label{fig:oblivious_NAT}
	\end{minipage}
	\begin{minipage}[b]{0.32\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/oblivious_WTD}
		\subcaption{Travel cost (m)}
		\label{fig:oblivious_WTD}
	\end{minipage}
	\begin{minipage}[b]{0.32\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/oblivious_FH}
		\subcaption{\#False hits}
		\label{fig:oblivious_FH}
	\end{minipage}
	\caption{Comparison of the variants of the algorithms by varying privacy guarantee $r$.}
\label{fig:variants}
\end{figure}

Last but not least, we report two main results by comparing the performance of different algorithms.
Figure~\ref{fig:base_reach} illustrates the results by varying privacy guarantee $\epsilon$. 
The first result is that the \emph{Probabilistic-Model} outperforms \emph{Oblivious-RN} in all key metrics, including higher utility ($\times3$ in Figure~\ref{fig:base_reach_NAT}), smaller travel cost ($\times2/3$ in Figure~\ref{fig:base_reach_WTD}), and much lower disclosure of task location ($/100$ in Figure~\ref{fig:base_reach_FH}), with only a slight increase in overhead (20\% in Figure~\ref{fig:base_reach_CAN}). These improvements are more significant with higher privacy level (low $\epsilon$). The results confirm that the proposed probabilistic models are superior to the binary model in estimating the worker-task pair reachability.
The second result is that when compared to the ground truth, privacy provided by the probability-based algorithm does not significantly affect utility (Figure~\ref{fig:base_reach_NAT}) and travel cost (Figure~\ref{fig:base_reach_WTD}), proving that tasks can be effectively assigned to nearby workers without compromising the key metrics. This result is significant because utility and travel cost are perhaps the most important factors in SC.

\begin{figure*}[!ht]
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/base_reach_NAT}
		\subcaption{Utility (\#Tasks)}
		\label{fig:base_reach_NAT}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/base_reach_WTD}
		\subcaption{Travel cost (meters)}
		\label{fig:base_reach_WTD}
	\end{minipage}	
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/base_reach_FH}
		\subcaption{Privacy leak (\#False hits)}
		\label{fig:base_reach_FH}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/base_reach_CAN}
		\subcaption{Overhead (\#Workers)}
		\label{fig:base_reach_CAN}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/base_reach_PR}
		\subcaption{Precision/recall}
		\label{fig:base_reach_PR}
	\end{minipage}
	\caption{Comparison of the algorithms by varying $\epsilon$.}
\label{fig:base_reach}
\end{figure*}

\subsubsection{Details of Results}

We further compare the algorithms with respect to each performance metric.

\paragraph{Utility (Number of Assigned Tasks)}

Figure~\ref{fig:base_reach_NAT} shows the results when varying privacy loss $\epsilon$. 
\emph{GroundTruth-NN} achieves the highest utility followed by \emph{Probabilistic-Model}, which obtains up to 300\% higher utility than \emph{Oblivious-RR}, especially with higher privacy level (smaller $\epsilon$).
We also observe that when $\epsilon$ increases (less privacy), the utility of both \emph{Probabilistic-Model} and \emph{Oblivious-RN} asymptotically increases to the utility of \emph{GroundTruth-NN}. This is because when less noise is injected, the perturbed locations tend to be closer to the actual ones.
%, resulting in smaller observed worker-task distance $d'$ and higher probability of reachability between workers and tasks (see Figure~\ref{fig:reachable_prob}).
This yields a higher number of assigned tasks.

\paragraph{Worker Travel Cost}

Figure~\ref{fig:base_reach_WTD} shows the results when varying privacy loss $\epsilon$.
It is expected that \emph{GroundTruth-NN} obtains the lowest travel cost as it has access to actual location data.
\emph{Probabilistic-Model} achieves significantly lower travel cost (up to 30\%) when compared to \emph{Oblivious-RN}. The improvement is higher at smaller $\epsilon$ (higher privacy).
We also observe that as $\epsilon$ grows (less privacy), the worker travel cost of both \emph{Probabilistic-Model} and \emph{Oblivious-RN} asymptotically reduces to the travel cost of \emph{GroundTruth-NN}. 
% TODO: CAN BE REMOVED
%The reason for this is that, as we observed, higher $\epsilon$ leads to more accurate perturbed location information; hence, nearby workers are more likely to be selected for a task.

\paragraph{Overhead and Privacy Leak}

%\begin{figure*}[!ht]
%	\begin{minipage}[b]{0.195\linewidth}
%	\centering
%		\includegraphics[width=\textwidth]{exps/reach_nonpriv_NAT}
%		\subcaption{Utility (\#Tasks)}
%		\label{fig:reach_nonpriv_NAT}
%	\end{minipage}
%	\begin{minipage}[b]{0.195\linewidth}
%	\centering
%		\includegraphics[width=\textwidth]{exps/reach_nonpriv_WTD}
%		\subcaption{Travel cost (meters)}
%		\label{fig:reach_nonpriv_WTD}
%	\end{minipage}	
%	\begin{minipage}[b]{0.195\linewidth}
%	\centering
%		\includegraphics[width=\textwidth]{exps/reach_nonpriv_CAN}
%		\subcaption{Overhead (\#Workers)}
%		\label{fig:reach_nonpriv_CAN}
%	\end{minipage}
%	\begin{minipage}[b]{0.195\linewidth}
%		\centering
%		\includegraphics[width=\textwidth]{exps/reach_nonpriv_PR}
%		\subcaption{Precision/recall}
%		\label{fig:reach_nonpriv_PR}
%	\end{minipage}
%	\begin{minipage}[b]{0.195\linewidth}
%	\centering	
%		\includegraphics[width=\textwidth]{exps/reach_nonpriv_FH}
%		\subcaption{Privacy leak (\#False hits)}
%		\label{fig:reach_nonpriv_FH}
%	\end{minipage}
%	\caption{Overhead of privacy by varying privacy level $(\epsilon, r)$.}
%\label{fig:reach_nonpriv}
%\end{figure*}

Figures~\ref{fig:base_reach_FH},~\ref{fig:base_reach_CAN} show the results when varying privacy loss $\epsilon$.
Although the overhead of \emph{Probabilistic-Model} is slightly higher than \emph{Oblivious-RN}'s (i.e., up to 500 workers vs. up to 400 workers in the candidate set), \emph{Probabilistic-Model} has a much smaller disclosure of location information (i.e., up to 14 false hits vs. up to 1239 false hits). This means that before a task can be assigned, \emph{Oblivious-RN} needs to send the task to $\sim$23 workers on average while the number is $\sim$1.05 for \emph{Probabilistic-Model}. Unlike \emph{Oblivious-RN}, \emph{Probabilistic-Model} usually identifies a candidate worker who is reachable to a task at the first try, without the need of sending the task to multiple workers during U2E. This result shows that our proposed approaches, the probability-based ranking and the thresholding heuristic in Section~\ref{sec:u2e_improve}, effectively limit the disclosure of location information.
This is crucial because the disclosure of location information (among requesters and workers) is the only privacy leak in \SCG.

We also show the impact of privacy loss $\epsilon$ on system overhead and privacy leak. As expected, when $\epsilon$ increases (less privacy), system overhead and privacy leak decrease while both precision and recall increase.
% TODO: CAN BE REMOVED
%The reason for this is that when less noise is injected, the server can accurately find the candidate workers who are reachable to a given task, resulting in smaller (sufficient) number of workers in the candidate set. Likewise, when the requester can accurately rank the candidate workers based on their probability of reachability to a task, the task location needs to be sent to a sufficient number of workers (small privacy leak).

\subsubsection{Effect of Parameter Settings}
\label{label:thresholds}
We evaluate the performance of  the  \emph{Probabilistic-Model} by varying the U2U and U2E thresholds ($\alpha$, $\beta$).

\begin{figure*}[!ht]
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/u2u_thres_count}
		\subcaption{Countable metrics}
		\label{fig:u2u_thres_count}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/u2u_thres_WTD}
		\subcaption{Travel cost (meters)}
		\label{fig:u2u_thres_WTD}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/u2u_thres_PR}
		\subcaption{U2U metrics}
		\label{fig:u2u_thres_PR}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/u2u_thres_false}
		\subcaption{U2E metrics}
		\label{fig:u2u_thres_false}
	\end{minipage}
	\begin{minipage}[b]{0.195\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/runtime}
		\subcaption{Average runtime (s)}
		\label{fig:runtime}
	\end{minipage}
	\caption{Performance of \emph{Probabilistic-Model} by decreasing U2U threshold ($\alpha$).}
\label{fig:vary_alpha}
\end{figure*}

\paragraph{Impact of varying U2U threshold $\alpha$}
We first show the impact of varying $\alpha$ on both the U2U and U2E stages (see Figure~\ref{fig:vary_alpha}). The main observation is that by \emph{decreasing} the value of $\alpha$, the algorithm achieves higher utility (Figure~\ref{fig:u2u_thres_count}), lower travel distance (Figure~\ref{fig:u2u_thres_WTD}) and lower privacy leak\footnote{$\alpha$ does not directly impact false hit and false dismissal because they are U2E metrics.} (Figure~\ref{fig:u2u_thres_false}) at the expense of higher system overhead (Figure~\ref{fig:u2u_thres_count}). The reason for this is that the smaller U2U threshold $\alpha$, the larger the worker candidate set (see Line~\ref{line:pruning} of Algorithm~\ref{alg:probabilistic}), providing the task more chances to be assigned at the U2E phase. The impact of $\alpha$ on recall in Figure~\ref{fig:u2u_thres_PR} confirms our intuition of achieving higher recall in Section~\ref{sec:u2u_improve}.

Consequently, to achieve high SC utility, the value of U2U threshold $\alpha$ should be as small as possible, while the size of the worker candidate set is manageable by the requester in terms of runtime (e.g., $\alpha=0.1$). Figure\footnote{We are not applying the runtime optimization proposed in Section~\ref{sec:u2u_improve}.}~\ref{fig:runtime} shows the runtime of the U2E stage when varying $\alpha$. As expected, the smaller $\alpha$, the higher the runtime due to having a larger candidate set per task.

\paragraph{Impact of varying U2E threshold $\beta$}
We present the impact of varying U2E threshold\footnote{$\beta$ does not have any impact on system overhead and the precision/recall scores as they are U2U metrics.} $\beta$ on the U2E stage (see Figure~\ref{fig:vary_beta}). In most cases, we observe that as U2E threshold $\beta$ grows, utility, travel cost and the number of false dismissals stay flat while location disclosure decreases linearly. This result confirms our aim to reduce privacy leak by introducing U2E threshold $\beta$ in Section~\ref{sec:u2e_improve}.
However, false dismissal increases at a certain value of $\beta$ (i.e., 0.3), which obviously decreases utility. The reason for this is that the higher U2E threshold $\beta$, the more likely a requester misses a candidate worker who is reachable to a task.
In sum, to reduce privacy leak, the value of U2E threshold $\beta$ should be as large as possible, but at the same time should not incur significant utility loss (e.g., $\beta=0.3$).

\begin{figure}[!ht]
	\begin{minipage}[b]{0.32\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/u2e_thres_count}
		\subcaption{Countable metrics}
		\label{fig:u2e_thres_count}
	\end{minipage}
	\begin{minipage}[b]{0.32\linewidth}
		\centering
		\includegraphics[width=\textwidth]{exps/u2e_thres_false}
		\subcaption{U2E metrics}
		\label{fig:u2e_thres_false}
	\end{minipage}
	\begin{minipage}[b]{0.32\linewidth}
	\centering
		\includegraphics[width=\textwidth]{exps/u2e_thres_WTD}
		\subcaption{Travel cost (m)}
		\label{fig:u2e_thres_WTD}
	\end{minipage}
	\caption{Performance of \emph{Probabilistic-Model} by increasing U2E threshold ($\beta$).}
\label{fig:vary_beta}
\end{figure}

\chapter{Conclusion}

With the popularity of mobile devices, spatial crowdsourcing is rising as a framework that enables
human workers to solve tasks in the physical world. With spatial crowdsourcing, requesters outsource a set of spatiotemporal tasks to a set of workers, i.e., individuals with mobile devices that perform the tasks by physically traveling to the specified locations of interest. However, current solutions require a worker to disclose his location to the server and/or to other requesters even before accepting a task---or a requester to disclose his tasks' locations, which can be used to infer his own location, to untrusted entities.

In this thesis, we identified potential privacy threats from the adversaries (server, requester and worker) and present countermeasures to prevent such threats from occurring. We introduced novel privacy-aware frameworks to protect locations of both workers and tasks in spatial crowdsourcing. 
The first framework~\cite{to2014framework} enables the participation of workers without compromising their location privacy.
We identified geocasting as a needed step to ensure that privacy is protected prior to workers consenting to a task. We also provided heuristics and optimizations for determining effective geocast regions that achieve high task assignment rate with low overhead. Our experimental results on real data demonstrated that the proposed techniques are effective, and the cost of privacy is practical.
The second framework~\cite{to2018privacy} protects locations of both workers and tasks in spatial crowdsourcing without any trusted entity. We proposed models for quantifying the probability of reachability between a worker and a task, from which the probability-based algorithm was introduced to assign tasks to workers in an online manner. We introduced the performance metrics to evaluate and compare our different privacy-preserving algorithms. Our experimental results on real data demonstrated that the proposed techniques, algorithms, and heuristics achieve high utility, small worker travel cost, and low disclosure of location information.

As future directions, we plan to use the optimal geo-indistinguishable mechanisms~\cite{bordenabe2014optimal} and adopt the elastic distinguishability metric~\cite{chatzikokolakis2015constructing} to further improve the utility of the task assignment. The reason is that Geo-I is based on Euclidean distance, meaning that privacy protection is uniform in space. However, in spatial crowdsourcing, we may want to tune the amount of noise injected to each location, such as less noise to dense areas and more noise to sparse areas. Another promising direction is to consider powerful adversaries with knowledge about temporal correlations of a moving user's locations~\cite{xiao2015protecting}. We may also consider collusion between workers and the server; for example, some workers may work for the spatial crowdsourcing company or the company may use driverless cars.




%\bibliographystyle{namedplus}
%\bibliographystyle{natbib}
\bibliographystyle{abbrv}
%\begin{footnotesize}
\bibliography{references}
%\end{footnotesize}

\balance

%\input{appendix}

\end{document}


